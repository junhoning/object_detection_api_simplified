{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blue_\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "\n",
    "import json\n",
    "import functools\n",
    "from abc import abstractmethod\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.protos import image_resizer_pb2\n",
    "from object_detection.protos import anchor_generator_pb2\n",
    "from object_detection.protos import model_pb2\n",
    "from object_detection.protos import hyperparams_pb2\n",
    "from object_detection.protos import box_predictor_pb2\n",
    "from object_detection.protos import post_processing_pb2\n",
    "from object_detection.protos import losses_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = ''  # Name of the TensorFlow master to use.\n",
    "task = 0  # task id\n",
    "num_clones = 1  # Number of clones to deploy per worker.\n",
    "clone_on_cpu = False  # Force clones to be deployed on CPU.  Note that even if \n",
    "                      # set to False (allowing ops to run on gpu), some ops may\n",
    "                      # still be run on the CPU if they have no GPU kernel.\n",
    "worker_replicas = 1  # Number of worker+trainer replicas.\n",
    "ps_tasks = 0  # Number of parameter server tasks. If None, does not use a parameter server.\n",
    "train_dir = 'training'  # Directory to save the checkpoints and training summaries.\n",
    "pipeline_config_path = 'configs/faster_rcnn_resnet50_facenet.config'  # Directory to save the checkpoints and training summaries.\n",
    "train_config_path = ''  # Path to a pipeline_pb2.TrainEvalPipelineConfig config file. If provided, other configs are ignored\n",
    "input_config_path = ''  # Path to a train_pb2.TrainConfig config file.\n",
    "model_config_path = ''  # Path to a model_pb2.DetectionModel config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import config_util\n",
    "\n",
    "assert train_dir, '`train_dir` is missing.'\n",
    "if task == 0: \n",
    "    tf.gfile.MakeDirs(train_dir)\n",
    "if pipeline_config_path:\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "    if task == 0:\n",
    "        tf.gfile.Copy(pipeline_config_path,\n",
    "                      os.path.join(train_dir, 'pipeline.config'),\n",
    "                      overwrite=True)\n",
    "else:\n",
    "    configs = config_util.get_configs_from_multiple_files(\n",
    "        model_config_path=model_config_path,\n",
    "        train_config_path=train_config_path,\n",
    "        train_input_config_path=input_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "        for name, config in [('model.config', model_config_path),\n",
    "                             ('train.config', train_config_path),\n",
    "                             ('input.config', input_config_path)]:\n",
    "            tf.gfile.Copy(config, os.path.join(train_dir, name),\n",
    "                          overwrite=True)\n",
    "\n",
    "model_config = configs['model']\n",
    "train_config = configs['train_config']\n",
    "input_config = configs['train_input_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "# meta_architectures/faster_rcnn_meta_arch.py\n",
    "class FasterRCNNFeatureExtractor(object):\n",
    "  \"\"\"Faster R-CNN Feature Extractor definition.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: A boolean indicating whether the training version of the\n",
    "        computation graph should be constructed.\n",
    "      first_stage_features_stride: Output stride of extracted RPN feature map.\n",
    "      batch_norm_trainable: Whether to update batch norm parameters during\n",
    "        training or not. When training with a relative large batch size\n",
    "        (e.g. 8), it could be desirable to enable batch norm update.\n",
    "      reuse_weights: Whether to reuse variables. Default is None.\n",
    "      weight_decay: float weight decay for feature extractor (default: 0.0).\n",
    "    \"\"\"\n",
    "    self._is_training = is_training\n",
    "    self._first_stage_features_stride = first_stage_features_stride\n",
    "    self._train_batch_norm = (batch_norm_trainable and is_training)\n",
    "    self._reuse_weights = reuse_weights\n",
    "    self._weight_decay = weight_decay\n",
    "\n",
    "  @abstractmethod\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Feature-extractor specific preprocessing (minus image resizing).\"\"\"\n",
    "    pass\n",
    "\n",
    "  def extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features.\n",
    "\n",
    "    This function is responsible for extracting feature maps from preprocessed\n",
    "    images.  These features are used by the region proposal network (RPN) to\n",
    "    predict proposals.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: A [batch, height, width, channels] float tensor\n",
    "        representing a batch of images.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n",
    "      activations: A dictionary mapping activation tensor names to tensors.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, values=[preprocessed_inputs]):\n",
    "      return self._extract_proposal_features(preprocessed_inputs, scope)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features, to be overridden.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features.\n",
    "\n",
    "    Args:\n",
    "      proposal_feature_maps: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n",
    "        representing the feature map cropped to each proposal.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      proposal_classifier_features: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, height, width, depth]\n",
    "        representing box classifier features for each proposal.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "        scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):\n",
    "      return self._extract_box_classifier_features(proposal_feature_maps, scope)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features, to be overridden.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def restore_from_classification_checkpoint_fn(\n",
    "      self,\n",
    "      first_stage_feature_extractor_scope,\n",
    "      second_stage_feature_extractor_scope):\n",
    "    \"\"\"Returns a map of variables to load from a foreign checkpoint.\n",
    "\n",
    "    Args:\n",
    "      first_stage_feature_extractor_scope: A scope name for the first stage\n",
    "        feature extractor.\n",
    "      second_stage_feature_extractor_scope: A scope name for the second stage\n",
    "        feature extractor.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping variable names (to load from a checkpoint) to variables in\n",
    "      the model graph.\n",
    "    \"\"\"\n",
    "    variables_to_restore = {}\n",
    "    for variable in tf.global_variables():\n",
    "      for scope_name in [first_stage_feature_extractor_scope,\n",
    "                         second_stage_feature_extractor_scope]:\n",
    "        if variable.op.name.startswith(scope_name):\n",
    "          var_name = variable.op.name.replace(scope_name + '/', '')\n",
    "          variables_to_restore[var_name] = variable\n",
    "    return variables_to_restore\n",
    "\n",
    "\n",
    "# model/faster_rcnn_resnet_v1_feature_extractor.py\n",
    "class FasterRCNNResnetV1FeatureExtractor(FasterRCNNFeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet V1 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               architecture,\n",
    "               resnet_model,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      architecture: Architecture name of the Resnet V1 model.\n",
    "      resnet_model: Definition of the Resnet V1 model.\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16.\n",
    "    \"\"\"\n",
    "    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n",
    "      raise ValueError('`first_stage_features_stride` must be 8 or 16.')\n",
    "    self._architecture = architecture\n",
    "    self._resnet_model = resnet_model\n",
    "    super(FasterRCNNResnetV1FeatureExtractor, self).__init__(\n",
    "        is_training, first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n",
    "\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Faster R-CNN Resnet V1 preprocessing.\n",
    "\n",
    "    VGG style channel mean subtraction as described here:\n",
    "    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n",
    "\n",
    "    Args:\n",
    "      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n",
    "        representing a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n",
    "        tensor representing a batch of images.\n",
    "\n",
    "    \"\"\"\n",
    "    channel_means = [123.68, 116.779, 103.939]\n",
    "    return resized_inputs - [[channel_means]]\n",
    "\n",
    "\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n",
    "      raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
    "                       'tensor of shape %s' % preprocessed_inputs.get_shape())\n",
    "    shape_assert = tf.Assert(\n",
    "        tf.logical_and(\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n",
    "        ['image size must at least be 33 in both height and width.'])\n",
    "\n",
    "    with tf.control_dependencies([shape_assert]):\n",
    "      # Disables batchnorm for fine-tuning with smaller batch sizes.\n",
    "      # TODO(chensun): Figure out if it is needed when image\n",
    "      # batch size is bigger.\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with tf.variable_scope(\n",
    "            self._architecture, reuse=self._reuse_weights) as var_scope:\n",
    "          _, activations = self._resnet_model(\n",
    "              preprocessed_inputs,\n",
    "              num_classes=None,\n",
    "              is_training=self._train_batch_norm,\n",
    "              global_pool=False,\n",
    "              output_stride=self._first_stage_features_stride,\n",
    "              spatial_squeeze=False,\n",
    "              scope=var_scope)\n",
    "\n",
    "    handle = scope + '/%s/block3' % self._architecture\n",
    "    return activations[handle], activations\n",
    "\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            is_training=self._train_batch_norm):\n",
    "          blocks = [\n",
    "              resnet_utils.Block('block4', resnet_v1.bottleneck, [{\n",
    "                  'depth': 2048,\n",
    "                  'depth_bottleneck': 512,\n",
    "                  'stride': 1\n",
    "              }] * 3)\n",
    "          ]\n",
    "          proposal_classifier_features = resnet_utils.stack_blocks_dense(\n",
    "              proposal_feature_maps, blocks)\n",
    "    return proposal_classifier_features\n",
    "\n",
    "\n",
    "# model/faster_rcnn_resnet_v1_feature_extractor.py\n",
    "class FasterRCNNResnet50FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet 50 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16,\n",
    "        or if `architecture` is not supported.\n",
    "    \"\"\"\n",
    "    super(FasterRCNNResnet50FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_50', resnet_v1.resnet_v1_50, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder.py\n",
    "# A map of names to Faster R-CNN feature extractors.\n",
    "FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'faster_rcnn_resnet50': FasterRCNNResnet50FeatureExtractor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/preprocessor.py\n",
    "def resize_to_range(image,\n",
    "                    masks=None,\n",
    "                    min_dimension=None,\n",
    "                    max_dimension=None,\n",
    "                    method=tf.image.ResizeMethod.BILINEAR,\n",
    "                    align_corners=False,\n",
    "                    pad_to_max_dimension=False,\n",
    "                    per_channel_pad_value=(0, 0, 0)):\n",
    "  \"\"\"Resizes an image so its dimensions are within the provided value.\n",
    "\n",
    "  The output size can be described by two cases:\n",
    "  1. If the image can be rescaled so its minimum dimension is equal to the\n",
    "     provided value without the other dimension exceeding max_dimension,\n",
    "     then do so.\n",
    "  2. Otherwise, resize so the largest dimension is equal to max_dimension.\n",
    "\n",
    "  Args:\n",
    "    image: A 3D tensor of shape [height, width, channels]\n",
    "    masks: (optional) rank 3 float32 tensor with shape\n",
    "           [num_instances, height, width] containing instance masks.\n",
    "    min_dimension: (optional) (scalar) desired size of the smaller image\n",
    "                   dimension.\n",
    "    max_dimension: (optional) (scalar) maximum allowed size\n",
    "                   of the larger image dimension.\n",
    "    method: (optional) interpolation method used in resizing. Defaults to\n",
    "            BILINEAR.\n",
    "    align_corners: bool. If true, exactly align all 4 corners of the input\n",
    "                   and output. Defaults to False.\n",
    "    pad_to_max_dimension: Whether to resize the image and pad it with zeros\n",
    "      so the resulting image is of the spatial size\n",
    "      [max_dimension, max_dimension]. If masks are included they are padded\n",
    "      similarly.\n",
    "    per_channel_pad_value: A tuple of per-channel scalar value to use for\n",
    "      padding. By default pads zeros.\n",
    "\n",
    "  Returns:\n",
    "    Note that the position of the resized_image_shape changes based on whether\n",
    "    masks are present.\n",
    "    resized_image: A 3D tensor of shape [new_height, new_width, channels],\n",
    "      where the image has been resized (with bilinear interpolation) so that\n",
    "      min(new_height, new_width) == min_dimension or\n",
    "      max(new_height, new_width) == max_dimension.\n",
    "    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n",
    "      shape [num_instances, new_height, new_width].\n",
    "    resized_image_shape: A 1D tensor of shape [3] containing shape of the\n",
    "      resized image.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the image is not a 3D tensor.\n",
    "  \"\"\"\n",
    "  if len(image.get_shape()) != 3:\n",
    "    raise ValueError('Image should be 3D tensor')\n",
    "\n",
    "  with tf.name_scope('ResizeToRange', values=[image, min_dimension]):\n",
    "    if image.get_shape().is_fully_defined():\n",
    "      new_size = _compute_new_static_size(image, min_dimension, max_dimension)\n",
    "    else:\n",
    "      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)\n",
    "    new_image = tf.image.resize_images(\n",
    "        image, new_size[:-1], method=method, align_corners=align_corners)\n",
    "\n",
    "    if pad_to_max_dimension:\n",
    "      channels = tf.unstack(new_image, axis=2)\n",
    "      if len(channels) != len(per_channel_pad_value):\n",
    "        raise ValueError('Number of channels must be equal to the length of '\n",
    "                         'per-channel pad value.')\n",
    "      new_image = tf.stack(\n",
    "          [\n",
    "              tf.pad(\n",
    "                  channels[i], [[0, max_dimension - new_size[0]],\n",
    "                                [0, max_dimension - new_size[1]]],\n",
    "                  constant_values=per_channel_pad_value[i])\n",
    "              for i in range(len(channels))\n",
    "          ],\n",
    "          axis=2)\n",
    "      new_image.set_shape([max_dimension, max_dimension, 3])\n",
    "\n",
    "    result = [new_image]\n",
    "    if masks is not None:\n",
    "      new_masks = tf.expand_dims(masks, 3)\n",
    "      new_masks = tf.image.resize_images(\n",
    "          new_masks,\n",
    "          new_size[:-1],\n",
    "          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "          align_corners=align_corners)\n",
    "      if pad_to_max_dimension:\n",
    "        new_masks = tf.image.pad_to_bounding_box(\n",
    "            new_masks, 0, 0, max_dimension, max_dimension)\n",
    "      new_masks = tf.squeeze(new_masks, 3)\n",
    "      result.append(new_masks)\n",
    "\n",
    "    result.append(new_size)\n",
    "    return result\n",
    "\n",
    "\n",
    "# core/preprocessor.py\n",
    "# TODO(alirezafathi): Investigate if instead the function should return None if\n",
    "# masks is None.\n",
    "# pylint: disable=g-doc-return-or-yield\n",
    "def resize_image(image,\n",
    "                 masks=None,\n",
    "                 new_height=600,\n",
    "                 new_width=1024,\n",
    "                 method=tf.image.ResizeMethod.BILINEAR,\n",
    "                 align_corners=False):\n",
    "  \"\"\"Resizes images to the given height and width.\n",
    "\n",
    "  Args:\n",
    "    image: A 3D tensor of shape [height, width, channels]\n",
    "    masks: (optional) rank 3 float32 tensor with shape\n",
    "           [num_instances, height, width] containing instance masks.\n",
    "    new_height: (optional) (scalar) desired height of the image.\n",
    "    new_width: (optional) (scalar) desired width of the image.\n",
    "    method: (optional) interpolation method used in resizing. Defaults to\n",
    "            BILINEAR.\n",
    "    align_corners: bool. If true, exactly align all 4 corners of the input\n",
    "                   and output. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    Note that the position of the resized_image_shape changes based on whether\n",
    "    masks are present.\n",
    "    resized_image: A tensor of size [new_height, new_width, channels].\n",
    "    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n",
    "      shape [num_instances, new_height, new_width]\n",
    "    resized_image_shape: A 1D tensor of shape [3] containing the shape of the\n",
    "      resized image.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(\n",
    "      'ResizeImage',\n",
    "      values=[image, new_height, new_width, method, align_corners]):\n",
    "    new_image = tf.image.resize_images(\n",
    "        image, tf.stack([new_height, new_width]),\n",
    "        method=method,\n",
    "        align_corners=align_corners)\n",
    "    image_shape = shape_utils.combined_static_and_dynamic_shape(image)\n",
    "    result = [new_image]\n",
    "    if masks is not None:\n",
    "      num_instances = tf.shape(masks)[0]\n",
    "      new_size = tf.stack([new_height, new_width])\n",
    "      def resize_masks_branch():\n",
    "        new_masks = tf.expand_dims(masks, 3)\n",
    "        new_masks = tf.image.resize_nearest_neighbor(\n",
    "            new_masks, new_size, align_corners=align_corners)\n",
    "        new_masks = tf.squeeze(new_masks, axis=3)\n",
    "        return new_masks\n",
    "\n",
    "      def reshape_masks_branch():\n",
    "        # The shape function will be computed for both branches of the\n",
    "        # condition, regardless of which branch is actually taken. Make sure\n",
    "        # that we don't trigger an assertion in the shape function when trying\n",
    "        # to reshape a non empty tensor into an empty one.\n",
    "        new_masks = tf.reshape(masks, [-1, new_size[0], new_size[1]])\n",
    "        return new_masks\n",
    "\n",
    "      masks = tf.cond(num_instances > 0, resize_masks_branch,\n",
    "                      reshape_masks_branch)\n",
    "      result.append(masks)\n",
    "\n",
    "    result.append(tf.stack([new_height, new_width, image_shape[2]]))\n",
    "    return result\n",
    "\n",
    "\n",
    "# core/preprocessor.py\n",
    "def _rgb_to_grayscale(images, name=None):\n",
    "  \"\"\"Converts one or more images from RGB to Grayscale.\n",
    "\n",
    "  Outputs a tensor of the same `DType` and rank as `images`.  The size of the\n",
    "  last dimension of the output is 1, containing the Grayscale value of the\n",
    "  pixels.\n",
    "\n",
    "  Args:\n",
    "    images: The RGB tensor to convert. Last dimension must have size 3 and\n",
    "      should contain RGB values.\n",
    "    name: A name for the operation (optional).\n",
    "\n",
    "  Returns:\n",
    "    The converted grayscale image(s).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'rgb_to_grayscale', [images]) as name:\n",
    "    images = tf.convert_to_tensor(images, name='images')\n",
    "    # Remember original dtype to so we can convert back if needed\n",
    "    orig_dtype = images.dtype\n",
    "    flt_image = tf.image.convert_image_dtype(images, tf.float32)\n",
    "\n",
    "    # Reference for converting between RGB and grayscale.\n",
    "    # https://en.wikipedia.org/wiki/Luma_%28video%29\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    rank_1 = tf.expand_dims(tf.rank(images) - 1, 0)\n",
    "    gray_float = tf.reduce_sum(\n",
    "        flt_image * rgb_weights, rank_1, keep_dims=True)\n",
    "    gray_float.set_shape(images.get_shape()[:-1].concatenate([1]))\n",
    "    return tf.image.convert_image_dtype(gray_float, orig_dtype, name=name)\n",
    "\n",
    "\n",
    "def rgb_to_gray(image):\n",
    "  \"\"\"Converts a 3 channel RGB image to a 1 channel grayscale image.\n",
    "\n",
    "  Args:\n",
    "    image: Rank 3 float32 tensor containing 1 image -> [height, width, 3]\n",
    "           with pixel values varying between [0, 1].\n",
    "\n",
    "  Returns:\n",
    "    image: A single channel grayscale image -> [image, height, 1].\n",
    "  \"\"\"\n",
    "  return _rgb_to_grayscale(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_ENCODINGS = 'box_encodings'\n",
    "CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'\n",
    "MASK_PREDICTIONS = 'mask_predictions'\n",
    "\n",
    "\n",
    "class BoxPredictor(object):\n",
    "  \"\"\"BoxPredictor.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, num_classes):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "    \"\"\"\n",
    "    self._is_training = is_training\n",
    "    self._num_classes = num_classes\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  def predict(self, image_features, num_predictions_per_location,\n",
    "              scope=None, **params):\n",
    "    \"\"\"Computes encoded object locations and corresponding confidences.\n",
    "\n",
    "    Takes a list of high level image feature maps as input and produces a list\n",
    "    of box encodings and a list of class scores where each element in the output\n",
    "    lists correspond to the feature maps in the input list.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "      width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "      scope: Variable and Op scope name.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              BoxPredictor.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing at least the following tensors.\n",
    "        box_encodings: A list of float tensors. Each entry in the list\n",
    "          corresponds to a feature map in the input `image_features` list. All\n",
    "          tensors in the list have one of the two following shapes:\n",
    "          a. [batch_size, num_anchors_i, q, code_size] representing the location\n",
    "            of the objects, where q is 1 or the number of classes.\n",
    "          b. [batch_size, num_anchors_i, code_size].\n",
    "        class_predictions_with_background: A list of float tensors of shape\n",
    "          [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "          predictions for the proposals. Each entry in the list corresponds to a\n",
    "          feature map in the input `image_features` list.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If length of `image_features` is not equal to length of\n",
    "        `num_predictions_per_location`.\n",
    "    \"\"\"\n",
    "    if len(image_features) != len(num_predictions_per_location):\n",
    "      raise ValueError('image_feature and num_predictions_per_location must '\n",
    "                       'be of same length, found: {} vs {}'.\n",
    "                       format(len(image_features),\n",
    "                              len(num_predictions_per_location)))\n",
    "    if scope is not None:\n",
    "      with tf.variable_scope(scope):\n",
    "        return self._predict(image_features, num_predictions_per_location,\n",
    "                             **params)\n",
    "    return self._predict(image_features, num_predictions_per_location,\n",
    "                         **params)\n",
    "\n",
    "  # TODO(rathodv): num_predictions_per_location could be moved to constructor.\n",
    "  # This is currently only used by ConvolutionalBoxPredictor.\n",
    "  @abstractmethod\n",
    "  def _predict(self, image_features, num_predictions_per_location, **params):\n",
    "    \"\"\"Implementations must override this method.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "        width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              BoxPredictor.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing at least the following tensors.\n",
    "        box_encodings: A list of float tensors. Each entry in the list\n",
    "          corresponds to a feature map in the input `image_features` list. All\n",
    "          tensors in the list have one of the two following shapes:\n",
    "          a. [batch_size, num_anchors_i, q, code_size] representing the location\n",
    "            of the objects, where q is 1 or the number of classes.\n",
    "          b. [batch_size, num_anchors_i, code_size].\n",
    "        class_predictions_with_background: A list of float tensors of shape\n",
    "          [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "          predictions for the proposals. Each entry in the list corresponds to a\n",
    "          feature map in the input `image_features` list.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO(rathodv): Change the implementation to return lists of predictions.\n",
    "class MaskRCNNBoxPredictor(BoxPredictor):\n",
    "  \"\"\"Mask R-CNN Box Predictor.\n",
    "\n",
    "  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).\n",
    "  Mask R-CNN. arXiv preprint arXiv:1703.06870.\n",
    "\n",
    "  This is used for the second stage of the Mask R-CNN detector where proposals\n",
    "  cropped from an image are arranged along the batch dimension of the input\n",
    "  image_features tensor. Notice that locations are *not* shared across classes,\n",
    "  thus for each anchor, a separate prediction is made for each class.\n",
    "\n",
    "  In addition to predicting boxes and classes, optionally this class allows\n",
    "  predicting masks and/or keypoints inside detection boxes.\n",
    "\n",
    "  Currently this box predictor makes per-class predictions; that is, each\n",
    "  anchor makes a separate box prediction for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               num_classes,\n",
    "               fc_hyperparams_fn,\n",
    "               use_dropout,\n",
    "               dropout_keep_prob,\n",
    "               box_code_size,\n",
    "               conv_hyperparams_fn=None,\n",
    "               predict_instance_masks=False,\n",
    "               mask_height=14,\n",
    "               mask_width=14,\n",
    "               mask_prediction_num_conv_layers=2,\n",
    "               mask_prediction_conv_depth=256,\n",
    "               masks_are_class_agnostic=False,\n",
    "               predict_keypoints=False,\n",
    "               share_box_across_classes=False):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "      fc_hyperparams_fn: A function to generate tf-slim arg_scope with\n",
    "        hyperparameters for fully connected ops.\n",
    "      use_dropout: Option to use dropout or not.  Note that a single dropout\n",
    "        op is applied here prior to both box and class predictions, which stands\n",
    "        in contrast to the ConvolutionalBoxPredictor below.\n",
    "      dropout_keep_prob: Keep probability for dropout.\n",
    "        This is only used if use_dropout is True.\n",
    "      box_code_size: Size of encoding for each box.\n",
    "      conv_hyperparams_fn: A function to generate tf-slim arg_scope with\n",
    "        hyperparameters for convolution ops.\n",
    "      predict_instance_masks: Whether to predict object masks inside detection\n",
    "        boxes.\n",
    "      mask_height: Desired output mask height. The default value is 14.\n",
    "      mask_width: Desired output mask width. The default value is 14.\n",
    "      mask_prediction_num_conv_layers: Number of convolution layers applied to\n",
    "        the image_features in mask prediction branch.\n",
    "      mask_prediction_conv_depth: The depth for the first conv2d_transpose op\n",
    "        applied to the image_features in the mask prediction branch. If set\n",
    "        to 0, the depth of the convolution layers will be automatically chosen\n",
    "        based on the number of object classes and the number of channels in the\n",
    "        image features.\n",
    "      masks_are_class_agnostic: Boolean determining if the mask-head is\n",
    "        class-agnostic or not.\n",
    "      predict_keypoints: Whether to predict keypoints insde detection boxes.\n",
    "      share_box_across_classes: Whether to share boxes across classes rather\n",
    "        than use a different box for each class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If predict_instance_masks is true but conv_hyperparams is not\n",
    "        set.\n",
    "      ValueError: If predict_keypoints is true since it is not implemented yet.\n",
    "      ValueError: If mask_prediction_num_conv_layers is smaller than two.\n",
    "    \"\"\"\n",
    "    super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)\n",
    "    self._fc_hyperparams_fn = fc_hyperparams_fn\n",
    "    self._use_dropout = use_dropout\n",
    "    self._box_code_size = box_code_size\n",
    "    self._dropout_keep_prob = dropout_keep_prob\n",
    "    self._conv_hyperparams_fn = conv_hyperparams_fn\n",
    "    self._predict_instance_masks = predict_instance_masks\n",
    "    self._mask_height = mask_height\n",
    "    self._mask_width = mask_width\n",
    "    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers\n",
    "    self._mask_prediction_conv_depth = mask_prediction_conv_depth\n",
    "    self._masks_are_class_agnostic = masks_are_class_agnostic\n",
    "    self._predict_keypoints = predict_keypoints\n",
    "    self._share_box_across_classes = share_box_across_classes\n",
    "    if self._predict_keypoints:\n",
    "      raise ValueError('Keypoint prediction is unimplemented.')\n",
    "    if ((self._predict_instance_masks or self._predict_keypoints) and\n",
    "        self._conv_hyperparams_fn is None):\n",
    "      raise ValueError('`conv_hyperparams` must be provided when predicting '\n",
    "                       'masks.')\n",
    "    if self._mask_prediction_num_conv_layers < 2:\n",
    "      raise ValueError(\n",
    "          'Mask prediction should consist of at least 2 conv layers')\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  @property\n",
    "  def predicts_instance_masks(self):\n",
    "    return self._predict_instance_masks\n",
    "\n",
    "  def _predict_boxes_and_classes(self, image_features):\n",
    "    \"\"\"Predicts boxes and class scores.\n",
    "\n",
    "    Args:\n",
    "      image_features: A float tensor of shape [batch_size, height, width,\n",
    "        channels] containing features for a batch of images.\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: A float tensor of shape\n",
    "        [batch_size, 1, num_classes, code_size] representing the location of the\n",
    "        objects.\n",
    "      class_predictions_with_background: A float tensor of shape\n",
    "        [batch_size, 1, num_classes + 1] representing the class predictions for\n",
    "        the proposals.\n",
    "    \"\"\"\n",
    "    spatial_averaged_image_features = tf.reduce_mean(image_features, [1, 2],\n",
    "                                                     keep_dims=True,\n",
    "                                                     name='AvgPool')\n",
    "    flattened_image_features = slim.flatten(spatial_averaged_image_features)\n",
    "    if self._use_dropout:\n",
    "      flattened_image_features = slim.dropout(flattened_image_features,\n",
    "                                              keep_prob=self._dropout_keep_prob,\n",
    "                                              is_training=self._is_training)\n",
    "    number_of_boxes = 1\n",
    "    if not self._share_box_across_classes:\n",
    "      number_of_boxes = self._num_classes\n",
    "\n",
    "    with slim.arg_scope(self._fc_hyperparams_fn()):\n",
    "      box_encodings = slim.fully_connected(\n",
    "          flattened_image_features,\n",
    "          number_of_boxes * self._box_code_size,\n",
    "          activation_fn=None,\n",
    "          scope='BoxEncodingPredictor')\n",
    "      class_predictions_with_background = slim.fully_connected(\n",
    "          flattened_image_features,\n",
    "          self._num_classes + 1,\n",
    "          activation_fn=None,\n",
    "          scope='ClassPredictor')\n",
    "    box_encodings = tf.reshape(\n",
    "        box_encodings, [-1, 1, number_of_boxes, self._box_code_size])\n",
    "    class_predictions_with_background = tf.reshape(\n",
    "        class_predictions_with_background, [-1, 1, self._num_classes + 1])\n",
    "    return box_encodings, class_predictions_with_background\n",
    "\n",
    "  def _get_mask_predictor_conv_depth(self, num_feature_channels, num_classes,\n",
    "                                     class_weight=3.0, feature_weight=2.0):\n",
    "    \"\"\"Computes the depth of the mask predictor convolutions.\n",
    "\n",
    "    Computes the depth of the mask predictor convolutions given feature channels\n",
    "    and number of classes by performing a weighted average of the two in\n",
    "    log space to compute the number of convolution channels. The weights that\n",
    "    are used for computing the weighted average do not need to sum to 1.\n",
    "\n",
    "    Args:\n",
    "      num_feature_channels: An integer containing the number of feature\n",
    "        channels.\n",
    "      num_classes: An integer containing the number of classes.\n",
    "      class_weight: Class weight used in computing the weighted average.\n",
    "      feature_weight: Feature weight used in computing the weighted average.\n",
    "\n",
    "    Returns:\n",
    "      An integer containing the number of convolution channels used by mask\n",
    "        predictor.\n",
    "    \"\"\"\n",
    "    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)\n",
    "    num_classes_log = math.log(float(num_classes), 2.0)\n",
    "    weighted_num_feature_channels_log = (\n",
    "        num_feature_channels_log * feature_weight)\n",
    "    weighted_num_classes_log = num_classes_log * class_weight\n",
    "    total_weight = feature_weight + class_weight\n",
    "    num_conv_channels_log = round(\n",
    "        (weighted_num_feature_channels_log + weighted_num_classes_log) /\n",
    "        total_weight)\n",
    "    return int(math.pow(2.0, num_conv_channels_log))\n",
    "\n",
    "  def _predict_masks(self, image_features):\n",
    "    \"\"\"Performs mask prediction.\n",
    "\n",
    "    Args:\n",
    "      image_features: A float tensor of shape [batch_size, height, width,\n",
    "        channels] containing features for a batch of images.\n",
    "\n",
    "    Returns:\n",
    "      instance_masks: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, image_height, image_width].\n",
    "    \"\"\"\n",
    "    num_conv_channels = self._mask_prediction_conv_depth\n",
    "    if num_conv_channels == 0:\n",
    "      num_feature_channels = image_features.get_shape().as_list()[3]\n",
    "      num_conv_channels = self._get_mask_predictor_conv_depth(\n",
    "          num_feature_channels, self.num_classes)\n",
    "    with slim.arg_scope(self._conv_hyperparams_fn()):\n",
    "      upsampled_features = tf.image.resize_bilinear(\n",
    "          image_features,\n",
    "          [self._mask_height, self._mask_width],\n",
    "          align_corners=True)\n",
    "      for _ in range(self._mask_prediction_num_conv_layers - 1):\n",
    "        upsampled_features = slim.conv2d(\n",
    "            upsampled_features,\n",
    "            num_outputs=num_conv_channels,\n",
    "            kernel_size=[3, 3])\n",
    "      num_masks = 1 if self._masks_are_class_agnostic else self.num_classes\n",
    "      mask_predictions = slim.conv2d(upsampled_features,\n",
    "                                     num_outputs=num_masks,\n",
    "                                     activation_fn=None,\n",
    "                                     kernel_size=[3, 3])\n",
    "      return tf.expand_dims(\n",
    "          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),\n",
    "          axis=1,\n",
    "          name='MaskPredictor')\n",
    "\n",
    "  def _predict(self, image_features, num_predictions_per_location,\n",
    "               predict_boxes_and_classes=True, predict_auxiliary_outputs=False):\n",
    "    \"\"\"Optionally computes encoded object locations, confidences, and masks.\n",
    "\n",
    "    Flattens image_features and applies fully connected ops (with no\n",
    "    non-linearity) to predict box encodings and class predictions.  In this\n",
    "    setting, anchors are not spatially arranged in any way and are assumed to\n",
    "    have been folded into the batch dimension.  Thus we output 1 for the\n",
    "    anchors dimension.\n",
    "\n",
    "    Also optionally predicts instance masks.\n",
    "    The mask prediction head is based on the Mask RCNN paper with the following\n",
    "    modifications: We replace the deconvolution layer with a bilinear resize\n",
    "    and a convolution.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "        width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "        Currently, this must be set to [1], or an error will be raised.\n",
    "      predict_boxes_and_classes: If true, the function will perform box\n",
    "        refinement and classification.\n",
    "      predict_auxiliary_outputs: If true, the function will perform other\n",
    "        predictions such as mask, keypoint, boundaries, etc. if any.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing the following tensors.\n",
    "        box_encodings: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, code_size] representing the\n",
    "          location of the objects.\n",
    "        class_predictions_with_background: A float tensor of shape\n",
    "          [batch_size, 1, num_classes + 1] representing the class\n",
    "          predictions for the proposals.\n",
    "      If predict_masks is True the dictionary also contains:\n",
    "        instance_masks: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, image_height, image_width]\n",
    "      If predict_keypoints is True the dictionary also contains:\n",
    "        keypoints: [batch_size, 1, num_keypoints, 2]\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If num_predictions_per_location is not 1 or if both\n",
    "        predict_boxes_and_classes and predict_auxiliary_outputs are false or if\n",
    "        len(image_features) is not 1.\n",
    "    \"\"\"\n",
    "    if (len(num_predictions_per_location) != 1 or\n",
    "        num_predictions_per_location[0] != 1):\n",
    "      raise ValueError('Currently FullyConnectedBoxPredictor only supports '\n",
    "                       'predicting a single box per class per location.')\n",
    "    if not predict_boxes_and_classes and not predict_auxiliary_outputs:\n",
    "      raise ValueError('Should perform at least one prediction.')\n",
    "    if len(image_features) != 1:\n",
    "      raise ValueError('length of `image_features` must be 1. Found {}'.\n",
    "                       format(len(image_features)))\n",
    "    image_feature = image_features[0]\n",
    "    num_predictions_per_location = num_predictions_per_location[0]\n",
    "    predictions_dict = {}\n",
    "\n",
    "    if predict_boxes_and_classes:\n",
    "      (box_encodings, class_predictions_with_background\n",
    "      ) = self._predict_boxes_and_classes(image_feature)\n",
    "      predictions_dict[BOX_ENCODINGS] = box_encodings\n",
    "      predictions_dict[\n",
    "          CLASS_PREDICTIONS_WITH_BACKGROUND] = class_predictions_with_background\n",
    "\n",
    "    if self._predict_instance_masks and predict_auxiliary_outputs:\n",
    "      predictions_dict[MASK_PREDICTIONS] = self._predict_masks(image_feature)\n",
    "\n",
    "    return predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_resize_method(resize_method):\n",
    "  \"\"\"Maps image resize method from enumeration type to TensorFlow.\n",
    "\n",
    "  Args:\n",
    "    resize_method: The resize_method attribute of keep_aspect_ratio_resizer or\n",
    "      fixed_shape_resizer.\n",
    "\n",
    "  Returns:\n",
    "    method: The corresponding TensorFlow ResizeMethod.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if `resize_method` is of unknown type.\n",
    "  \"\"\"\n",
    "  dict_method = {\n",
    "      image_resizer_pb2.BILINEAR:\n",
    "          tf.image.ResizeMethod.BILINEAR,\n",
    "      image_resizer_pb2.NEAREST_NEIGHBOR:\n",
    "          tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "      image_resizer_pb2.BICUBIC:\n",
    "          tf.image.ResizeMethod.BICUBIC,\n",
    "      image_resizer_pb2.AREA:\n",
    "          tf.image.ResizeMethod.AREA\n",
    "  }\n",
    "  if resize_method in dict_method:\n",
    "    return dict_method[resize_method]\n",
    "  else:\n",
    "    raise ValueError('Unknown resize_method')\n",
    "\n",
    "\n",
    "def image_resizer_builder(image_resizer_config):\n",
    "  \"\"\"Builds callable for image resizing operations.\n",
    "\n",
    "  Args:\n",
    "    image_resizer_config: image_resizer.proto object containing parameters for\n",
    "      an image resizing operation.\n",
    "\n",
    "  Returns:\n",
    "    image_resizer_fn: Callable for image resizing.  This callable always takes\n",
    "      a rank-3 image tensor (corresponding to a single image) and returns a\n",
    "      rank-3 image tensor, possibly with new spatial dimensions.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if `image_resizer_config` is of incorrect type.\n",
    "    ValueError: if `image_resizer_config.image_resizer_oneof` is of expected\n",
    "      type.\n",
    "    ValueError: if min_dimension > max_dimension when keep_aspect_ratio_resizer\n",
    "      is used.\n",
    "  \"\"\"\n",
    "  if not isinstance(image_resizer_config, image_resizer_pb2.ImageResizer):\n",
    "    raise ValueError('image_resizer_config not of type '\n",
    "                     'image_resizer_pb2.ImageResizer.')\n",
    "\n",
    "  image_resizer_oneof = image_resizer_config.WhichOneof('image_resizer_oneof')\n",
    "  if image_resizer_oneof == 'keep_aspect_ratio_resizer':\n",
    "    keep_aspect_ratio_config = image_resizer_config.keep_aspect_ratio_resizer\n",
    "    if not (keep_aspect_ratio_config.min_dimension <=\n",
    "            keep_aspect_ratio_config.max_dimension):\n",
    "      raise ValueError('min_dimension > max_dimension')\n",
    "    method = _tf_resize_method(keep_aspect_ratio_config.resize_method)\n",
    "    per_channel_pad_value = (0, 0, 0)\n",
    "    if keep_aspect_ratio_config.per_channel_pad_value:\n",
    "      per_channel_pad_value = tuple(keep_aspect_ratio_config.\n",
    "                                    per_channel_pad_value)\n",
    "    image_resizer_fn = functools.partial(\n",
    "        resize_to_range,\n",
    "        min_dimension=keep_aspect_ratio_config.min_dimension,\n",
    "        max_dimension=keep_aspect_ratio_config.max_dimension,\n",
    "        method=method,\n",
    "        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension,\n",
    "        per_channel_pad_value=per_channel_pad_value)\n",
    "    if not keep_aspect_ratio_config.convert_to_grayscale:\n",
    "      return image_resizer_fn\n",
    "  elif image_resizer_oneof == 'fixed_shape_resizer':\n",
    "    fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer\n",
    "    method = _tf_resize_method(fixed_shape_resizer_config.resize_method)\n",
    "    image_resizer_fn = functools.partial(\n",
    "        resize_image,\n",
    "        new_height=fixed_shape_resizer_config.height,\n",
    "        new_width=fixed_shape_resizer_config.width,\n",
    "        method=method)\n",
    "    if not fixed_shape_resizer_config.convert_to_grayscale:\n",
    "      return image_resizer_fn\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        'Invalid image resizer option: \\'%s\\'.' % image_resizer_oneof)\n",
    "\n",
    "  def grayscale_image_resizer(image):\n",
    "    [resized_image, resized_image_shape] = image_resizer_fn(image)\n",
    "    grayscale_image = rgb_to_gray(resized_image)\n",
    "    grayscale_image_shape = tf.concat([resized_image_shape[:-1], [1]], 0)\n",
    "    return [grayscale_image, grayscale_image_shape]\n",
    "\n",
    "  return functools.partial(grayscale_image_resizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets import resnet_utils\n",
    "from nets import resnet_v1\n",
    "\n",
    "def _build_faster_rcnn_feature_extractor(\n",
    "    feature_extractor_config, is_training, reuse_weights=None,\n",
    "    inplace_batchnorm_update=False):\n",
    "  \"\"\"Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Args:\n",
    "    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from\n",
    "      faster_rcnn.proto.\n",
    "    is_training: True if this feature extractor is being built for training.\n",
    "    reuse_weights: if the feature extractor should reuse weights.\n",
    "    inplace_batchnorm_update: Whether to update batch_norm inplace during\n",
    "      training. This is required for batch norm to work correctly on TPUs. When\n",
    "      this is false, user must add a control dependency on\n",
    "      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch\n",
    "      norm moving average parameters.\n",
    "\n",
    "  Returns:\n",
    "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid feature extractor type.\n",
    "  \"\"\"\n",
    "  if inplace_batchnorm_update:\n",
    "    raise ValueError('inplace batchnorm updates not supported.')\n",
    "  feature_type = feature_extractor_config.type\n",
    "  first_stage_features_stride = (\n",
    "      feature_extractor_config.first_stage_features_stride)\n",
    "  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n",
    "\n",
    "  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:\n",
    "    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(\n",
    "        feature_type))\n",
    "  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[\n",
    "      feature_type]\n",
    "  return feature_extractor_class(\n",
    "      is_training, first_stage_features_stride,\n",
    "      batch_norm_trainable, reuse_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchor_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/anchor_generator\n",
    "class AnchorGenerator(object):\n",
    "  \"\"\"Abstract base class for anchor generators.\"\"\"\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  @abstractmethod\n",
    "  def name_scope(self):\n",
    "    \"\"\"Name scope.\n",
    "\n",
    "    Must be defined by implementations.\n",
    "\n",
    "    Returns:\n",
    "      a string representing the name scope of the anchor generation operation.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def check_num_anchors(self):\n",
    "    \"\"\"Whether to dynamically check the number of anchors generated.\n",
    "\n",
    "    Can be overridden by implementations that would like to disable this\n",
    "    behavior.\n",
    "\n",
    "    Returns:\n",
    "      a boolean controlling whether the Generate function should dynamically\n",
    "      check the number of anchors generated against the mathematically\n",
    "      expected number of anchors.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "  @abstractmethod\n",
    "  def num_anchors_per_location(self):\n",
    "    \"\"\"Returns the number of anchors per spatial location.\n",
    "\n",
    "    Returns:\n",
    "      a list of integers, one for each expected feature map to be passed to\n",
    "      the `generate` function.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def generate(self, feature_map_shape_list, **params):\n",
    "    \"\"\"Generates a collection of bounding boxes to be used as anchors.\n",
    "\n",
    "    TODO(rathodv): remove **params from argument list and make stride and\n",
    "      offsets (for multiple_grid_anchor_generator) constructor arguments.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.  Pairs can be provided as 1-dimensional\n",
    "        integer tensors of length 2 or simply as tuples of integers.\n",
    "      **params: parameters for anchor generation op\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n",
    "        the input feature map shapes.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the number of feature map shapes does not match the length\n",
    "        of NumAnchorsPerLocation.\n",
    "    \"\"\"\n",
    "    if self.check_num_anchors and (\n",
    "        len(feature_map_shape_list) != len(self.num_anchors_per_location())):\n",
    "      raise ValueError('Number of feature maps is expected to equal the length '\n",
    "                       'of `num_anchors_per_location`.')\n",
    "    with tf.name_scope(self.name_scope()):\n",
    "      anchors_list = self._generate(feature_map_shape_list, **params)\n",
    "      if self.check_num_anchors:\n",
    "        with tf.control_dependencies([\n",
    "            self._assert_correct_number_of_anchors(\n",
    "                anchors_list, feature_map_shape_list)]):\n",
    "          for item in anchors_list:\n",
    "            item.set(tf.identity(item.get()))\n",
    "      return anchors_list\n",
    "\n",
    "  @abstractmethod\n",
    "  def _generate(self, feature_map_shape_list, **params):\n",
    "    \"\"\"To be overridden by implementations.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.\n",
    "      **params: parameters for anchor generation op\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxList, each holding a collection of N anchor\n",
    "        boxes.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def _assert_correct_number_of_anchors(self, anchors_list,\n",
    "                                        feature_map_shape_list):\n",
    "    \"\"\"Assert that correct number of anchors was generated.\n",
    "\n",
    "    Args:\n",
    "      anchors_list: A list of box_list.BoxList object holding anchors generated.\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.\n",
    "    Returns:\n",
    "      Op that raises InvalidArgumentError if the number of anchors does not\n",
    "        match the number of expected anchors.\n",
    "    \"\"\"\n",
    "    expected_num_anchors = 0\n",
    "    actual_num_anchors = 0\n",
    "    for num_anchors_per_location, feature_map_shape, anchors in zip(\n",
    "        self.num_anchors_per_location(), feature_map_shape_list, anchors_list):\n",
    "      expected_num_anchors += (num_anchors_per_location\n",
    "                               * feature_map_shape[0]\n",
    "                               * feature_map_shape[1])\n",
    "      actual_num_anchors += anchors.num_boxes()\n",
    "    return tf.assert_equal(expected_num_anchors, actual_num_anchors)\n",
    "\n",
    "\n",
    "\n",
    "# anchor_generator/anchor_generator.py\n",
    "class GridAnchorGenerator(AnchorGenerator):\n",
    "  \"\"\"Generates a grid of anchors at given scales and aspect ratios.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               scales=(0.5, 1.0, 2.0),\n",
    "               aspect_ratios=(0.5, 1.0, 2.0),\n",
    "               base_anchor_size=None,\n",
    "               anchor_stride=None,\n",
    "               anchor_offset=None):\n",
    "    \"\"\"Constructs a GridAnchorGenerator.\n",
    "\n",
    "    Args:\n",
    "      scales: a list of (float) scales, default=(0.5, 1.0, 2.0)\n",
    "      aspect_ratios: a list of (float) aspect ratios, default=(0.5, 1.0, 2.0)\n",
    "      base_anchor_size: base anchor size as height, width (\n",
    "                        (length-2 float32 list or tensor, default=[256, 256])\n",
    "      anchor_stride: difference in centers between base anchors for adjacent\n",
    "                     grid positions (length-2 float32 list or tensor,\n",
    "                     default=[16, 16])\n",
    "      anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n",
    "                     upper left element of the grid, this should be zero for\n",
    "                     feature networks with only VALID padding and even receptive\n",
    "                     field size, but may need additional calculation if other\n",
    "                     padding is used (length-2 float32 list or tensor,\n",
    "                     default=[0, 0])\n",
    "    \"\"\"\n",
    "    # Handle argument defaults\n",
    "    if base_anchor_size is None:\n",
    "      base_anchor_size = [256, 256]\n",
    "    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))\n",
    "    if anchor_stride is None:\n",
    "      anchor_stride = [16, 16]\n",
    "    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))\n",
    "    if anchor_offset is None:\n",
    "      anchor_offset = [0, 0]\n",
    "    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))\n",
    "\n",
    "    self._scales = scales\n",
    "    self._aspect_ratios = aspect_ratios\n",
    "    self._base_anchor_size = base_anchor_size\n",
    "    self._anchor_stride = anchor_stride\n",
    "    self._anchor_offset = anchor_offset\n",
    "\n",
    "  def name_scope(self):\n",
    "    return 'GridAnchorGenerator'\n",
    "\n",
    "  def num_anchors_per_location(self):\n",
    "    \"\"\"Returns the number of anchors per spatial location.\n",
    "\n",
    "    Returns:\n",
    "      a list of integers, one for each expected feature map to be passed to\n",
    "      the `generate` function.\n",
    "    \"\"\"\n",
    "    return [len(self._scales) * len(self._aspect_ratios)]\n",
    "\n",
    "  def _generate(self, feature_map_shape_list):\n",
    "    \"\"\"Generates a collection of bounding boxes to be used as anchors.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of pairs of convnet layer resolutions in the\n",
    "        format [(height_0, width_0)].  For example, setting\n",
    "        feature_map_shape_list=[(8, 8)] asks for anchors that correspond\n",
    "        to an 8x8 layer.  For this anchor generator, only lists of length 1 are\n",
    "        allowed.\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n",
    "        the input feature map shapes.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if feature_map_shape_list, box_specs_list do not have the same\n",
    "        length.\n",
    "      ValueError: if feature_map_shape_list does not consist of pairs of\n",
    "        integers\n",
    "    \"\"\"\n",
    "    if not (isinstance(feature_map_shape_list, list)\n",
    "            and len(feature_map_shape_list) == 1):\n",
    "      raise ValueError('feature_map_shape_list must be a list of length 1.')\n",
    "    if not all([isinstance(list_item, tuple) and len(list_item) == 2\n",
    "                for list_item in feature_map_shape_list]):\n",
    "      raise ValueError('feature_map_shape_list must be a list of pairs.')\n",
    "    grid_height, grid_width = feature_map_shape_list[0]\n",
    "    scales_grid, aspect_ratios_grid = ops.meshgrid(self._scales,\n",
    "                                                   self._aspect_ratios)\n",
    "    scales_grid = tf.reshape(scales_grid, [-1])\n",
    "    aspect_ratios_grid = tf.reshape(aspect_ratios_grid, [-1])\n",
    "    anchors = tile_anchors(grid_height,\n",
    "                           grid_width,\n",
    "                           scales_grid,\n",
    "                           aspect_ratios_grid,\n",
    "                           self._base_anchor_size,\n",
    "                           self._anchor_stride,\n",
    "                           self._anchor_offset)\n",
    "\n",
    "    num_anchors = anchors.num_boxes_static()\n",
    "    if num_anchors is None:\n",
    "      num_anchors = anchors.num_boxes()\n",
    "    anchor_indices = tf.zeros([num_anchors])\n",
    "    anchors.add_field('feature_map_index', anchor_indices)\n",
    "    return [anchors]\n",
    "\n",
    "\n",
    "def tile_anchors(grid_height,\n",
    "                 grid_width,\n",
    "                 scales,\n",
    "                 aspect_ratios,\n",
    "                 base_anchor_size,\n",
    "                 anchor_stride,\n",
    "                 anchor_offset):\n",
    "  \"\"\"Create a tiled set of anchors strided along a grid in image space.\n",
    "\n",
    "  This op creates a set of anchor boxes by placing a \"basis\" collection of\n",
    "  boxes with user-specified scales and aspect ratios centered at evenly\n",
    "  distributed points along a grid.  The basis collection is specified via the\n",
    "  scale and aspect_ratios arguments.  For example, setting scales=[.1, .2, .2]\n",
    "  and aspect ratios = [2,2,1/2] means that we create three boxes: one with scale\n",
    "  .1, aspect ratio 2, one with scale .2, aspect ratio 2, and one with scale .2\n",
    "  and aspect ratio 1/2.  Each box is multiplied by \"base_anchor_size\" before\n",
    "  placing it over its respective center.\n",
    "\n",
    "  Grid points are specified via grid_height, grid_width parameters as well as\n",
    "  the anchor_stride and anchor_offset parameters.\n",
    "\n",
    "  Args:\n",
    "    grid_height: size of the grid in the y direction (int or int scalar tensor)\n",
    "    grid_width: size of the grid in the x direction (int or int scalar tensor)\n",
    "    scales: a 1-d  (float) tensor representing the scale of each box in the\n",
    "      basis set.\n",
    "    aspect_ratios: a 1-d (float) tensor representing the aspect ratio of each\n",
    "      box in the basis set.  The length of the scales and aspect_ratios tensors\n",
    "      must be equal.\n",
    "    base_anchor_size: base anchor size as [height, width]\n",
    "      (float tensor of shape [2])\n",
    "    anchor_stride: difference in centers between base anchors for adjacent grid\n",
    "                   positions (float tensor of shape [2])\n",
    "    anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n",
    "                   upper left element of the grid, this should be zero for\n",
    "                   feature networks with only VALID padding and even receptive\n",
    "                   field size, but may need some additional calculation if other\n",
    "                   padding is used (float tensor of shape [2])\n",
    "  Returns:\n",
    "    a BoxList holding a collection of N anchor boxes\n",
    "  \"\"\"\n",
    "  ratio_sqrts = tf.sqrt(aspect_ratios)\n",
    "  heights = scales / ratio_sqrts * base_anchor_size[0]\n",
    "  widths = scales * ratio_sqrts * base_anchor_size[1]\n",
    "\n",
    "  # Get a grid of box centers\n",
    "  y_centers = tf.to_float(tf.range(grid_height))\n",
    "  y_centers = y_centers * anchor_stride[0] + anchor_offset[0]\n",
    "  x_centers = tf.to_float(tf.range(grid_width))\n",
    "  x_centers = x_centers * anchor_stride[1] + anchor_offset[1]\n",
    "  x_centers, y_centers = ops.meshgrid(x_centers, y_centers)\n",
    "\n",
    "  widths_grid, x_centers_grid = ops.meshgrid(widths, x_centers)\n",
    "  heights_grid, y_centers_grid = ops.meshgrid(heights, y_centers)\n",
    "  bbox_centers = tf.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "  bbox_sizes = tf.stack([heights_grid, widths_grid], axis=3)\n",
    "  bbox_centers = tf.reshape(bbox_centers, [-1, 2])\n",
    "  bbox_sizes = tf.reshape(bbox_sizes, [-1, 2])\n",
    "  bbox_corners = _center_size_bbox_to_corners_bbox(bbox_centers, bbox_sizes)\n",
    "  return box_list.BoxList(bbox_corners)\n",
    "\n",
    "\n",
    "def _center_size_bbox_to_corners_bbox(centers, sizes):\n",
    "  \"\"\"Converts bbox center-size representation to corners representation.\n",
    "\n",
    "  Args:\n",
    "    centers: a tensor with shape [N, 2] representing bounding box centers\n",
    "    sizes: a tensor with shape [N, 2] representing bounding boxes\n",
    "\n",
    "  Returns:\n",
    "    corners: tensor with shape [N, 4] representing bounding boxes in corners\n",
    "      representation\n",
    "  \"\"\"\n",
    "  return tf.concat([centers - .5 * sizes, centers + .5 * sizes], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builders/anchor_generator_builder.py\n",
    "def anchor_generator_builder(anchor_generator_config):\n",
    "  \"\"\"Builds an anchor generator based on the config.\n",
    "\n",
    "  Args:\n",
    "    anchor_generator_config: An anchor_generator.proto object containing the\n",
    "      config for the desired anchor generator.\n",
    "\n",
    "  Returns:\n",
    "    Anchor generator based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On empty anchor generator proto.\n",
    "  \"\"\"\n",
    "  if not isinstance(anchor_generator_config,\n",
    "                    anchor_generator_pb2.AnchorGenerator):\n",
    "    raise ValueError('anchor_generator_config not of type '\n",
    "                     'anchor_generator_pb2.AnchorGenerator')\n",
    "  if anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'grid_anchor_generator':\n",
    "    grid_anchor_generator_config = anchor_generator_config.grid_anchor_generator\n",
    "    return GridAnchorGenerator(\n",
    "        scales=[float(scale) for scale in grid_anchor_generator_config.scales],\n",
    "        aspect_ratios=[float(aspect_ratio)\n",
    "                       for aspect_ratio\n",
    "                       in grid_anchor_generator_config.aspect_ratios],\n",
    "        base_anchor_size=[grid_anchor_generator_config.height,\n",
    "                          grid_anchor_generator_config.width],\n",
    "        anchor_stride=[grid_anchor_generator_config.height_stride,\n",
    "                       grid_anchor_generator_config.width_stride],\n",
    "        anchor_offset=[grid_anchor_generator_config.height_offset,\n",
    "                       grid_anchor_generator_config.width_offset])\n",
    "  elif anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'ssd_anchor_generator':\n",
    "    ssd_anchor_generator_config = anchor_generator_config.ssd_anchor_generator\n",
    "    anchor_strides = None\n",
    "    if ssd_anchor_generator_config.height_stride:\n",
    "      anchor_strides = zip(ssd_anchor_generator_config.height_stride,\n",
    "                           ssd_anchor_generator_config.width_stride)\n",
    "    anchor_offsets = None\n",
    "    if ssd_anchor_generator_config.height_offset:\n",
    "      anchor_offsets = zip(ssd_anchor_generator_config.height_offset,\n",
    "                           ssd_anchor_generator_config.width_offset)\n",
    "    return multiple_grid_anchor_generator.create_ssd_anchors(\n",
    "        num_layers=ssd_anchor_generator_config.num_layers,\n",
    "        min_scale=ssd_anchor_generator_config.min_scale,\n",
    "        max_scale=ssd_anchor_generator_config.max_scale,\n",
    "        scales=[float(scale) for scale in ssd_anchor_generator_config.scales],\n",
    "        aspect_ratios=ssd_anchor_generator_config.aspect_ratios,\n",
    "        interpolated_scale_aspect_ratio=(\n",
    "            ssd_anchor_generator_config.interpolated_scale_aspect_ratio),\n",
    "        base_anchor_size=[\n",
    "            ssd_anchor_generator_config.base_anchor_height,\n",
    "            ssd_anchor_generator_config.base_anchor_width\n",
    "        ],\n",
    "        anchor_strides=anchor_strides,\n",
    "        anchor_offsets=anchor_offsets,\n",
    "        reduce_boxes_in_lowest_layer=(\n",
    "            ssd_anchor_generator_config.reduce_boxes_in_lowest_layer))\n",
    "  elif anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'multiscale_anchor_generator':\n",
    "    cfg = anchor_generator_config.multiscale_anchor_generator\n",
    "    return multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator(\n",
    "        cfg.min_level,\n",
    "        cfg.max_level,\n",
    "        cfg.anchor_scale,\n",
    "        [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],\n",
    "        cfg.scales_per_octave,\n",
    "        cfg.normalize_coordinates\n",
    "    )\n",
    "  else:\n",
    "    raise ValueError('Empty anchor generator.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/box_predictor.py\n",
    "\n",
    "class RfcnBoxPredictor(BoxPredictor):\n",
    "  \"\"\"RFCN Box Predictor.\n",
    "\n",
    "  Applies a position sensitive ROI pooling on position sensitive feature maps to\n",
    "  predict classes and refined locations. See https://arxiv.org/abs/1605.06409\n",
    "  for details.\n",
    "\n",
    "  This is used for the second stage of the RFCN meta architecture. Notice that\n",
    "  locations are *not* shared across classes, thus for each anchor, a separate\n",
    "  prediction is made for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               num_classes,\n",
    "               conv_hyperparams_fn,\n",
    "               num_spatial_bins,\n",
    "               depth,\n",
    "               crop_size,\n",
    "               box_code_size):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "      conv_hyperparams_fn: A function to construct tf-slim arg_scope with\n",
    "        hyperparameters for convolutional layers.\n",
    "      num_spatial_bins: A list of two integers `[spatial_bins_y,\n",
    "        spatial_bins_x]`.\n",
    "      depth: Target depth to reduce the input feature maps to.\n",
    "      crop_size: A list of two integers `[crop_height, crop_width]`.\n",
    "      box_code_size: Size of encoding for each box.\n",
    "    \"\"\"\n",
    "    super(RfcnBoxPredictor, self).__init__(is_training, num_classes)\n",
    "    self._conv_hyperparams_fn = conv_hyperparams_fn\n",
    "    self._num_spatial_bins = num_spatial_bins\n",
    "    self._depth = depth\n",
    "    self._crop_size = crop_size\n",
    "    self._box_code_size = box_code_size\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  def _predict(self, image_features, num_predictions_per_location,\n",
    "               proposal_boxes):\n",
    "    \"\"\"Computes encoded object locations and corresponding confidences.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "      width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "        Currently, this must be set to [1], or an error will be raised.\n",
    "      proposal_boxes: A float tensor of shape [batch_size, num_proposals,\n",
    "        box_code_size].\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: A list of float tensors of shape\n",
    "        [batch_size, num_anchors_i, q, code_size] representing the location of\n",
    "        the objects, where q is 1 or the number of classes. Each entry in the\n",
    "        list corresponds to a feature map in the input `image_features` list.\n",
    "      class_predictions_with_background: A list of float tensors of shape\n",
    "        [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "        predictions for the proposals. Each entry in the list corresponds to a\n",
    "        feature map in the input `image_features` list.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if num_predictions_per_location is not 1 or if\n",
    "        len(image_features) is not 1.\n",
    "    \"\"\"\n",
    "    if (len(num_predictions_per_location) != 1 or\n",
    "        num_predictions_per_location[0] != 1):\n",
    "      raise ValueError('Currently RfcnBoxPredictor only supports '\n",
    "                       'predicting a single box per class per location.')\n",
    "    if len(image_features) != 1:\n",
    "      raise ValueError('length of `image_features` must be 1. Found {}'.\n",
    "                       format(len(image_features)))\n",
    "    image_feature = image_features[0]\n",
    "    num_predictions_per_location = num_predictions_per_location[0]\n",
    "    batch_size = tf.shape(proposal_boxes)[0]\n",
    "    num_boxes = tf.shape(proposal_boxes)[1]\n",
    "    def get_box_indices(proposals):\n",
    "      proposals_shape = proposals.get_shape().as_list()\n",
    "      if any(dim is None for dim in proposals_shape):\n",
    "        proposals_shape = tf.shape(proposals)\n",
    "      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "      multiplier = tf.expand_dims(\n",
    "          tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "      return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "    net = image_feature\n",
    "    with slim.arg_scope(self._conv_hyperparams_fn()):\n",
    "      net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')\n",
    "      # Location predictions.\n",
    "      location_feature_map_depth = (self._num_spatial_bins[0] *\n",
    "                                    self._num_spatial_bins[1] *\n",
    "                                    self.num_classes *\n",
    "                                    self._box_code_size)\n",
    "      location_feature_map = slim.conv2d(net, location_feature_map_depth,\n",
    "                                         [1, 1], activation_fn=None,\n",
    "                                         scope='refined_locations')\n",
    "      box_encodings = ops.position_sensitive_crop_regions(\n",
    "          location_feature_map,\n",
    "          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n",
    "          box_ind=get_box_indices(proposal_boxes),\n",
    "          crop_size=self._crop_size,\n",
    "          num_spatial_bins=self._num_spatial_bins,\n",
    "          global_pool=True)\n",
    "      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[1, 2])\n",
    "      box_encodings = tf.reshape(box_encodings,\n",
    "                                 [batch_size * num_boxes, 1, self.num_classes,\n",
    "                                  self._box_code_size])\n",
    "\n",
    "      # Class predictions.\n",
    "      total_classes = self.num_classes + 1  # Account for background class.\n",
    "      class_feature_map_depth = (self._num_spatial_bins[0] *\n",
    "                                 self._num_spatial_bins[1] *\n",
    "                                 total_classes)\n",
    "      class_feature_map = slim.conv2d(net, class_feature_map_depth, [1, 1],\n",
    "                                      activation_fn=None,\n",
    "                                      scope='class_predictions')\n",
    "      class_predictions_with_background = ops.position_sensitive_crop_regions(\n",
    "          class_feature_map,\n",
    "          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n",
    "          box_ind=get_box_indices(proposal_boxes),\n",
    "          crop_size=self._crop_size,\n",
    "          num_spatial_bins=self._num_spatial_bins,\n",
    "          global_pool=True)\n",
    "      class_predictions_with_background = tf.squeeze(\n",
    "          class_predictions_with_background, squeeze_dims=[1, 2])\n",
    "      class_predictions_with_background = tf.reshape(\n",
    "          class_predictions_with_background,\n",
    "          [batch_size * num_boxes, 1, total_classes])\n",
    "\n",
    "    return {BOX_ENCODINGS: [box_encodings],\n",
    "            CLASS_PREDICTIONS_WITH_BACKGROUND:\n",
    "            [class_predictions_with_background]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.meta_architectures import faster_rcnn_meta_arch\n",
    "\n",
    "def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):\n",
    "  \"\"\"Builds a Faster R-CNN or R-FCN detection model based on the model config.\n",
    "\n",
    "  Builds R-FCN model if the second_stage_box_predictor in the config is of type\n",
    "  `rfcn_box_predictor` else builds a Faster R-CNN model.\n",
    "\n",
    "  Args:\n",
    "    frcnn_config: A faster_rcnn.proto object containing the config for the\n",
    "      desired FasterRCNNMetaArch or RFCNMetaArch.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tf summaries in the model.\n",
    "\n",
    "  Returns:\n",
    "    FasterRCNNMetaArch based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If frcnn_config.type is not recognized (i.e. not registered in\n",
    "      model_class_map).\n",
    "  \"\"\"\n",
    "  num_classes = frcnn_config.num_classes\n",
    "  image_resizer_fn = image_resizer_builder(frcnn_config.image_resizer)\n",
    "\n",
    "  feature_extractor = _build_faster_rcnn_feature_extractor(\n",
    "      frcnn_config.feature_extractor, is_training,\n",
    "      frcnn_config.inplace_batchnorm_update)\n",
    "\n",
    "  number_of_stages = frcnn_config.number_of_stages\n",
    "  first_stage_anchor_generator = anchor_generator_builder(\n",
    "      frcnn_config.first_stage_anchor_generator)\n",
    "\n",
    "  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate\n",
    "  first_stage_box_predictor_arg_scope_fn = hyperparams_builder(\n",
    "      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\n",
    "  first_stage_box_predictor_kernel_size = (\n",
    "      frcnn_config.first_stage_box_predictor_kernel_size)\n",
    "  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth\n",
    "  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size\n",
    "  first_stage_positive_balance_fraction = (\n",
    "      frcnn_config.first_stage_positive_balance_fraction)\n",
    "  first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold\n",
    "  first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold\n",
    "  first_stage_max_proposals = frcnn_config.first_stage_max_proposals\n",
    "  first_stage_loc_loss_weight = (\n",
    "      frcnn_config.first_stage_localization_loss_weight)\n",
    "  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight\n",
    "\n",
    "  initial_crop_size = frcnn_config.initial_crop_size\n",
    "  maxpool_kernel_size = frcnn_config.maxpool_kernel_size\n",
    "  maxpool_stride = frcnn_config.maxpool_stride\n",
    "\n",
    "  second_stage_box_predictor = box_predictor_builder(\n",
    "      hyperparams_builder,\n",
    "      frcnn_config.second_stage_box_predictor,\n",
    "      is_training=is_training,\n",
    "      num_classes=num_classes)\n",
    "  second_stage_batch_size = frcnn_config.second_stage_batch_size\n",
    "  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction\n",
    "  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn\n",
    "  ) = post_processing_builder(frcnn_config.second_stage_post_processing)\n",
    "  second_stage_localization_loss_weight = (\n",
    "      frcnn_config.second_stage_localization_loss_weight)\n",
    "  second_stage_classification_loss = (\n",
    "      build_faster_rcnn_classification_loss(\n",
    "          frcnn_config.second_stage_classification_loss))\n",
    "  second_stage_classification_loss_weight = (\n",
    "      frcnn_config.second_stage_classification_loss_weight)\n",
    "  second_stage_mask_prediction_loss_weight = (\n",
    "      frcnn_config.second_stage_mask_prediction_loss_weight)\n",
    "\n",
    "  hard_example_miner = None\n",
    "  if frcnn_config.HasField('hard_example_miner'):\n",
    "    hard_example_miner = losses_builder.build_hard_example_miner(\n",
    "        frcnn_config.hard_example_miner,\n",
    "        second_stage_classification_loss_weight,\n",
    "        second_stage_localization_loss_weight)\n",
    "\n",
    "  common_kwargs = {\n",
    "      'is_training': is_training,\n",
    "      'num_classes': num_classes,\n",
    "      'image_resizer_fn': image_resizer_fn,\n",
    "      'feature_extractor': feature_extractor,\n",
    "      'number_of_stages': number_of_stages,\n",
    "      'first_stage_anchor_generator': first_stage_anchor_generator,\n",
    "      'first_stage_atrous_rate': first_stage_atrous_rate,\n",
    "      'first_stage_box_predictor_arg_scope_fn':\n",
    "      first_stage_box_predictor_arg_scope_fn,\n",
    "      'first_stage_box_predictor_kernel_size':\n",
    "      first_stage_box_predictor_kernel_size,\n",
    "      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,\n",
    "      'first_stage_minibatch_size': first_stage_minibatch_size,\n",
    "      'first_stage_positive_balance_fraction':\n",
    "      first_stage_positive_balance_fraction,\n",
    "      'first_stage_nms_score_threshold': first_stage_nms_score_threshold,\n",
    "      'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,\n",
    "      'first_stage_max_proposals': first_stage_max_proposals,\n",
    "      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,\n",
    "      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,\n",
    "      'second_stage_batch_size': second_stage_batch_size,\n",
    "      'second_stage_balance_fraction': second_stage_balance_fraction,\n",
    "      'second_stage_non_max_suppression_fn':\n",
    "      second_stage_non_max_suppression_fn,\n",
    "      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,\n",
    "      'second_stage_localization_loss_weight':\n",
    "      second_stage_localization_loss_weight,\n",
    "      'second_stage_classification_loss':\n",
    "      second_stage_classification_loss,\n",
    "      'second_stage_classification_loss_weight':\n",
    "      second_stage_classification_loss_weight,\n",
    "      'hard_example_miner': hard_example_miner,\n",
    "      'add_summaries': add_summaries}\n",
    "\n",
    "  if isinstance(second_stage_box_predictor, RfcnBoxPredictor):\n",
    "    return rfcn_meta_arch.RFCNMetaArch(\n",
    "        second_stage_rfcn_box_predictor=second_stage_box_predictor,\n",
    "        **common_kwargs)\n",
    "  else:\n",
    "    return faster_rcnn_meta_arch.FasterRCNNMetaArch(\n",
    "        initial_crop_size=initial_crop_size,\n",
    "        maxpool_kernel_size=maxpool_kernel_size,\n",
    "        maxpool_stride=maxpool_stride,\n",
    "        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,\n",
    "        second_stage_mask_prediction_loss_weight=(\n",
    "            second_stage_mask_prediction_loss_weight),\n",
    "        **common_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(model_config, is_training, add_summaries=True,\n",
    "          add_background_class=True):\n",
    "  \"\"\"Builds a DetectionModel based on the model config.\n",
    "\n",
    "  Args:\n",
    "    model_config: A model.proto object containing the config for the desired\n",
    "      DetectionModel.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tensorflow summaries in the model graph.\n",
    "    add_background_class: Whether to add an implicit background class to one-hot\n",
    "      encodings of groundtruth labels. Set to false if using groundtruth labels\n",
    "      with an explicit background class or using multiclass scores instead of\n",
    "      truth in the case of distillation. Ignored in the case of faster_rcnn.\n",
    "  Returns:\n",
    "    DetectionModel based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid meta architecture or model.\n",
    "  \"\"\"\n",
    "  if not isinstance(model_config, model_pb2.DetectionModel):\n",
    "    raise ValueError('model_config not of type model_pb2.DetectionModel.')\n",
    "  meta_architecture = model_config.WhichOneof('model')\n",
    "  if meta_architecture == 'ssd':\n",
    "    return _build_ssd_model(model_config.ssd, is_training, add_summaries,\n",
    "                            add_background_class)\n",
    "  if meta_architecture == 'faster_rcnn':\n",
    "    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n",
    "                                    add_summaries)\n",
    "  raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = functools.partial(\n",
    "    model_builder,\n",
    "    model_config=model_config,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(config):\n",
    "    return dataset_builder.make_initializable_iterator(\n",
    "        dataset_builder.build(config)).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_input_dict_fn = functools.partial(get_next, input_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "cluster_data = env.get('cluster', None)\n",
    "cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
    "task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
    "task_info = type('TaskSpec', (object,), task_data)\n",
    "\n",
    "# Parameters for a single worker.\n",
    "ps_tasks = 0\n",
    "worker_replicas = 1\n",
    "worker_job_name = 'lonely_worker'\n",
    "task = 0\n",
    "is_chief = True\n",
    "master = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_data and 'worker' in cluster_data:\n",
    "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
    "    worker_replicas = len(cluster_data['worker']) + 1\n",
    "if cluster_data and 'ps' in cluster_data:\n",
    "    ps_tasks = len(cluster_data['ps'])\n",
    "\n",
    "if worker_replicas > 1 and ps_tasks < 1:\n",
    "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
    "\n",
    "if worker_replicas >= 1 and ps_tasks > 0:\n",
    "    # Set up distributed training.\n",
    "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
    "                             job_name=task_info.type,\n",
    "                             task_index=task_info.index)\n",
    "    if task_info.type == 'ps':\n",
    "        server.join()\n",
    "        raise (\"end\")  # It was 'return' of main()\n",
    "\n",
    "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
    "    task = task_info.index\n",
    "    is_chief = (task_info.type == 'master')\n",
    "    master = server.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rewriter_fn = None\n",
    "if 'graph_rewriter_config' in configs:\n",
    "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
    "        configs['graph_rewriter_config'], is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_builder\n",
    "# hyperparams_builder\n",
    "def hyperparams_builder(hyperparams_config, is_training):\n",
    "  if not isinstance(hyperparams_config,\n",
    "                    hyperparams_pb2.Hyperparams):\n",
    "    raise ValueError('hyperparams_config not of type '\n",
    "                     'hyperparams_pb.Hyperparams.')\n",
    "\n",
    "  batch_norm = None\n",
    "  batch_norm_params = None\n",
    "  if hyperparams_config.HasField('batch_norm'):\n",
    "    batch_norm = slim.batch_norm\n",
    "    batch_norm_params = _build_batch_norm_params(\n",
    "        hyperparams_config.batch_norm, is_training)\n",
    "\n",
    "  affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]\n",
    "  if hyperparams_config.HasField('op') and (\n",
    "      hyperparams_config.op == hyperparams_pb2.Hyperparams.FC):\n",
    "    affected_ops = [slim.fully_connected]\n",
    "  def scope_fn():\n",
    "    with (slim.arg_scope([slim.batch_norm], **batch_norm_params)\n",
    "          if batch_norm_params is not None else\n",
    "          context_manager.IdentityContextManager()):\n",
    "      with slim.arg_scope(\n",
    "          affected_ops,\n",
    "          weights_regularizer=_build_regularizer(\n",
    "              hyperparams_config.regularizer),\n",
    "          weights_initializer=_build_initializer(\n",
    "              hyperparams_config.initializer),\n",
    "          activation_fn=_build_activation_fn(hyperparams_config.activation),\n",
    "          normalizer_fn=batch_norm) as sc:\n",
    "        return sc\n",
    "\n",
    "  return scope_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor_builder(argscope_fn, box_predictor_config, is_training, num_classes):\n",
    "  \"\"\"Builds box predictor based on the configuration.\n",
    "\n",
    "  Builds box predictor based on the configuration. See box_predictor.proto for\n",
    "  configurable options. Also, see box_predictor.py for more details.\n",
    "\n",
    "  Args:\n",
    "    argscope_fn: A function that takes the following inputs:\n",
    "        * hyperparams_pb2.Hyperparams proto\n",
    "        * a boolean indicating if the model is in training mode.\n",
    "      and returns a tf slim argscope for Conv and FC hyperparameters.\n",
    "    box_predictor_config: box_predictor_pb2.BoxPredictor proto containing\n",
    "      configuration.\n",
    "    is_training: Whether the models is in training mode.\n",
    "    num_classes: Number of classes to predict.\n",
    "\n",
    "  Returns:\n",
    "    box_predictor: box_predictor.BoxPredictor object.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On unknown box predictor.\n",
    "  \"\"\"\n",
    "  if not isinstance(box_predictor_config, box_predictor_pb2.BoxPredictor):\n",
    "    raise ValueError('box_predictor_config not of type '\n",
    "                     'box_predictor_pb2.BoxPredictor.')\n",
    "\n",
    "  box_predictor_oneof = box_predictor_config.WhichOneof('box_predictor_oneof')\n",
    "\n",
    "  if  box_predictor_oneof == 'convolutional_box_predictor':\n",
    "    conv_box_predictor = box_predictor_config.convolutional_box_predictor\n",
    "    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.ConvolutionalBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        min_depth=conv_box_predictor.min_depth,\n",
    "        max_depth=conv_box_predictor.max_depth,\n",
    "        num_layers_before_predictor=(conv_box_predictor.\n",
    "                                     num_layers_before_predictor),\n",
    "        use_dropout=conv_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,\n",
    "        kernel_size=conv_box_predictor.kernel_size,\n",
    "        box_code_size=conv_box_predictor.box_code_size,\n",
    "        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,\n",
    "        class_prediction_bias_init=(conv_box_predictor.\n",
    "                                    class_prediction_bias_init),\n",
    "        use_depthwise=conv_box_predictor.use_depthwise\n",
    "    )\n",
    "    return box_predictor_object\n",
    "\n",
    "  if  box_predictor_oneof == 'weight_shared_convolutional_box_predictor':\n",
    "    conv_box_predictor = (box_predictor_config.\n",
    "                          weight_shared_convolutional_box_predictor)\n",
    "    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.WeightSharedConvolutionalBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        depth=conv_box_predictor.depth,\n",
    "        num_layers_before_predictor=(\n",
    "            conv_box_predictor.num_layers_before_predictor),\n",
    "        kernel_size=conv_box_predictor.kernel_size,\n",
    "        box_code_size=conv_box_predictor.box_code_size,\n",
    "        class_prediction_bias_init=conv_box_predictor.\n",
    "        class_prediction_bias_init,\n",
    "        use_dropout=conv_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=conv_box_predictor.dropout_keep_probability)\n",
    "    return box_predictor_object\n",
    "\n",
    "  if box_predictor_oneof == 'mask_rcnn_box_predictor':\n",
    "    mask_rcnn_box_predictor = box_predictor_config.mask_rcnn_box_predictor\n",
    "    fc_hyperparams_fn = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,\n",
    "                                    is_training)\n",
    "    conv_hyperparams_fn = None\n",
    "    if mask_rcnn_box_predictor.HasField('conv_hyperparams'):\n",
    "      conv_hyperparams_fn = argscope_fn(\n",
    "          mask_rcnn_box_predictor.conv_hyperparams, is_training)\n",
    "    box_predictor_object = MaskRCNNBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        fc_hyperparams_fn=fc_hyperparams_fn,\n",
    "        use_dropout=mask_rcnn_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=mask_rcnn_box_predictor.dropout_keep_probability,\n",
    "        box_code_size=mask_rcnn_box_predictor.box_code_size,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        predict_instance_masks=mask_rcnn_box_predictor.predict_instance_masks,\n",
    "        mask_height=mask_rcnn_box_predictor.mask_height,\n",
    "        mask_width=mask_rcnn_box_predictor.mask_width,\n",
    "        mask_prediction_num_conv_layers=(\n",
    "            mask_rcnn_box_predictor.mask_prediction_num_conv_layers),\n",
    "        mask_prediction_conv_depth=(\n",
    "            mask_rcnn_box_predictor.mask_prediction_conv_depth),\n",
    "        masks_are_class_agnostic=(\n",
    "            mask_rcnn_box_predictor.masks_are_class_agnostic),\n",
    "        predict_keypoints=mask_rcnn_box_predictor.predict_keypoints,\n",
    "        share_box_across_classes=(\n",
    "            mask_rcnn_box_predictor.share_box_across_classes))\n",
    "    return box_predictor_object\n",
    "\n",
    "  if box_predictor_oneof == 'rfcn_box_predictor':\n",
    "    rfcn_box_predictor = box_predictor_config.rfcn_box_predictor\n",
    "    conv_hyperparams_fn = argscope_fn(rfcn_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.RfcnBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        crop_size=[rfcn_box_predictor.crop_height,\n",
    "                   rfcn_box_predictor.crop_width],\n",
    "        num_spatial_bins=[rfcn_box_predictor.num_spatial_bins_height,\n",
    "                          rfcn_box_predictor.num_spatial_bins_width],\n",
    "        depth=rfcn_box_predictor.depth,\n",
    "        box_code_size=rfcn_box_predictor.box_code_size)\n",
    "    return box_predictor_object\n",
    "  raise ValueError('Unknown box predictor: {}'.format(box_predictor_oneof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder/post_processing_builder.py\n",
    "def post_processing_builder(post_processing_config):\n",
    "  \"\"\"Builds callables for post-processing operations.\n",
    "\n",
    "  Builds callables for non-max suppression and score conversion based on the\n",
    "  configuration.\n",
    "\n",
    "  Non-max suppression callable takes `boxes`, `scores`, and optionally\n",
    "  `clip_window`, `parallel_iterations` `masks, and `scope` as inputs. It returns\n",
    "  `nms_boxes`, `nms_scores`, `nms_classes` `nms_masks` and `num_detections`. See\n",
    "  post_processing.batch_multiclass_non_max_suppression for the type and shape\n",
    "  of these tensors.\n",
    "\n",
    "  Score converter callable should be called with `input` tensor. The callable\n",
    "  returns the output from one of 3 tf operations based on the configuration -\n",
    "  tf.identity, tf.sigmoid or tf.nn.softmax. See tensorflow documentation for\n",
    "  argument and return value descriptions.\n",
    "\n",
    "  Args:\n",
    "    post_processing_config: post_processing.proto object containing the\n",
    "      parameters for the post-processing operations.\n",
    "\n",
    "  Returns:\n",
    "    non_max_suppressor_fn: Callable for non-max suppression.\n",
    "    score_converter_fn: Callable for score conversion.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the post_processing_config is of incorrect type.\n",
    "  \"\"\"\n",
    "  if not isinstance(post_processing_config, post_processing_pb2.PostProcessing):\n",
    "    raise ValueError('post_processing_config not of type '\n",
    "                     'post_processing_pb2.Postprocessing.')\n",
    "  non_max_suppressor_fn = _build_non_max_suppressor(\n",
    "      post_processing_config.batch_non_max_suppression)\n",
    "  score_converter_fn = _build_score_converter(\n",
    "      post_processing_config.score_converter,\n",
    "      post_processing_config.logit_scale)\n",
    "  return non_max_suppressor_fn, score_converter_fn\n",
    "\n",
    "\n",
    "def _build_non_max_suppressor(nms_config):\n",
    "  \"\"\"Builds non-max suppresson based on the nms config.\n",
    "\n",
    "  Args:\n",
    "    nms_config: post_processing_pb2.PostProcessing.BatchNonMaxSuppression proto.\n",
    "\n",
    "  Returns:\n",
    "    non_max_suppressor_fn: Callable non-max suppressor.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On incorrect iou_threshold or on incompatible values of\n",
    "      max_total_detections and max_detections_per_class.\n",
    "  \"\"\"\n",
    "  if nms_config.iou_threshold < 0 or nms_config.iou_threshold > 1.0:\n",
    "    raise ValueError('iou_threshold not in [0, 1.0].')\n",
    "  if nms_config.max_detections_per_class > nms_config.max_total_detections:\n",
    "    raise ValueError('max_detections_per_class should be no greater than '\n",
    "                     'max_total_detections.')\n",
    "\n",
    "  non_max_suppressor_fn = functools.partial(\n",
    "      batch_multiclass_non_max_suppression,\n",
    "      score_thresh=nms_config.score_threshold,\n",
    "      iou_thresh=nms_config.iou_threshold,\n",
    "      max_size_per_class=nms_config.max_detections_per_class,\n",
    "      max_total_size=nms_config.max_total_detections)\n",
    "  return non_max_suppressor_fn\n",
    "\n",
    "\n",
    "def batch_multiclass_non_max_suppression(boxes,\n",
    "                                         scores,\n",
    "                                         score_thresh,\n",
    "                                         iou_thresh,\n",
    "                                         max_size_per_class,\n",
    "                                         max_total_size=0,\n",
    "                                         clip_window=None,\n",
    "                                         change_coordinate_frame=False,\n",
    "                                         num_valid_boxes=None,\n",
    "                                         masks=None,\n",
    "                                         additional_fields=None,\n",
    "                                         scope=None,\n",
    "                                         parallel_iterations=32):\n",
    "  q = boxes.shape[2].value\n",
    "  num_classes = scores.shape[2].value\n",
    "  if q != 1 and q != num_classes:\n",
    "    raise ValueError('third dimension of boxes must be either 1 or equal '\n",
    "                     'to the third dimension of scores')\n",
    "  if change_coordinate_frame and clip_window is None:\n",
    "    raise ValueError('if change_coordinate_frame is True, then a clip_window'\n",
    "                     'must be specified.')\n",
    "  original_masks = masks\n",
    "  original_additional_fields = additional_fields\n",
    "  with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):\n",
    "    boxes_shape = boxes.shape\n",
    "    batch_size = boxes_shape[0].value\n",
    "    num_anchors = boxes_shape[1].value\n",
    "\n",
    "    if batch_size is None:\n",
    "      batch_size = tf.shape(boxes)[0]\n",
    "    if num_anchors is None:\n",
    "      num_anchors = tf.shape(boxes)[1]\n",
    "\n",
    "    # If num valid boxes aren't provided, create one and mark all boxes as\n",
    "    # valid.\n",
    "    if num_valid_boxes is None:\n",
    "      num_valid_boxes = tf.ones([batch_size], dtype=tf.int32) * num_anchors\n",
    "\n",
    "    # If masks aren't provided, create dummy masks so we can only have one copy\n",
    "    # of _single_image_nms_fn and discard the dummy masks after map_fn.\n",
    "    if masks is None:\n",
    "      masks_shape = tf.stack([batch_size, num_anchors, 1, 0, 0])\n",
    "      masks = tf.zeros(masks_shape)\n",
    "\n",
    "    if clip_window is None:\n",
    "      clip_window = tf.stack([\n",
    "          tf.reduce_min(boxes[:, :, :, 0]),\n",
    "          tf.reduce_min(boxes[:, :, :, 1]),\n",
    "          tf.reduce_max(boxes[:, :, :, 2]),\n",
    "          tf.reduce_max(boxes[:, :, :, 3])\n",
    "      ])\n",
    "    if clip_window.shape.ndims == 1:\n",
    "      clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])\n",
    "\n",
    "    if additional_fields is None:\n",
    "      additional_fields = {}\n",
    "\n",
    "    def _single_image_nms_fn(args):\n",
    "      \"\"\"Runs NMS on a single image and returns padded output.\n",
    "\n",
    "      Args:\n",
    "        args: A list of tensors consisting of the following:\n",
    "          per_image_boxes - A [num_anchors, q, 4] float32 tensor containing\n",
    "            detections. If `q` is 1 then same boxes are used for all classes\n",
    "            otherwise, if `q` is equal to number of classes, class-specific\n",
    "            boxes are used.\n",
    "          per_image_scores - A [num_anchors, num_classes] float32 tensor\n",
    "            containing the scores for each of the `num_anchors` detections.\n",
    "          per_image_masks - A [num_anchors, q, mask_height, mask_width] float32\n",
    "            tensor containing box masks. `q` can be either number of classes\n",
    "            or 1 depending on whether a separate mask is predicted per class.\n",
    "          per_image_clip_window - A 1D float32 tensor of the form\n",
    "            [ymin, xmin, ymax, xmax] representing the window to clip the boxes\n",
    "            to.\n",
    "          per_image_additional_fields - (optional) A variable number of float32\n",
    "            tensors each with size [num_anchors, ...].\n",
    "          per_image_num_valid_boxes - A tensor of type `int32`. A 1-D tensor of\n",
    "            shape [batch_size] representing the number of valid boxes to be\n",
    "            considered for each image in the batch.  This parameter allows for\n",
    "            ignoring zero paddings.\n",
    "\n",
    "      Returns:\n",
    "        'nmsed_boxes': A [max_detections, 4] float32 tensor containing the\n",
    "          non-max suppressed boxes.\n",
    "        'nmsed_scores': A [max_detections] float32 tensor containing the scores\n",
    "          for the boxes.\n",
    "        'nmsed_classes': A [max_detections] float32 tensor containing the class\n",
    "          for boxes.\n",
    "        'nmsed_masks': (optional) a [max_detections, mask_height, mask_width]\n",
    "          float32 tensor containing masks for each selected box. This is set to\n",
    "          None if input `masks` is None.\n",
    "        'nmsed_additional_fields':  (optional) A variable number of float32\n",
    "          tensors each with size [max_detections, ...] corresponding to the\n",
    "          input `per_image_additional_fields`.\n",
    "        'num_detections': A [batch_size] int32 tensor indicating the number of\n",
    "          valid detections per batch item. Only the top num_detections[i]\n",
    "          entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The\n",
    "          rest of the entries are zero paddings.\n",
    "      \"\"\"\n",
    "      per_image_boxes = args[0]\n",
    "      per_image_scores = args[1]\n",
    "      per_image_masks = args[2]\n",
    "      per_image_clip_window = args[3]\n",
    "      per_image_additional_fields = {\n",
    "          key: value\n",
    "          for key, value in zip(additional_fields, args[4:-1])\n",
    "      }\n",
    "      per_image_num_valid_boxes = args[-1]\n",
    "      per_image_boxes = tf.reshape(\n",
    "          tf.slice(per_image_boxes, 3 * [0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1, -1])), [-1, q, 4])\n",
    "      per_image_scores = tf.reshape(\n",
    "          tf.slice(per_image_scores, [0, 0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1])),\n",
    "          [-1, num_classes])\n",
    "      per_image_masks = tf.reshape(\n",
    "          tf.slice(per_image_masks, 4 * [0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1, -1, -1])),\n",
    "          [-1, q, per_image_masks.shape[2].value,\n",
    "           per_image_masks.shape[3].value])\n",
    "      if per_image_additional_fields is not None:\n",
    "        for key, tensor in per_image_additional_fields.items():\n",
    "          additional_field_shape = tensor.get_shape()\n",
    "          additional_field_dim = len(additional_field_shape)\n",
    "          per_image_additional_fields[key] = tf.reshape(\n",
    "              tf.slice(per_image_additional_fields[key],\n",
    "                       additional_field_dim * [0],\n",
    "                       tf.stack([per_image_num_valid_boxes] +\n",
    "                                (additional_field_dim - 1) * [-1])),\n",
    "              [-1] + [dim.value for dim in additional_field_shape[1:]])\n",
    "      nmsed_boxlist = multiclass_non_max_suppression(\n",
    "          per_image_boxes,\n",
    "          per_image_scores,\n",
    "          score_thresh,\n",
    "          iou_thresh,\n",
    "          max_size_per_class,\n",
    "          max_total_size,\n",
    "          clip_window=per_image_clip_window,\n",
    "          change_coordinate_frame=change_coordinate_frame,\n",
    "          masks=per_image_masks,\n",
    "          additional_fields=per_image_additional_fields)\n",
    "      padded_boxlist = box_list_ops.pad_or_clip_box_list(nmsed_boxlist,\n",
    "                                                         max_total_size)\n",
    "      num_detections = nmsed_boxlist.num_boxes()\n",
    "      nmsed_boxes = padded_boxlist.get()\n",
    "      nmsed_scores = padded_boxlist.get_field(fields.BoxListFields.scores)\n",
    "      nmsed_classes = padded_boxlist.get_field(fields.BoxListFields.classes)\n",
    "      nmsed_masks = padded_boxlist.get_field(fields.BoxListFields.masks)\n",
    "      nmsed_additional_fields = [\n",
    "          padded_boxlist.get_field(key) for key in per_image_additional_fields\n",
    "      ]\n",
    "      return ([nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks] +\n",
    "              nmsed_additional_fields + [num_detections])\n",
    "\n",
    "    num_additional_fields = 0\n",
    "    if additional_fields is not None:\n",
    "      num_additional_fields = len(additional_fields)\n",
    "    num_nmsed_outputs = 4 + num_additional_fields\n",
    "\n",
    "    batch_outputs = shape_utils.static_or_dynamic_map_fn(\n",
    "        _single_image_nms_fn,\n",
    "        elems=([boxes, scores, masks, clip_window] +\n",
    "               list(additional_fields.values()) + [num_valid_boxes]),\n",
    "        dtype=(num_nmsed_outputs * [tf.float32] + [tf.int32]),\n",
    "        parallel_iterations=parallel_iterations)\n",
    "\n",
    "    batch_nmsed_boxes = batch_outputs[0]\n",
    "    batch_nmsed_scores = batch_outputs[1]\n",
    "    batch_nmsed_classes = batch_outputs[2]\n",
    "    batch_nmsed_masks = batch_outputs[3]\n",
    "    batch_nmsed_additional_fields = {\n",
    "        key: value\n",
    "        for key, value in zip(additional_fields, batch_outputs[4:-1])\n",
    "    }\n",
    "    batch_num_detections = batch_outputs[-1]\n",
    "\n",
    "    if original_masks is None:\n",
    "      batch_nmsed_masks = None\n",
    "\n",
    "    if original_additional_fields is None:\n",
    "      batch_nmsed_additional_fields = None\n",
    "\n",
    "    return (batch_nmsed_boxes, batch_nmsed_scores, batch_nmsed_classes,\n",
    "            batch_nmsed_masks, batch_nmsed_additional_fields,\n",
    "            batch_num_detections)\n",
    "\n",
    "\n",
    "def _score_converter_fn_with_logit_scale(tf_score_converter_fn, logit_scale):\n",
    "  \"\"\"Create a function to scale logits then apply a Tensorflow function.\"\"\"\n",
    "  def score_converter_fn(logits):\n",
    "    scaled_logits = tf.divide(logits, logit_scale, name='scale_logits')\n",
    "    return tf_score_converter_fn(scaled_logits, name='convert_scores')\n",
    "  score_converter_fn.__name__ = '%s_with_logit_scale' % (\n",
    "      tf_score_converter_fn.__name__)\n",
    "  return score_converter_fn\n",
    "\n",
    "\n",
    "\n",
    "def _build_score_converter(score_converter_config, logit_scale):\n",
    "  \"\"\"Builds score converter based on the config.\n",
    "\n",
    "  Builds one of [tf.identity, tf.sigmoid, tf.softmax] score converters based on\n",
    "  the config.\n",
    "\n",
    "  Args:\n",
    "    score_converter_config: post_processing_pb2.PostProcessing.score_converter.\n",
    "    logit_scale: temperature to use for SOFTMAX score_converter.\n",
    "\n",
    "  Returns:\n",
    "    Callable score converter op.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On unknown score converter.\n",
    "  \"\"\"\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.IDENTITY:\n",
    "    return _score_converter_fn_with_logit_scale(tf.identity, logit_scale)\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.SIGMOID:\n",
    "    return _score_converter_fn_with_logit_scale(tf.sigmoid, logit_scale)\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.SOFTMAX:\n",
    "    return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)\n",
    "  raise ValueError('Unknown score converter.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_builder.py\n",
    "def build_faster_rcnn_classification_loss(loss_config):\n",
    "  \"\"\"Builds a classification loss for Faster RCNN based on the loss config.\n",
    "\n",
    "  Args:\n",
    "    loss_config: A losses_pb2.ClassificationLoss object.\n",
    "\n",
    "  Returns:\n",
    "    Loss based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid loss_config.\n",
    "  \"\"\"\n",
    "  if not isinstance(loss_config, losses_pb2.ClassificationLoss):\n",
    "    raise ValueError('loss_config not of type losses_pb2.ClassificationLoss.')\n",
    "\n",
    "  loss_type = loss_config.WhichOneof('classification_loss')\n",
    "\n",
    "  if loss_type == 'weighted_sigmoid':\n",
    "    return losses.WeightedSigmoidClassificationLoss()\n",
    "  if loss_type == 'weighted_softmax':\n",
    "    config = loss_config.weighted_softmax\n",
    "    return WeightedSoftmaxClassificationLoss(\n",
    "        logit_scale=config.logit_scale)\n",
    "  if loss_type == 'weighted_logits_softmax':\n",
    "    config = loss_config.weighted_logits_softmax\n",
    "    return losses.WeightedSoftmaxClassificationAgainstLogitsLoss(\n",
    "        logit_scale=config.logit_scale)\n",
    "\n",
    "  # By default, Faster RCNN second stage classifier uses Softmax loss\n",
    "  # with anchor-wise outputs.\n",
    "  config = loss_config.weighted_softmax\n",
    "  return WeightedSoftmaxClassificationLoss(\n",
    "      logit_scale=config.logit_scale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_hard_example_miner(config,\n",
    "                             classification_weight,\n",
    "                             localization_weight):\n",
    "  \"\"\"Builds hard example miner based on the config.\n",
    "\n",
    "  Args:\n",
    "    config: A losses_pb2.HardExampleMiner object.\n",
    "    classification_weight: Classification loss weight.\n",
    "    localization_weight: Localization loss weight.\n",
    "\n",
    "  Returns:\n",
    "    Hard example miner.\n",
    "\n",
    "  \"\"\"\n",
    "  loss_type = None\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.BOTH:\n",
    "    loss_type = 'both'\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.CLASSIFICATION:\n",
    "    loss_type = 'cls'\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.LOCALIZATION:\n",
    "    loss_type = 'loc'\n",
    "\n",
    "  max_negatives_per_positive = None\n",
    "  num_hard_examples = None\n",
    "  if config.max_negatives_per_positive > 0:\n",
    "    max_negatives_per_positive = config.max_negatives_per_positive\n",
    "  if config.num_hard_examples > 0:\n",
    "    num_hard_examples = config.num_hard_examples\n",
    "  hard_example_miner = losses.HardExampleMiner(\n",
    "      num_hard_examples=num_hard_examples,\n",
    "      iou_threshold=config.iou_threshold,\n",
    "      loss_type=loss_type,\n",
    "      cls_loss_weight=classification_weight,\n",
    "      loc_loss_weight=localization_weight,\n",
    "      max_negatives_per_positive=max_negatives_per_positive,\n",
    "      min_negatives_per_image=config.min_negatives_per_image)\n",
    "  return hard_example_miner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/losses.py\n",
    "class Loss(object):\n",
    "  \"\"\"Abstract base class for loss functions.\"\"\"\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  def __call__(self,\n",
    "               prediction_tensor,\n",
    "               target_tensor,\n",
    "               ignore_nan_targets=False,\n",
    "               scope=None,\n",
    "               **params):\n",
    "    \"\"\"Call the loss function.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]\n",
    "        representing predicted quantities.\n",
    "      target_tensor: an N-d tensor of shape [batch, anchors, ...] representing\n",
    "        regression or classification targets.\n",
    "      ignore_nan_targets: whether to ignore nan targets in the loss computation.\n",
    "        E.g. can be used if the target tensor is missing groundtruth data that\n",
    "        shouldn't be factored into the loss.\n",
    "      scope: Op scope name. Defaults to 'Loss' if None.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              the Loss.\n",
    "\n",
    "    Returns:\n",
    "      loss: a tensor representing the value of the loss function.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'Loss',\n",
    "                       [prediction_tensor, target_tensor, params]) as scope:\n",
    "      if ignore_nan_targets:\n",
    "        target_tensor = tf.where(tf.is_nan(target_tensor),\n",
    "                                 prediction_tensor,\n",
    "                                 target_tensor)\n",
    "      return self._compute_loss(prediction_tensor, target_tensor, **params)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _compute_loss(self, prediction_tensor, target_tensor, **params):\n",
    "    \"\"\"Method to be overridden by implementations.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: a tensor representing predicted quantities\n",
    "      target_tensor: a tensor representing regression or classification targets\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              the Loss.\n",
    "\n",
    "    Returns:\n",
    "      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per\n",
    "        anchor\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class WeightedSoftmaxClassificationLoss(Loss):\n",
    "  \"\"\"Softmax loss function.\"\"\"\n",
    "\n",
    "  def __init__(self, logit_scale=1.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      logit_scale: When this value is high, the prediction is \"diffused\" and\n",
    "                   when this value is low, the prediction is made peakier.\n",
    "                   (default 1.0)\n",
    "\n",
    "    \"\"\"\n",
    "    self._logit_scale = logit_scale\n",
    "\n",
    "  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n",
    "    \"\"\"Compute loss function.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "      target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "      weights: a float tensor of shape [batch_size, num_anchors]\n",
    "\n",
    "    Returns:\n",
    "      loss: a float tensor of shape [batch_size, num_anchors]\n",
    "        representing the value of the loss function.\n",
    "    \"\"\"\n",
    "    num_classes = prediction_tensor.get_shape().as_list()[-1]\n",
    "    prediction_tensor = tf.divide(\n",
    "        prediction_tensor, self._logit_scale, name='scale_logit')\n",
    "    per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.reshape(target_tensor, [-1, num_classes]),\n",
    "        logits=tf.reshape(prediction_tensor, [-1, num_classes])))\n",
    "    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensor_dict_fn = create_input_dict_fn\n",
    "create_model_fn = model_fn\n",
    "train_config = train_config\n",
    "master = master\n",
    "task = task\n",
    "num_clones = num_clones\n",
    "worker_replicas = worker_replicas\n",
    "clone_on_cpu = clone_on_cpu\n",
    "ps_tasks = ps_tasks\n",
    "worker_job_name = worker_job_name\n",
    "is_chief = is_chief\n",
    "train_dir = train_dir\n",
    "graph_hook_fn=graph_rewriter_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "first_stage_anchor_generator must be of type grid_anchor_generator.GridAnchorGenerator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b95fd2eb34a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetection_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-0d7d4ed30bdd>\u001b[0m in \u001b[0;36mmodel_builder\u001b[1;34m(model_config, is_training, add_summaries, add_background_class)\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmeta_architecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'faster_rcnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n\u001b[1;32m---> 28\u001b[1;33m                                     add_summaries)\n\u001b[0m\u001b[0;32m     29\u001b[0m   \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown meta architecture: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-6f0dd9b1ae60>\u001b[0m in \u001b[0;36m_build_faster_rcnn_model\u001b[1;34m(frcnn_config, is_training, add_summaries)\u001b[0m\n\u001b[0;32m    124\u001b[0m         second_stage_mask_prediction_loss_weight=(\n\u001b[0;32m    125\u001b[0m             second_stage_mask_prediction_loss_weight),\n\u001b[1;32m--> 126\u001b[1;33m         **common_kwargs)\n\u001b[0m",
      "\u001b[1;32mC:\\Workspace\\Study Codes\\object_detection_api_simplified\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_positive_balance_fraction, first_stage_nms_score_threshold, first_stage_nms_iou_threshold, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, initial_crop_size, maxpool_kernel_size, maxpool_stride, second_stage_mask_rcnn_box_predictor, second_stage_batch_size, second_stage_balance_fraction, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, second_stage_mask_prediction_loss_weight, hard_example_miner, parallel_iterations, add_summaries, use_matmul_crop_and_resize)\u001b[0m\n\u001b[0;32m    381\u001b[0m     if not isinstance(first_stage_anchor_generator,\n\u001b[0;32m    382\u001b[0m                       grid_anchor_generator.GridAnchorGenerator):\n\u001b[1;32m--> 383\u001b[1;33m       raise ValueError('first_stage_anchor_generator must be of type '\n\u001b[0m\u001b[0;32m    384\u001b[0m                        'grid_anchor_generator.GridAnchorGenerator.')\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: first_stage_anchor_generator must be of type grid_anchor_generator.GridAnchorGenerator."
     ]
    }
   ],
   "source": [
    "detection_model = create_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_options = [preprocessor_builder.build(step)\n",
    "                             for step in train_config.data_augmentation_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "deploy_config = model_deploy.DeploymentConfig(\n",
    "    num_clones=num_clones,\n",
    "    clone_on_cpu=clone_on_cpu,\n",
    "    replica_id=task,\n",
    "    num_replicas=worker_replicas,\n",
    "    num_ps_tasks=ps_tasks,\n",
    "    worker_job_name=worker_job_name)\n",
    "\n",
    "# Place the global step on the device storing the variables.\n",
    "with tf.device(deploy_config.variables_device()):\n",
    "    global_step = slim.create_global_step()\n",
    "\n",
    "if num_clones != 1 and train_config.sync_replicas:\n",
    "    raise ValueError('In Synchronous SGD mode num_clones must ',\n",
    "                     'be 1. Found num_clones: {}'.format(num_clones))\n",
    "batch_size = train_config.batch_size // num_clones\n",
    "if train_config.sync_replicas:\n",
    "    batch_size //= train_config.replicas_to_aggregate\n",
    "\n",
    "with tf.device(deploy_config.inputs_device()):\n",
    "    input_queue = create_input_queue(\n",
    "        batch_size, create_tensor_dict_fn,\n",
    "        train_config.batch_queue_capacity,\n",
    "        train_config.num_batch_queue_threads,\n",
    "        train_config.prefetch_queue_capacity, data_augmentation_options)\n",
    "\n",
    "# Gather initial summaries.\n",
    "# TODO(rathodv): See if summaries can be added/extracted from global tf\n",
    "# collections so that they don't have to be passed around.\n",
    "summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "global_summaries = set([])\n",
    "\n",
    "model_fn = functools.partial(_create_losses,\n",
    "                             create_model_fn=create_model_fn,\n",
    "                             train_config=train_config)\n",
    "clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\n",
    "first_clone_scope = clones[0].scope\n",
    "\n",
    "if graph_hook_fn:\n",
    "    with tf.device(deploy_config.variables_device()):\n",
    "        graph_hook_fn()\n",
    "\n",
    "# Gather update_ops from the first clone. These contain, for example,\n",
    "# the updates for the batch_norm variables created by model_fn.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "with tf.device(deploy_config.optimizer_device()):\n",
    "    training_optimizer, optimizer_summary_vars = optimizer_builder.build(\n",
    "        train_config.optimizer)\n",
    "    for var in optimizer_summary_vars:\n",
    "        tf.summary.scalar(var.op.name, var, family='LearningRate')\n",
    "\n",
    "sync_optimizer = None\n",
    "if train_config.sync_replicas:\n",
    "    training_optimizer = tf.train.SyncReplicasOptimizer(\n",
    "        training_optimizer,\n",
    "        replicas_to_aggregate=train_config.replicas_to_aggregate,\n",
    "        total_num_replicas=worker_replicas)\n",
    "    sync_optimizer = training_optimizer\n",
    "\n",
    "with tf.device(deploy_config.optimizer_device()):\n",
    "    regularization_losses = (None if train_config.add_regularization_loss\n",
    "                             else [])\n",
    "    total_loss, grads_and_vars = model_deploy.optimize_clones(\n",
    "        clones, training_optimizer,\n",
    "        regularization_losses=regularization_losses)\n",
    "    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')\n",
    "\n",
    "    # Optionally multiply bias gradients by train_config.bias_grad_multiplier.\n",
    "    if train_config.bias_grad_multiplier:\n",
    "        biases_regex_list = ['.*/biases']\n",
    "        grads_and_vars = variables_helper.multiply_gradients_matching_regex(\n",
    "            grads_and_vars,\n",
    "            biases_regex_list,\n",
    "            multiplier=train_config.bias_grad_multiplier)\n",
    "\n",
    "    # Optionally freeze some layers by setting their gradients to be zero.\n",
    "    if train_config.freeze_variables:\n",
    "        grads_and_vars = variables_helper.freeze_gradients_matching_regex(\n",
    "            grads_and_vars, train_config.freeze_variables)\n",
    "\n",
    "    # Optionally clip gradients\n",
    "    if train_config.gradient_clipping_by_norm > 0:\n",
    "        with tf.name_scope('clip_grads'):\n",
    "            grads_and_vars = slim.learning.clip_gradient_norms(\n",
    "                grads_and_vars, train_config.gradient_clipping_by_norm)\n",
    "\n",
    "    # Create gradient updates.\n",
    "    grad_updates = training_optimizer.apply_gradients(grads_and_vars,\n",
    "                                                      global_step=global_step)\n",
    "    update_ops.append(grad_updates)\n",
    "    update_op = tf.group(*update_ops, name='update_barrier')\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        train_tensor = tf.identity(total_loss, name='train_op')\n",
    "\n",
    "# Add summaries.\n",
    "for model_var in slim.get_model_variables():\n",
    "    global_summaries.add(tf.summary.histogram('ModelVars/' +\n",
    "                                              model_var.op.name, model_var))\n",
    "for loss_tensor in tf.losses.get_losses():\n",
    "    global_summaries.add(tf.summary.scalar('Losses/' + loss_tensor.op.name,\n",
    "                                           loss_tensor))\n",
    "global_summaries.add(\n",
    "    tf.summary.scalar('Losses/TotalLoss', tf.losses.get_total_loss()))\n",
    "\n",
    "# Add the summaries from the first clone. These contain the summaries\n",
    "# created by model_fn and either optimize_clones() or _gather_clone_loss().\n",
    "summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                   first_clone_scope))\n",
    "summaries |= global_summaries\n",
    "\n",
    "# Merge all summaries together.\n",
    "summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "# Soft placement allows placing on CPU ops without GPU implementation.\n",
    "session_config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                                log_device_placement=False)\n",
    "\n",
    "# Save checkpoints regularly.\n",
    "keep_checkpoint_every_n_hours = train_config.keep_checkpoint_every_n_hours\n",
    "saver = tf.train.Saver(\n",
    "    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n",
    "\n",
    "# Create ops required to initialize the model from a given checkpoint.\n",
    "init_fn = None\n",
    "if train_config.fine_tune_checkpoint:\n",
    "    if not train_config.fine_tune_checkpoint_type:\n",
    "        # train_config.from_detection_checkpoint field is deprecated. For\n",
    "        # backward compatibility, fine_tune_checkpoint_type is set based on\n",
    "        # from_detection_checkpoint.\n",
    "        if train_config.from_detection_checkpoint:\n",
    "            train_config.fine_tune_checkpoint_type = 'detection'\n",
    "        else:\n",
    "            train_config.fine_tune_checkpoint_type = 'classification'\n",
    "    var_map = detection_model.restore_map(\n",
    "        fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,\n",
    "        load_all_detection_checkpoint_vars=(\n",
    "            train_config.load_all_detection_checkpoint_vars))\n",
    "    available_var_map = (variables_helper.\n",
    "        get_variables_available_in_checkpoint(\n",
    "        var_map, train_config.fine_tune_checkpoint,\n",
    "        include_global_step=False))\n",
    "    init_saver = tf.train.Saver(available_var_map)\n",
    "\n",
    "\n",
    "    def initializer_fn(sess):\n",
    "        init_saver.restore(sess, train_config.fine_tune_checkpoint)\n",
    "\n",
    "\n",
    "    init_fn = initializer_fn\n",
    "\n",
    "slim.learning.train(\n",
    "    train_tensor,\n",
    "    logdir=train_dir,\n",
    "    master=master,\n",
    "    is_chief=is_chief,\n",
    "    session_config=session_config,\n",
    "    startup_delay_steps=train_config.startup_delay_steps,\n",
    "    init_fn=init_fn,\n",
    "    summary_op=summary_op,\n",
    "    number_of_steps=(\n",
    "        train_config.num_steps if train_config.num_steps else None),\n",
    "    save_summaries_secs=120,\n",
    "    sync_optimizer=sync_optimizer,\n",
    "    saver=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
