{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junhoning\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob \n",
    "\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "\n",
    "import json\n",
    "import functools\n",
    "from abc import abstractmethod\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.protos import image_resizer_pb2\n",
    "from object_detection.protos import anchor_generator_pb2\n",
    "from object_detection.protos import model_pb2\n",
    "from object_detection.protos import hyperparams_pb2\n",
    "from object_detection.protos import box_predictor_pb2\n",
    "from object_detection.protos import post_processing_pb2\n",
    "from object_detection.protos import losses_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = ''  # Name of the TensorFlow master to use.\n",
    "task = 0  # task id\n",
    "num_clones = 1  # Number of clones to deploy per worker.\n",
    "clone_on_cpu = False  # Force clones to be deployed on CPU.  Note that even if \n",
    "                      # set to False (allowing ops to run on gpu), some ops may\n",
    "                      # still be run on the CPU if they have no GPU kernel.\n",
    "worker_replicas = 1  # Number of worker+trainer replicas.\n",
    "ps_tasks = 0  # Number of parameter server tasks. If None, does not use a parameter server.\n",
    "train_dir = 'training'  # Directory to save the checkpoints and training summaries.\n",
    "pipeline_config_path = 'configs/faster_rcnn_resnet50_facenet.config'  # Directory to save the checkpoints and training summaries.\n",
    "train_config_path = ''  # Path to a pipeline_pb2.TrainEvalPipelineConfig config file. If provided, other configs are ignored\n",
    "input_config_path = ''  # Path to a train_pb2.TrainConfig config file.\n",
    "model_config_path = ''  # Path to a model_pb2.DetectionModel config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import config_util\n",
    "\n",
    "assert train_dir, '`train_dir` is missing.'\n",
    "if task == 0: \n",
    "    tf.gfile.MakeDirs(train_dir)\n",
    "if pipeline_config_path:\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "    if task == 0:\n",
    "        tf.gfile.Copy(pipeline_config_path,\n",
    "                      os.path.join(train_dir, 'pipeline.config'),\n",
    "                      overwrite=True)\n",
    "else:\n",
    "    configs = config_util.get_configs_from_multiple_files(\n",
    "        model_config_path=model_config_path,\n",
    "        train_config_path=train_config_path,\n",
    "        train_input_config_path=input_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "        for name, config in [('model.config', model_config_path),\n",
    "                             ('train.config', train_config_path),\n",
    "                             ('input.config', input_config_path)]:\n",
    "            tf.gfile.Copy(config, os.path.join(train_dir, name),\n",
    "                          overwrite=True)\n",
    "\n",
    "model_config = configs['model']\n",
    "train_config = configs['train_config']\n",
    "input_config = configs['train_input_config']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Model Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "# meta_architectures/faster_rcnn_meta_arch.py\n",
    "class FasterRCNNFeatureExtractor(object):\n",
    "  \"\"\"Faster R-CNN Feature Extractor definition.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: A boolean indicating whether the training version of the\n",
    "        computation graph should be constructed.\n",
    "      first_stage_features_stride: Output stride of extracted RPN feature map.\n",
    "      batch_norm_trainable: Whether to update batch norm parameters during\n",
    "        training or not. When training with a relative large batch size\n",
    "        (e.g. 8), it could be desirable to enable batch norm update.\n",
    "      reuse_weights: Whether to reuse variables. Default is None.\n",
    "      weight_decay: float weight decay for feature extractor (default: 0.0).\n",
    "    \"\"\"\n",
    "    self._is_training = is_training\n",
    "    self._first_stage_features_stride = first_stage_features_stride\n",
    "    self._train_batch_norm = (batch_norm_trainable and is_training)\n",
    "    self._reuse_weights = reuse_weights\n",
    "    self._weight_decay = weight_decay\n",
    "\n",
    "  @abstractmethod\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Feature-extractor specific preprocessing (minus image resizing).\"\"\"\n",
    "    pass\n",
    "\n",
    "  def extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features.\n",
    "\n",
    "    This function is responsible for extracting feature maps from preprocessed\n",
    "    images.  These features are used by the region proposal network (RPN) to\n",
    "    predict proposals.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: A [batch, height, width, channels] float tensor\n",
    "        representing a batch of images.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      rpn_feature_map: A tensor with shape [batch, height, width, depth]\n",
    "      activations: A dictionary mapping activation tensor names to tensors.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, values=[preprocessed_inputs]):\n",
    "      return self._extract_proposal_features(preprocessed_inputs, scope)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    \"\"\"Extracts first stage RPN features, to be overridden.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features.\n",
    "\n",
    "    Args:\n",
    "      proposal_feature_maps: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, crop_height, crop_width, depth]\n",
    "        representing the feature map cropped to each proposal.\n",
    "      scope: A scope name.\n",
    "\n",
    "    Returns:\n",
    "      proposal_classifier_features: A 4-D float tensor with shape\n",
    "        [batch_size * self.max_num_proposals, height, width, depth]\n",
    "        representing box classifier features for each proposal.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\n",
    "        scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):\n",
    "      return self._extract_box_classifier_features(proposal_feature_maps, scope)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    \"\"\"Extracts second stage box classifier features, to be overridden.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def restore_from_classification_checkpoint_fn(\n",
    "      self,\n",
    "      first_stage_feature_extractor_scope,\n",
    "      second_stage_feature_extractor_scope):\n",
    "    \"\"\"Returns a map of variables to load from a foreign checkpoint.\n",
    "\n",
    "    Args:\n",
    "      first_stage_feature_extractor_scope: A scope name for the first stage\n",
    "        feature extractor.\n",
    "      second_stage_feature_extractor_scope: A scope name for the second stage\n",
    "        feature extractor.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping variable names (to load from a checkpoint) to variables in\n",
    "      the model graph.\n",
    "    \"\"\"\n",
    "    variables_to_restore = {}\n",
    "    for variable in tf.global_variables():\n",
    "      for scope_name in [first_stage_feature_extractor_scope,\n",
    "                         second_stage_feature_extractor_scope]:\n",
    "        if variable.op.name.startswith(scope_name):\n",
    "          var_name = variable.op.name.replace(scope_name + '/', '')\n",
    "          variables_to_restore[var_name] = variable\n",
    "    return variables_to_restore\n",
    "\n",
    "\n",
    "# model/faster_rcnn_resnet_v1_feature_extractor.py\n",
    "class FasterRCNNResnetV1FeatureExtractor(FasterRCNNFeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet V1 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               architecture,\n",
    "               resnet_model,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      architecture: Architecture name of the Resnet V1 model.\n",
    "      resnet_model: Definition of the Resnet V1 model.\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16.\n",
    "    \"\"\"\n",
    "    if first_stage_features_stride != 8 and first_stage_features_stride != 16:\n",
    "      raise ValueError('`first_stage_features_stride` must be 8 or 16.')\n",
    "    self._architecture = architecture\n",
    "    self._resnet_model = resnet_model\n",
    "    super(FasterRCNNResnetV1FeatureExtractor, self).__init__(\n",
    "        is_training, first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)\n",
    "\n",
    "  def preprocess(self, resized_inputs):\n",
    "    \"\"\"Faster R-CNN Resnet V1 preprocessing.\n",
    "\n",
    "    VGG style channel mean subtraction as described here:\n",
    "    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\n",
    "\n",
    "    Args:\n",
    "      resized_inputs: A [batch, height_in, width_in, channels] float32 tensor\n",
    "        representing a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: A [batch, height_out, width_out, channels] float32\n",
    "        tensor representing a batch of images.\n",
    "\n",
    "    \"\"\"\n",
    "    channel_means = [123.68, 116.779, 103.939]\n",
    "    return resized_inputs - [[channel_means]]\n",
    "\n",
    "\n",
    "  def _extract_proposal_features(self, preprocessed_inputs, scope):\n",
    "    if len(preprocessed_inputs.get_shape().as_list()) != 4:\n",
    "      raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
    "                       'tensor of shape %s' % preprocessed_inputs.get_shape())\n",
    "    shape_assert = tf.Assert(\n",
    "        tf.logical_and(\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),\n",
    "            tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),\n",
    "        ['image size must at least be 33 in both height and width.'])\n",
    "\n",
    "    with tf.control_dependencies([shape_assert]):\n",
    "      # Disables batchnorm for fine-tuning with smaller batch sizes.\n",
    "      # TODO(chensun): Figure out if it is needed when image\n",
    "      # batch size is bigger.\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with tf.variable_scope(\n",
    "            self._architecture, reuse=self._reuse_weights) as var_scope:\n",
    "          _, activations = self._resnet_model(\n",
    "              preprocessed_inputs,\n",
    "              num_classes=None,\n",
    "              is_training=self._train_batch_norm,\n",
    "              global_pool=False,\n",
    "              output_stride=self._first_stage_features_stride,\n",
    "              spatial_squeeze=False,\n",
    "              scope=var_scope)\n",
    "\n",
    "    handle = scope + '/%s/block3' % self._architecture\n",
    "    return activations[handle], activations\n",
    "\n",
    "  def _extract_box_classifier_features(self, proposal_feature_maps, scope):\n",
    "    with tf.variable_scope(self._architecture, reuse=self._reuse_weights):\n",
    "      with slim.arg_scope(\n",
    "          resnet_utils.resnet_arg_scope(\n",
    "              batch_norm_epsilon=1e-5,\n",
    "              batch_norm_scale=True,\n",
    "              weight_decay=self._weight_decay)):\n",
    "        with slim.arg_scope([slim.batch_norm],\n",
    "                            is_training=self._train_batch_norm):\n",
    "          blocks = [\n",
    "              resnet_utils.Block('block4', resnet_v1.bottleneck, [{\n",
    "                  'depth': 2048,\n",
    "                  'depth_bottleneck': 512,\n",
    "                  'stride': 1\n",
    "              }] * 3)\n",
    "          ]\n",
    "          proposal_classifier_features = resnet_utils.stack_blocks_dense(\n",
    "              proposal_feature_maps, blocks)\n",
    "    return proposal_classifier_features\n",
    "\n",
    "\n",
    "# model/faster_rcnn_resnet_v1_feature_extractor.py\n",
    "class FasterRCNNResnet50FeatureExtractor(FasterRCNNResnetV1FeatureExtractor):\n",
    "  \"\"\"Faster R-CNN Resnet 50 feature extractor implementation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               first_stage_features_stride,\n",
    "               batch_norm_trainable=False,\n",
    "               reuse_weights=None,\n",
    "               weight_decay=0.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: See base class.\n",
    "      first_stage_features_stride: See base class.\n",
    "      batch_norm_trainable: See base class.\n",
    "      reuse_weights: See base class.\n",
    "      weight_decay: See base class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `first_stage_features_stride` is not 8 or 16,\n",
    "        or if `architecture` is not supported.\n",
    "    \"\"\"\n",
    "    super(FasterRCNNResnet50FeatureExtractor, self).__init__(\n",
    "        'resnet_v1_50', resnet_v1.resnet_v1_50, is_training,\n",
    "        first_stage_features_stride, batch_norm_trainable,\n",
    "        reuse_weights, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_builder.py\n",
    "# A map of names to Faster R-CNN feature extractors.\n",
    "FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {\n",
    "    'faster_rcnn_resnet50': FasterRCNNResnet50FeatureExtractor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/preprocessor.py\n",
    "def resize_to_range(image,\n",
    "                    masks=None,\n",
    "                    min_dimension=None,\n",
    "                    max_dimension=None,\n",
    "                    method=tf.image.ResizeMethod.BILINEAR,\n",
    "                    align_corners=False,\n",
    "                    pad_to_max_dimension=False,\n",
    "                    per_channel_pad_value=(0, 0, 0)):\n",
    "  \"\"\"Resizes an image so its dimensions are within the provided value.\n",
    "\n",
    "  The output size can be described by two cases:\n",
    "  1. If the image can be rescaled so its minimum dimension is equal to the\n",
    "     provided value without the other dimension exceeding max_dimension,\n",
    "     then do so.\n",
    "  2. Otherwise, resize so the largest dimension is equal to max_dimension.\n",
    "\n",
    "  Args:\n",
    "    image: A 3D tensor of shape [height, width, channels]\n",
    "    masks: (optional) rank 3 float32 tensor with shape\n",
    "           [num_instances, height, width] containing instance masks.\n",
    "    min_dimension: (optional) (scalar) desired size of the smaller image\n",
    "                   dimension.\n",
    "    max_dimension: (optional) (scalar) maximum allowed size\n",
    "                   of the larger image dimension.\n",
    "    method: (optional) interpolation method used in resizing. Defaults to\n",
    "            BILINEAR.\n",
    "    align_corners: bool. If true, exactly align all 4 corners of the input\n",
    "                   and output. Defaults to False.\n",
    "    pad_to_max_dimension: Whether to resize the image and pad it with zeros\n",
    "      so the resulting image is of the spatial size\n",
    "      [max_dimension, max_dimension]. If masks are included they are padded\n",
    "      similarly.\n",
    "    per_channel_pad_value: A tuple of per-channel scalar value to use for\n",
    "      padding. By default pads zeros.\n",
    "\n",
    "  Returns:\n",
    "    Note that the position of the resized_image_shape changes based on whether\n",
    "    masks are present.\n",
    "    resized_image: A 3D tensor of shape [new_height, new_width, channels],\n",
    "      where the image has been resized (with bilinear interpolation) so that\n",
    "      min(new_height, new_width) == min_dimension or\n",
    "      max(new_height, new_width) == max_dimension.\n",
    "    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n",
    "      shape [num_instances, new_height, new_width].\n",
    "    resized_image_shape: A 1D tensor of shape [3] containing shape of the\n",
    "      resized image.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the image is not a 3D tensor.\n",
    "  \"\"\"\n",
    "  if len(image.get_shape()) != 3:\n",
    "    raise ValueError('Image should be 3D tensor')\n",
    "\n",
    "  with tf.name_scope('ResizeToRange', values=[image, min_dimension]):\n",
    "    if image.get_shape().is_fully_defined():\n",
    "      new_size = _compute_new_static_size(image, min_dimension, max_dimension)\n",
    "    else:\n",
    "      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)\n",
    "    new_image = tf.image.resize_images(\n",
    "        image, new_size[:-1], method=method, align_corners=align_corners)\n",
    "\n",
    "    if pad_to_max_dimension:\n",
    "      channels = tf.unstack(new_image, axis=2)\n",
    "      if len(channels) != len(per_channel_pad_value):\n",
    "        raise ValueError('Number of channels must be equal to the length of '\n",
    "                         'per-channel pad value.')\n",
    "      new_image = tf.stack(\n",
    "          [\n",
    "              tf.pad(\n",
    "                  channels[i], [[0, max_dimension - new_size[0]],\n",
    "                                [0, max_dimension - new_size[1]]],\n",
    "                  constant_values=per_channel_pad_value[i])\n",
    "              for i in range(len(channels))\n",
    "          ],\n",
    "          axis=2)\n",
    "      new_image.set_shape([max_dimension, max_dimension, 3])\n",
    "\n",
    "    result = [new_image]\n",
    "    if masks is not None:\n",
    "      new_masks = tf.expand_dims(masks, 3)\n",
    "      new_masks = tf.image.resize_images(\n",
    "          new_masks,\n",
    "          new_size[:-1],\n",
    "          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "          align_corners=align_corners)\n",
    "      if pad_to_max_dimension:\n",
    "        new_masks = tf.image.pad_to_bounding_box(\n",
    "            new_masks, 0, 0, max_dimension, max_dimension)\n",
    "      new_masks = tf.squeeze(new_masks, 3)\n",
    "      result.append(new_masks)\n",
    "\n",
    "    result.append(new_size)\n",
    "    return result\n",
    "\n",
    "\n",
    "# core/preprocessor.py\n",
    "# TODO(alirezafathi): Investigate if instead the function should return None if\n",
    "# masks is None.\n",
    "# pylint: disable=g-doc-return-or-yield\n",
    "def resize_image(image,\n",
    "                 masks=None,\n",
    "                 new_height=600,\n",
    "                 new_width=1024,\n",
    "                 method=tf.image.ResizeMethod.BILINEAR,\n",
    "                 align_corners=False):\n",
    "  \"\"\"Resizes images to the given height and width.\n",
    "\n",
    "  Args:\n",
    "    image: A 3D tensor of shape [height, width, channels]\n",
    "    masks: (optional) rank 3 float32 tensor with shape\n",
    "           [num_instances, height, width] containing instance masks.\n",
    "    new_height: (optional) (scalar) desired height of the image.\n",
    "    new_width: (optional) (scalar) desired width of the image.\n",
    "    method: (optional) interpolation method used in resizing. Defaults to\n",
    "            BILINEAR.\n",
    "    align_corners: bool. If true, exactly align all 4 corners of the input\n",
    "                   and output. Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    Note that the position of the resized_image_shape changes based on whether\n",
    "    masks are present.\n",
    "    resized_image: A tensor of size [new_height, new_width, channels].\n",
    "    resized_masks: If masks is not None, also outputs masks. A 3D tensor of\n",
    "      shape [num_instances, new_height, new_width]\n",
    "    resized_image_shape: A 1D tensor of shape [3] containing the shape of the\n",
    "      resized image.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(\n",
    "      'ResizeImage',\n",
    "      values=[image, new_height, new_width, method, align_corners]):\n",
    "    new_image = tf.image.resize_images(\n",
    "        image, tf.stack([new_height, new_width]),\n",
    "        method=method,\n",
    "        align_corners=align_corners)\n",
    "    image_shape = shape_utils.combined_static_and_dynamic_shape(image)\n",
    "    result = [new_image]\n",
    "    if masks is not None:\n",
    "      num_instances = tf.shape(masks)[0]\n",
    "      new_size = tf.stack([new_height, new_width])\n",
    "      def resize_masks_branch():\n",
    "        new_masks = tf.expand_dims(masks, 3)\n",
    "        new_masks = tf.image.resize_nearest_neighbor(\n",
    "            new_masks, new_size, align_corners=align_corners)\n",
    "        new_masks = tf.squeeze(new_masks, axis=3)\n",
    "        return new_masks\n",
    "\n",
    "      def reshape_masks_branch():\n",
    "        # The shape function will be computed for both branches of the\n",
    "        # condition, regardless of which branch is actually taken. Make sure\n",
    "        # that we don't trigger an assertion in the shape function when trying\n",
    "        # to reshape a non empty tensor into an empty one.\n",
    "        new_masks = tf.reshape(masks, [-1, new_size[0], new_size[1]])\n",
    "        return new_masks\n",
    "\n",
    "      masks = tf.cond(num_instances > 0, resize_masks_branch,\n",
    "                      reshape_masks_branch)\n",
    "      result.append(masks)\n",
    "\n",
    "    result.append(tf.stack([new_height, new_width, image_shape[2]]))\n",
    "    return result\n",
    "\n",
    "\n",
    "# core/preprocessor.py\n",
    "def _rgb_to_grayscale(images, name=None):\n",
    "  \"\"\"Converts one or more images from RGB to Grayscale.\n",
    "\n",
    "  Outputs a tensor of the same `DType` and rank as `images`.  The size of the\n",
    "  last dimension of the output is 1, containing the Grayscale value of the\n",
    "  pixels.\n",
    "\n",
    "  Args:\n",
    "    images: The RGB tensor to convert. Last dimension must have size 3 and\n",
    "      should contain RGB values.\n",
    "    name: A name for the operation (optional).\n",
    "\n",
    "  Returns:\n",
    "    The converted grayscale image(s).\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, 'rgb_to_grayscale', [images]) as name:\n",
    "    images = tf.convert_to_tensor(images, name='images')\n",
    "    # Remember original dtype to so we can convert back if needed\n",
    "    orig_dtype = images.dtype\n",
    "    flt_image = tf.image.convert_image_dtype(images, tf.float32)\n",
    "\n",
    "    # Reference for converting between RGB and grayscale.\n",
    "    # https://en.wikipedia.org/wiki/Luma_%28video%29\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "    rank_1 = tf.expand_dims(tf.rank(images) - 1, 0)\n",
    "    gray_float = tf.reduce_sum(\n",
    "        flt_image * rgb_weights, rank_1, keep_dims=True)\n",
    "    gray_float.set_shape(images.get_shape()[:-1].concatenate([1]))\n",
    "    return tf.image.convert_image_dtype(gray_float, orig_dtype, name=name)\n",
    "\n",
    "\n",
    "def rgb_to_gray(image):\n",
    "  \"\"\"Converts a 3 channel RGB image to a 1 channel grayscale image.\n",
    "\n",
    "  Args:\n",
    "    image: Rank 3 float32 tensor containing 1 image -> [height, width, 3]\n",
    "           with pixel values varying between [0, 1].\n",
    "\n",
    "  Returns:\n",
    "    image: A single channel grayscale image -> [image, height, 1].\n",
    "  \"\"\"\n",
    "  return _rgb_to_grayscale(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_ENCODINGS = 'box_encodings'\n",
    "CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'\n",
    "MASK_PREDICTIONS = 'mask_predictions'\n",
    "\n",
    "\n",
    "class BoxPredictor(object):\n",
    "  \"\"\"BoxPredictor.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, num_classes):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "    \"\"\"\n",
    "    self._is_training = is_training\n",
    "    self._num_classes = num_classes\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  def predict(self, image_features, num_predictions_per_location,\n",
    "              scope=None, **params):\n",
    "    \"\"\"Computes encoded object locations and corresponding confidences.\n",
    "\n",
    "    Takes a list of high level image feature maps as input and produces a list\n",
    "    of box encodings and a list of class scores where each element in the output\n",
    "    lists correspond to the feature maps in the input list.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "      width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "      scope: Variable and Op scope name.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              BoxPredictor.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing at least the following tensors.\n",
    "        box_encodings: A list of float tensors. Each entry in the list\n",
    "          corresponds to a feature map in the input `image_features` list. All\n",
    "          tensors in the list have one of the two following shapes:\n",
    "          a. [batch_size, num_anchors_i, q, code_size] representing the location\n",
    "            of the objects, where q is 1 or the number of classes.\n",
    "          b. [batch_size, num_anchors_i, code_size].\n",
    "        class_predictions_with_background: A list of float tensors of shape\n",
    "          [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "          predictions for the proposals. Each entry in the list corresponds to a\n",
    "          feature map in the input `image_features` list.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If length of `image_features` is not equal to length of\n",
    "        `num_predictions_per_location`.\n",
    "    \"\"\"\n",
    "    if len(image_features) != len(num_predictions_per_location):\n",
    "      raise ValueError('image_feature and num_predictions_per_location must '\n",
    "                       'be of same length, found: {} vs {}'.\n",
    "                       format(len(image_features),\n",
    "                              len(num_predictions_per_location)))\n",
    "    if scope is not None:\n",
    "      with tf.variable_scope(scope):\n",
    "        return self._predict(image_features, num_predictions_per_location,\n",
    "                             **params)\n",
    "    return self._predict(image_features, num_predictions_per_location,\n",
    "                         **params)\n",
    "\n",
    "  # TODO(rathodv): num_predictions_per_location could be moved to constructor.\n",
    "  # This is currently only used by ConvolutionalBoxPredictor.\n",
    "  @abstractmethod\n",
    "  def _predict(self, image_features, num_predictions_per_location, **params):\n",
    "    \"\"\"Implementations must override this method.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "        width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              BoxPredictor.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing at least the following tensors.\n",
    "        box_encodings: A list of float tensors. Each entry in the list\n",
    "          corresponds to a feature map in the input `image_features` list. All\n",
    "          tensors in the list have one of the two following shapes:\n",
    "          a. [batch_size, num_anchors_i, q, code_size] representing the location\n",
    "            of the objects, where q is 1 or the number of classes.\n",
    "          b. [batch_size, num_anchors_i, code_size].\n",
    "        class_predictions_with_background: A list of float tensors of shape\n",
    "          [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "          predictions for the proposals. Each entry in the list corresponds to a\n",
    "          feature map in the input `image_features` list.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO(rathodv): Change the implementation to return lists of predictions.\n",
    "class MaskRCNNBoxPredictor(BoxPredictor):\n",
    "  \"\"\"Mask R-CNN Box Predictor.\n",
    "\n",
    "  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).\n",
    "  Mask R-CNN. arXiv preprint arXiv:1703.06870.\n",
    "\n",
    "  This is used for the second stage of the Mask R-CNN detector where proposals\n",
    "  cropped from an image are arranged along the batch dimension of the input\n",
    "  image_features tensor. Notice that locations are *not* shared across classes,\n",
    "  thus for each anchor, a separate prediction is made for each class.\n",
    "\n",
    "  In addition to predicting boxes and classes, optionally this class allows\n",
    "  predicting masks and/or keypoints inside detection boxes.\n",
    "\n",
    "  Currently this box predictor makes per-class predictions; that is, each\n",
    "  anchor makes a separate box prediction for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               num_classes,\n",
    "               fc_hyperparams_fn,\n",
    "               use_dropout,\n",
    "               dropout_keep_prob,\n",
    "               box_code_size,\n",
    "               conv_hyperparams_fn=None,\n",
    "               predict_instance_masks=False,\n",
    "               mask_height=14,\n",
    "               mask_width=14,\n",
    "               mask_prediction_num_conv_layers=2,\n",
    "               mask_prediction_conv_depth=256,\n",
    "               masks_are_class_agnostic=False,\n",
    "               predict_keypoints=False,\n",
    "               share_box_across_classes=False):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "      fc_hyperparams_fn: A function to generate tf-slim arg_scope with\n",
    "        hyperparameters for fully connected ops.\n",
    "      use_dropout: Option to use dropout or not.  Note that a single dropout\n",
    "        op is applied here prior to both box and class predictions, which stands\n",
    "        in contrast to the ConvolutionalBoxPredictor below.\n",
    "      dropout_keep_prob: Keep probability for dropout.\n",
    "        This is only used if use_dropout is True.\n",
    "      box_code_size: Size of encoding for each box.\n",
    "      conv_hyperparams_fn: A function to generate tf-slim arg_scope with\n",
    "        hyperparameters for convolution ops.\n",
    "      predict_instance_masks: Whether to predict object masks inside detection\n",
    "        boxes.\n",
    "      mask_height: Desired output mask height. The default value is 14.\n",
    "      mask_width: Desired output mask width. The default value is 14.\n",
    "      mask_prediction_num_conv_layers: Number of convolution layers applied to\n",
    "        the image_features in mask prediction branch.\n",
    "      mask_prediction_conv_depth: The depth for the first conv2d_transpose op\n",
    "        applied to the image_features in the mask prediction branch. If set\n",
    "        to 0, the depth of the convolution layers will be automatically chosen\n",
    "        based on the number of object classes and the number of channels in the\n",
    "        image features.\n",
    "      masks_are_class_agnostic: Boolean determining if the mask-head is\n",
    "        class-agnostic or not.\n",
    "      predict_keypoints: Whether to predict keypoints insde detection boxes.\n",
    "      share_box_across_classes: Whether to share boxes across classes rather\n",
    "        than use a different box for each class.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If predict_instance_masks is true but conv_hyperparams is not\n",
    "        set.\n",
    "      ValueError: If predict_keypoints is true since it is not implemented yet.\n",
    "      ValueError: If mask_prediction_num_conv_layers is smaller than two.\n",
    "    \"\"\"\n",
    "    super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)\n",
    "    self._fc_hyperparams_fn = fc_hyperparams_fn\n",
    "    self._use_dropout = use_dropout\n",
    "    self._box_code_size = box_code_size\n",
    "    self._dropout_keep_prob = dropout_keep_prob\n",
    "    self._conv_hyperparams_fn = conv_hyperparams_fn\n",
    "    self._predict_instance_masks = predict_instance_masks\n",
    "    self._mask_height = mask_height\n",
    "    self._mask_width = mask_width\n",
    "    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers\n",
    "    self._mask_prediction_conv_depth = mask_prediction_conv_depth\n",
    "    self._masks_are_class_agnostic = masks_are_class_agnostic\n",
    "    self._predict_keypoints = predict_keypoints\n",
    "    self._share_box_across_classes = share_box_across_classes\n",
    "    if self._predict_keypoints:\n",
    "      raise ValueError('Keypoint prediction is unimplemented.')\n",
    "    if ((self._predict_instance_masks or self._predict_keypoints) and\n",
    "        self._conv_hyperparams_fn is None):\n",
    "      raise ValueError('`conv_hyperparams` must be provided when predicting '\n",
    "                       'masks.')\n",
    "    if self._mask_prediction_num_conv_layers < 2:\n",
    "      raise ValueError(\n",
    "          'Mask prediction should consist of at least 2 conv layers')\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  @property\n",
    "  def predicts_instance_masks(self):\n",
    "    return self._predict_instance_masks\n",
    "\n",
    "  def _predict_boxes_and_classes(self, image_features):\n",
    "    \"\"\"Predicts boxes and class scores.\n",
    "\n",
    "    Args:\n",
    "      image_features: A float tensor of shape [batch_size, height, width,\n",
    "        channels] containing features for a batch of images.\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: A float tensor of shape\n",
    "        [batch_size, 1, num_classes, code_size] representing the location of the\n",
    "        objects.\n",
    "      class_predictions_with_background: A float tensor of shape\n",
    "        [batch_size, 1, num_classes + 1] representing the class predictions for\n",
    "        the proposals.\n",
    "    \"\"\"\n",
    "    spatial_averaged_image_features = tf.reduce_mean(image_features, [1, 2],\n",
    "                                                     keep_dims=True,\n",
    "                                                     name='AvgPool')\n",
    "    flattened_image_features = slim.flatten(spatial_averaged_image_features)\n",
    "    if self._use_dropout:\n",
    "      flattened_image_features = slim.dropout(flattened_image_features,\n",
    "                                              keep_prob=self._dropout_keep_prob,\n",
    "                                              is_training=self._is_training)\n",
    "    number_of_boxes = 1\n",
    "    if not self._share_box_across_classes:\n",
    "      number_of_boxes = self._num_classes\n",
    "\n",
    "    with slim.arg_scope(self._fc_hyperparams_fn()):\n",
    "      box_encodings = slim.fully_connected(\n",
    "          flattened_image_features,\n",
    "          number_of_boxes * self._box_code_size,\n",
    "          activation_fn=None,\n",
    "          scope='BoxEncodingPredictor')\n",
    "      class_predictions_with_background = slim.fully_connected(\n",
    "          flattened_image_features,\n",
    "          self._num_classes + 1,\n",
    "          activation_fn=None,\n",
    "          scope='ClassPredictor')\n",
    "    box_encodings = tf.reshape(\n",
    "        box_encodings, [-1, 1, number_of_boxes, self._box_code_size])\n",
    "    class_predictions_with_background = tf.reshape(\n",
    "        class_predictions_with_background, [-1, 1, self._num_classes + 1])\n",
    "    return box_encodings, class_predictions_with_background\n",
    "\n",
    "  def _get_mask_predictor_conv_depth(self, num_feature_channels, num_classes,\n",
    "                                     class_weight=3.0, feature_weight=2.0):\n",
    "    \"\"\"Computes the depth of the mask predictor convolutions.\n",
    "\n",
    "    Computes the depth of the mask predictor convolutions given feature channels\n",
    "    and number of classes by performing a weighted average of the two in\n",
    "    log space to compute the number of convolution channels. The weights that\n",
    "    are used for computing the weighted average do not need to sum to 1.\n",
    "\n",
    "    Args:\n",
    "      num_feature_channels: An integer containing the number of feature\n",
    "        channels.\n",
    "      num_classes: An integer containing the number of classes.\n",
    "      class_weight: Class weight used in computing the weighted average.\n",
    "      feature_weight: Feature weight used in computing the weighted average.\n",
    "\n",
    "    Returns:\n",
    "      An integer containing the number of convolution channels used by mask\n",
    "        predictor.\n",
    "    \"\"\"\n",
    "    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)\n",
    "    num_classes_log = math.log(float(num_classes), 2.0)\n",
    "    weighted_num_feature_channels_log = (\n",
    "        num_feature_channels_log * feature_weight)\n",
    "    weighted_num_classes_log = num_classes_log * class_weight\n",
    "    total_weight = feature_weight + class_weight\n",
    "    num_conv_channels_log = round(\n",
    "        (weighted_num_feature_channels_log + weighted_num_classes_log) /\n",
    "        total_weight)\n",
    "    return int(math.pow(2.0, num_conv_channels_log))\n",
    "\n",
    "  def _predict_masks(self, image_features):\n",
    "    \"\"\"Performs mask prediction.\n",
    "\n",
    "    Args:\n",
    "      image_features: A float tensor of shape [batch_size, height, width,\n",
    "        channels] containing features for a batch of images.\n",
    "\n",
    "    Returns:\n",
    "      instance_masks: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, image_height, image_width].\n",
    "    \"\"\"\n",
    "    num_conv_channels = self._mask_prediction_conv_depth\n",
    "    if num_conv_channels == 0:\n",
    "      num_feature_channels = image_features.get_shape().as_list()[3]\n",
    "      num_conv_channels = self._get_mask_predictor_conv_depth(\n",
    "          num_feature_channels, self.num_classes)\n",
    "    with slim.arg_scope(self._conv_hyperparams_fn()):\n",
    "      upsampled_features = tf.image.resize_bilinear(\n",
    "          image_features,\n",
    "          [self._mask_height, self._mask_width],\n",
    "          align_corners=True)\n",
    "      for _ in range(self._mask_prediction_num_conv_layers - 1):\n",
    "        upsampled_features = slim.conv2d(\n",
    "            upsampled_features,\n",
    "            num_outputs=num_conv_channels,\n",
    "            kernel_size=[3, 3])\n",
    "      num_masks = 1 if self._masks_are_class_agnostic else self.num_classes\n",
    "      mask_predictions = slim.conv2d(upsampled_features,\n",
    "                                     num_outputs=num_masks,\n",
    "                                     activation_fn=None,\n",
    "                                     kernel_size=[3, 3])\n",
    "      return tf.expand_dims(\n",
    "          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),\n",
    "          axis=1,\n",
    "          name='MaskPredictor')\n",
    "\n",
    "  def _predict(self, image_features, num_predictions_per_location,\n",
    "               predict_boxes_and_classes=True, predict_auxiliary_outputs=False):\n",
    "    \"\"\"Optionally computes encoded object locations, confidences, and masks.\n",
    "\n",
    "    Flattens image_features and applies fully connected ops (with no\n",
    "    non-linearity) to predict box encodings and class predictions.  In this\n",
    "    setting, anchors are not spatially arranged in any way and are assumed to\n",
    "    have been folded into the batch dimension.  Thus we output 1 for the\n",
    "    anchors dimension.\n",
    "\n",
    "    Also optionally predicts instance masks.\n",
    "    The mask prediction head is based on the Mask RCNN paper with the following\n",
    "    modifications: We replace the deconvolution layer with a bilinear resize\n",
    "    and a convolution.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "        width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "        Currently, this must be set to [1], or an error will be raised.\n",
    "      predict_boxes_and_classes: If true, the function will perform box\n",
    "        refinement and classification.\n",
    "      predict_auxiliary_outputs: If true, the function will perform other\n",
    "        predictions such as mask, keypoint, boundaries, etc. if any.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing the following tensors.\n",
    "        box_encodings: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, code_size] representing the\n",
    "          location of the objects.\n",
    "        class_predictions_with_background: A float tensor of shape\n",
    "          [batch_size, 1, num_classes + 1] representing the class\n",
    "          predictions for the proposals.\n",
    "      If predict_masks is True the dictionary also contains:\n",
    "        instance_masks: A float tensor of shape\n",
    "          [batch_size, 1, num_classes, image_height, image_width]\n",
    "      If predict_keypoints is True the dictionary also contains:\n",
    "        keypoints: [batch_size, 1, num_keypoints, 2]\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If num_predictions_per_location is not 1 or if both\n",
    "        predict_boxes_and_classes and predict_auxiliary_outputs are false or if\n",
    "        len(image_features) is not 1.\n",
    "    \"\"\"\n",
    "    if (len(num_predictions_per_location) != 1 or\n",
    "        num_predictions_per_location[0] != 1):\n",
    "      raise ValueError('Currently FullyConnectedBoxPredictor only supports '\n",
    "                       'predicting a single box per class per location.')\n",
    "    if not predict_boxes_and_classes and not predict_auxiliary_outputs:\n",
    "      raise ValueError('Should perform at least one prediction.')\n",
    "    if len(image_features) != 1:\n",
    "      raise ValueError('length of `image_features` must be 1. Found {}'.\n",
    "                       format(len(image_features)))\n",
    "    image_feature = image_features[0]\n",
    "    num_predictions_per_location = num_predictions_per_location[0]\n",
    "    predictions_dict = {}\n",
    "\n",
    "    if predict_boxes_and_classes:\n",
    "      (box_encodings, class_predictions_with_background\n",
    "      ) = self._predict_boxes_and_classes(image_feature)\n",
    "      predictions_dict[BOX_ENCODINGS] = box_encodings\n",
    "      predictions_dict[\n",
    "          CLASS_PREDICTIONS_WITH_BACKGROUND] = class_predictions_with_background\n",
    "\n",
    "    if self._predict_instance_masks and predict_auxiliary_outputs:\n",
    "      predictions_dict[MASK_PREDICTIONS] = self._predict_masks(image_feature)\n",
    "\n",
    "    return predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tf_resize_method(resize_method):\n",
    "  \"\"\"Maps image resize method from enumeration type to TensorFlow.\n",
    "\n",
    "  Args:\n",
    "    resize_method: The resize_method attribute of keep_aspect_ratio_resizer or\n",
    "      fixed_shape_resizer.\n",
    "\n",
    "  Returns:\n",
    "    method: The corresponding TensorFlow ResizeMethod.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if `resize_method` is of unknown type.\n",
    "  \"\"\"\n",
    "  dict_method = {\n",
    "      image_resizer_pb2.BILINEAR:\n",
    "          tf.image.ResizeMethod.BILINEAR,\n",
    "      image_resizer_pb2.NEAREST_NEIGHBOR:\n",
    "          tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "      image_resizer_pb2.BICUBIC:\n",
    "          tf.image.ResizeMethod.BICUBIC,\n",
    "      image_resizer_pb2.AREA:\n",
    "          tf.image.ResizeMethod.AREA\n",
    "  }\n",
    "  if resize_method in dict_method:\n",
    "    return dict_method[resize_method]\n",
    "  else:\n",
    "    raise ValueError('Unknown resize_method')\n",
    "\n",
    "\n",
    "def image_resizer_builder(image_resizer_config):\n",
    "  \"\"\"Builds callable for image resizing operations.\n",
    "\n",
    "  Args:\n",
    "    image_resizer_config: image_resizer.proto object containing parameters for\n",
    "      an image resizing operation.\n",
    "\n",
    "  Returns:\n",
    "    image_resizer_fn: Callable for image resizing.  This callable always takes\n",
    "      a rank-3 image tensor (corresponding to a single image) and returns a\n",
    "      rank-3 image tensor, possibly with new spatial dimensions.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if `image_resizer_config` is of incorrect type.\n",
    "    ValueError: if `image_resizer_config.image_resizer_oneof` is of expected\n",
    "      type.\n",
    "    ValueError: if min_dimension > max_dimension when keep_aspect_ratio_resizer\n",
    "      is used.\n",
    "  \"\"\"\n",
    "  if not isinstance(image_resizer_config, image_resizer_pb2.ImageResizer):\n",
    "    raise ValueError('image_resizer_config not of type '\n",
    "                     'image_resizer_pb2.ImageResizer.')\n",
    "\n",
    "  image_resizer_oneof = image_resizer_config.WhichOneof('image_resizer_oneof')\n",
    "  if image_resizer_oneof == 'keep_aspect_ratio_resizer':\n",
    "    keep_aspect_ratio_config = image_resizer_config.keep_aspect_ratio_resizer\n",
    "    if not (keep_aspect_ratio_config.min_dimension <=\n",
    "            keep_aspect_ratio_config.max_dimension):\n",
    "      raise ValueError('min_dimension > max_dimension')\n",
    "    method = _tf_resize_method(keep_aspect_ratio_config.resize_method)\n",
    "    per_channel_pad_value = (0, 0, 0)\n",
    "    if keep_aspect_ratio_config.per_channel_pad_value:\n",
    "      per_channel_pad_value = tuple(keep_aspect_ratio_config.\n",
    "                                    per_channel_pad_value)\n",
    "    image_resizer_fn = functools.partial(\n",
    "        resize_to_range,\n",
    "        min_dimension=keep_aspect_ratio_config.min_dimension,\n",
    "        max_dimension=keep_aspect_ratio_config.max_dimension,\n",
    "        method=method,\n",
    "        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension,\n",
    "        per_channel_pad_value=per_channel_pad_value)\n",
    "    if not keep_aspect_ratio_config.convert_to_grayscale:\n",
    "      return image_resizer_fn\n",
    "  elif image_resizer_oneof == 'fixed_shape_resizer':\n",
    "    fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer\n",
    "    method = _tf_resize_method(fixed_shape_resizer_config.resize_method)\n",
    "    image_resizer_fn = functools.partial(\n",
    "        resize_image,\n",
    "        new_height=fixed_shape_resizer_config.height,\n",
    "        new_width=fixed_shape_resizer_config.width,\n",
    "        method=method)\n",
    "    if not fixed_shape_resizer_config.convert_to_grayscale:\n",
    "      return image_resizer_fn\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        'Invalid image resizer option: \\'%s\\'.' % image_resizer_oneof)\n",
    "\n",
    "  def grayscale_image_resizer(image):\n",
    "    [resized_image, resized_image_shape] = image_resizer_fn(image)\n",
    "    grayscale_image = rgb_to_gray(resized_image)\n",
    "    grayscale_image_shape = tf.concat([resized_image_shape[:-1], [1]], 0)\n",
    "    return [grayscale_image, grayscale_image_shape]\n",
    "\n",
    "  return functools.partial(grayscale_image_resizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets import resnet_utils\n",
    "from nets import resnet_v1\n",
    "\n",
    "def _build_faster_rcnn_feature_extractor(\n",
    "    feature_extractor_config, is_training, reuse_weights=None,\n",
    "    inplace_batchnorm_update=False):\n",
    "  \"\"\"Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Args:\n",
    "    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from\n",
    "      faster_rcnn.proto.\n",
    "    is_training: True if this feature extractor is being built for training.\n",
    "    reuse_weights: if the feature extractor should reuse weights.\n",
    "    inplace_batchnorm_update: Whether to update batch_norm inplace during\n",
    "      training. This is required for batch norm to work correctly on TPUs. When\n",
    "      this is false, user must add a control dependency on\n",
    "      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch\n",
    "      norm moving average parameters.\n",
    "\n",
    "  Returns:\n",
    "    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid feature extractor type.\n",
    "  \"\"\"\n",
    "  if inplace_batchnorm_update:\n",
    "    raise ValueError('inplace batchnorm updates not supported.')\n",
    "  feature_type = feature_extractor_config.type\n",
    "  first_stage_features_stride = (\n",
    "      feature_extractor_config.first_stage_features_stride)\n",
    "  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n",
    "\n",
    "  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:\n",
    "    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(\n",
    "        feature_type))\n",
    "  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[\n",
    "      feature_type]\n",
    "  return feature_extractor_class(\n",
    "      is_training, first_stage_features_stride,\n",
    "      batch_norm_trainable, reuse_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchor_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/anchor_generator\n",
    "class AnchorGenerator(object):\n",
    "  \"\"\"Abstract base class for anchor generators.\"\"\"\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  @abstractmethod\n",
    "  def name_scope(self):\n",
    "    \"\"\"Name scope.\n",
    "\n",
    "    Must be defined by implementations.\n",
    "\n",
    "    Returns:\n",
    "      a string representing the name scope of the anchor generation operation.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def check_num_anchors(self):\n",
    "    \"\"\"Whether to dynamically check the number of anchors generated.\n",
    "\n",
    "    Can be overridden by implementations that would like to disable this\n",
    "    behavior.\n",
    "\n",
    "    Returns:\n",
    "      a boolean controlling whether the Generate function should dynamically\n",
    "      check the number of anchors generated against the mathematically\n",
    "      expected number of anchors.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "  @abstractmethod\n",
    "  def num_anchors_per_location(self):\n",
    "    \"\"\"Returns the number of anchors per spatial location.\n",
    "\n",
    "    Returns:\n",
    "      a list of integers, one for each expected feature map to be passed to\n",
    "      the `generate` function.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def generate(self, feature_map_shape_list, **params):\n",
    "    \"\"\"Generates a collection of bounding boxes to be used as anchors.\n",
    "\n",
    "    TODO(rathodv): remove **params from argument list and make stride and\n",
    "      offsets (for multiple_grid_anchor_generator) constructor arguments.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.  Pairs can be provided as 1-dimensional\n",
    "        integer tensors of length 2 or simply as tuples of integers.\n",
    "      **params: parameters for anchor generation op\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n",
    "        the input feature map shapes.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the number of feature map shapes does not match the length\n",
    "        of NumAnchorsPerLocation.\n",
    "    \"\"\"\n",
    "    if self.check_num_anchors and (\n",
    "        len(feature_map_shape_list) != len(self.num_anchors_per_location())):\n",
    "      raise ValueError('Number of feature maps is expected to equal the length '\n",
    "                       'of `num_anchors_per_location`.')\n",
    "    with tf.name_scope(self.name_scope()):\n",
    "      anchors_list = self._generate(feature_map_shape_list, **params)\n",
    "      if self.check_num_anchors:\n",
    "        with tf.control_dependencies([\n",
    "            self._assert_correct_number_of_anchors(\n",
    "                anchors_list, feature_map_shape_list)]):\n",
    "          for item in anchors_list:\n",
    "            item.set(tf.identity(item.get()))\n",
    "      return anchors_list\n",
    "\n",
    "  @abstractmethod\n",
    "  def _generate(self, feature_map_shape_list, **params):\n",
    "    \"\"\"To be overridden by implementations.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.\n",
    "      **params: parameters for anchor generation op\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxList, each holding a collection of N anchor\n",
    "        boxes.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def _assert_correct_number_of_anchors(self, anchors_list,\n",
    "                                        feature_map_shape_list):\n",
    "    \"\"\"Assert that correct number of anchors was generated.\n",
    "\n",
    "    Args:\n",
    "      anchors_list: A list of box_list.BoxList object holding anchors generated.\n",
    "      feature_map_shape_list: list of (height, width) pairs in the format\n",
    "        [(height_0, width_0), (height_1, width_1), ...] that the generated\n",
    "        anchors must align with.\n",
    "    Returns:\n",
    "      Op that raises InvalidArgumentError if the number of anchors does not\n",
    "        match the number of expected anchors.\n",
    "    \"\"\"\n",
    "    expected_num_anchors = 0\n",
    "    actual_num_anchors = 0\n",
    "    for num_anchors_per_location, feature_map_shape, anchors in zip(\n",
    "        self.num_anchors_per_location(), feature_map_shape_list, anchors_list):\n",
    "      expected_num_anchors += (num_anchors_per_location\n",
    "                               * feature_map_shape[0]\n",
    "                               * feature_map_shape[1])\n",
    "      actual_num_anchors += anchors.num_boxes()\n",
    "    return tf.assert_equal(expected_num_anchors, actual_num_anchors)\n",
    "\n",
    "\n",
    "\n",
    "# anchor_generator/anchor_generator.py\n",
    "class GridAnchorGenerator(AnchorGenerator):\n",
    "  \"\"\"Generates a grid of anchors at given scales and aspect ratios.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               scales=(0.5, 1.0, 2.0),\n",
    "               aspect_ratios=(0.5, 1.0, 2.0),\n",
    "               base_anchor_size=None,\n",
    "               anchor_stride=None,\n",
    "               anchor_offset=None):\n",
    "    \"\"\"Constructs a GridAnchorGenerator.\n",
    "\n",
    "    Args:\n",
    "      scales: a list of (float) scales, default=(0.5, 1.0, 2.0)\n",
    "      aspect_ratios: a list of (float) aspect ratios, default=(0.5, 1.0, 2.0)\n",
    "      base_anchor_size: base anchor size as height, width (\n",
    "                        (length-2 float32 list or tensor, default=[256, 256])\n",
    "      anchor_stride: difference in centers between base anchors for adjacent\n",
    "                     grid positions (length-2 float32 list or tensor,\n",
    "                     default=[16, 16])\n",
    "      anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n",
    "                     upper left element of the grid, this should be zero for\n",
    "                     feature networks with only VALID padding and even receptive\n",
    "                     field size, but may need additional calculation if other\n",
    "                     padding is used (length-2 float32 list or tensor,\n",
    "                     default=[0, 0])\n",
    "    \"\"\"\n",
    "    # Handle argument defaults\n",
    "    if base_anchor_size is None:\n",
    "      base_anchor_size = [256, 256]\n",
    "    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))\n",
    "    if anchor_stride is None:\n",
    "      anchor_stride = [16, 16]\n",
    "    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))\n",
    "    if anchor_offset is None:\n",
    "      anchor_offset = [0, 0]\n",
    "    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))\n",
    "\n",
    "    self._scales = scales\n",
    "    self._aspect_ratios = aspect_ratios\n",
    "    self._base_anchor_size = base_anchor_size\n",
    "    self._anchor_stride = anchor_stride\n",
    "    self._anchor_offset = anchor_offset\n",
    "\n",
    "  def name_scope(self):\n",
    "    return 'GridAnchorGenerator'\n",
    "\n",
    "  def num_anchors_per_location(self):\n",
    "    \"\"\"Returns the number of anchors per spatial location.\n",
    "\n",
    "    Returns:\n",
    "      a list of integers, one for each expected feature map to be passed to\n",
    "      the `generate` function.\n",
    "    \"\"\"\n",
    "    return [len(self._scales) * len(self._aspect_ratios)]\n",
    "\n",
    "  def _generate(self, feature_map_shape_list):\n",
    "    \"\"\"Generates a collection of bounding boxes to be used as anchors.\n",
    "\n",
    "    Args:\n",
    "      feature_map_shape_list: list of pairs of convnet layer resolutions in the\n",
    "        format [(height_0, width_0)].  For example, setting\n",
    "        feature_map_shape_list=[(8, 8)] asks for anchors that correspond\n",
    "        to an 8x8 layer.  For this anchor generator, only lists of length 1 are\n",
    "        allowed.\n",
    "\n",
    "    Returns:\n",
    "      boxes_list: a list of BoxLists each holding anchor boxes corresponding to\n",
    "        the input feature map shapes.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if feature_map_shape_list, box_specs_list do not have the same\n",
    "        length.\n",
    "      ValueError: if feature_map_shape_list does not consist of pairs of\n",
    "        integers\n",
    "    \"\"\"\n",
    "    if not (isinstance(feature_map_shape_list, list)\n",
    "            and len(feature_map_shape_list) == 1):\n",
    "      raise ValueError('feature_map_shape_list must be a list of length 1.')\n",
    "    if not all([isinstance(list_item, tuple) and len(list_item) == 2\n",
    "                for list_item in feature_map_shape_list]):\n",
    "      raise ValueError('feature_map_shape_list must be a list of pairs.')\n",
    "    grid_height, grid_width = feature_map_shape_list[0]\n",
    "    scales_grid, aspect_ratios_grid = ops.meshgrid(self._scales,\n",
    "                                                   self._aspect_ratios)\n",
    "    scales_grid = tf.reshape(scales_grid, [-1])\n",
    "    aspect_ratios_grid = tf.reshape(aspect_ratios_grid, [-1])\n",
    "    anchors = tile_anchors(grid_height,\n",
    "                           grid_width,\n",
    "                           scales_grid,\n",
    "                           aspect_ratios_grid,\n",
    "                           self._base_anchor_size,\n",
    "                           self._anchor_stride,\n",
    "                           self._anchor_offset)\n",
    "\n",
    "    num_anchors = anchors.num_boxes_static()\n",
    "    if num_anchors is None:\n",
    "      num_anchors = anchors.num_boxes()\n",
    "    anchor_indices = tf.zeros([num_anchors])\n",
    "    anchors.add_field('feature_map_index', anchor_indices)\n",
    "    return [anchors]\n",
    "\n",
    "\n",
    "def tile_anchors(grid_height,\n",
    "                 grid_width,\n",
    "                 scales,\n",
    "                 aspect_ratios,\n",
    "                 base_anchor_size,\n",
    "                 anchor_stride,\n",
    "                 anchor_offset):\n",
    "  \"\"\"Create a tiled set of anchors strided along a grid in image space.\n",
    "\n",
    "  This op creates a set of anchor boxes by placing a \"basis\" collection of\n",
    "  boxes with user-specified scales and aspect ratios centered at evenly\n",
    "  distributed points along a grid.  The basis collection is specified via the\n",
    "  scale and aspect_ratios arguments.  For example, setting scales=[.1, .2, .2]\n",
    "  and aspect ratios = [2,2,1/2] means that we create three boxes: one with scale\n",
    "  .1, aspect ratio 2, one with scale .2, aspect ratio 2, and one with scale .2\n",
    "  and aspect ratio 1/2.  Each box is multiplied by \"base_anchor_size\" before\n",
    "  placing it over its respective center.\n",
    "\n",
    "  Grid points are specified via grid_height, grid_width parameters as well as\n",
    "  the anchor_stride and anchor_offset parameters.\n",
    "\n",
    "  Args:\n",
    "    grid_height: size of the grid in the y direction (int or int scalar tensor)\n",
    "    grid_width: size of the grid in the x direction (int or int scalar tensor)\n",
    "    scales: a 1-d  (float) tensor representing the scale of each box in the\n",
    "      basis set.\n",
    "    aspect_ratios: a 1-d (float) tensor representing the aspect ratio of each\n",
    "      box in the basis set.  The length of the scales and aspect_ratios tensors\n",
    "      must be equal.\n",
    "    base_anchor_size: base anchor size as [height, width]\n",
    "      (float tensor of shape [2])\n",
    "    anchor_stride: difference in centers between base anchors for adjacent grid\n",
    "                   positions (float tensor of shape [2])\n",
    "    anchor_offset: center of the anchor with scale and aspect ratio 1 for the\n",
    "                   upper left element of the grid, this should be zero for\n",
    "                   feature networks with only VALID padding and even receptive\n",
    "                   field size, but may need some additional calculation if other\n",
    "                   padding is used (float tensor of shape [2])\n",
    "  Returns:\n",
    "    a BoxList holding a collection of N anchor boxes\n",
    "  \"\"\"\n",
    "  ratio_sqrts = tf.sqrt(aspect_ratios)\n",
    "  heights = scales / ratio_sqrts * base_anchor_size[0]\n",
    "  widths = scales * ratio_sqrts * base_anchor_size[1]\n",
    "\n",
    "  # Get a grid of box centers\n",
    "  y_centers = tf.to_float(tf.range(grid_height))\n",
    "  y_centers = y_centers * anchor_stride[0] + anchor_offset[0]\n",
    "  x_centers = tf.to_float(tf.range(grid_width))\n",
    "  x_centers = x_centers * anchor_stride[1] + anchor_offset[1]\n",
    "  x_centers, y_centers = ops.meshgrid(x_centers, y_centers)\n",
    "\n",
    "  widths_grid, x_centers_grid = ops.meshgrid(widths, x_centers)\n",
    "  heights_grid, y_centers_grid = ops.meshgrid(heights, y_centers)\n",
    "  bbox_centers = tf.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "  bbox_sizes = tf.stack([heights_grid, widths_grid], axis=3)\n",
    "  bbox_centers = tf.reshape(bbox_centers, [-1, 2])\n",
    "  bbox_sizes = tf.reshape(bbox_sizes, [-1, 2])\n",
    "  bbox_corners = _center_size_bbox_to_corners_bbox(bbox_centers, bbox_sizes)\n",
    "  return box_list.BoxList(bbox_corners)\n",
    "\n",
    "\n",
    "def _center_size_bbox_to_corners_bbox(centers, sizes):\n",
    "  \"\"\"Converts bbox center-size representation to corners representation.\n",
    "\n",
    "  Args:\n",
    "    centers: a tensor with shape [N, 2] representing bounding box centers\n",
    "    sizes: a tensor with shape [N, 2] representing bounding boxes\n",
    "\n",
    "  Returns:\n",
    "    corners: tensor with shape [N, 4] representing bounding boxes in corners\n",
    "      representation\n",
    "  \"\"\"\n",
    "  return tf.concat([centers - .5 * sizes, centers + .5 * sizes], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builders/anchor_generator_builder.py\n",
    "def anchor_generator_builder(anchor_generator_config):\n",
    "  \"\"\"Builds an anchor generator based on the config.\n",
    "\n",
    "  Args:\n",
    "    anchor_generator_config: An anchor_generator.proto object containing the\n",
    "      config for the desired anchor generator.\n",
    "\n",
    "  Returns:\n",
    "    Anchor generator based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On empty anchor generator proto.\n",
    "  \"\"\"\n",
    "  if not isinstance(anchor_generator_config,\n",
    "                    anchor_generator_pb2.AnchorGenerator):\n",
    "    raise ValueError('anchor_generator_config not of type '\n",
    "                     'anchor_generator_pb2.AnchorGenerator')\n",
    "  if anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'grid_anchor_generator':\n",
    "    grid_anchor_generator_config = anchor_generator_config.grid_anchor_generator\n",
    "    return GridAnchorGenerator(\n",
    "        scales=[float(scale) for scale in grid_anchor_generator_config.scales],\n",
    "        aspect_ratios=[float(aspect_ratio)\n",
    "                       for aspect_ratio\n",
    "                       in grid_anchor_generator_config.aspect_ratios],\n",
    "        base_anchor_size=[grid_anchor_generator_config.height,\n",
    "                          grid_anchor_generator_config.width],\n",
    "        anchor_stride=[grid_anchor_generator_config.height_stride,\n",
    "                       grid_anchor_generator_config.width_stride],\n",
    "        anchor_offset=[grid_anchor_generator_config.height_offset,\n",
    "                       grid_anchor_generator_config.width_offset])\n",
    "  elif anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'ssd_anchor_generator':\n",
    "    ssd_anchor_generator_config = anchor_generator_config.ssd_anchor_generator\n",
    "    anchor_strides = None\n",
    "    if ssd_anchor_generator_config.height_stride:\n",
    "      anchor_strides = zip(ssd_anchor_generator_config.height_stride,\n",
    "                           ssd_anchor_generator_config.width_stride)\n",
    "    anchor_offsets = None\n",
    "    if ssd_anchor_generator_config.height_offset:\n",
    "      anchor_offsets = zip(ssd_anchor_generator_config.height_offset,\n",
    "                           ssd_anchor_generator_config.width_offset)\n",
    "    return multiple_grid_anchor_generator.create_ssd_anchors(\n",
    "        num_layers=ssd_anchor_generator_config.num_layers,\n",
    "        min_scale=ssd_anchor_generator_config.min_scale,\n",
    "        max_scale=ssd_anchor_generator_config.max_scale,\n",
    "        scales=[float(scale) for scale in ssd_anchor_generator_config.scales],\n",
    "        aspect_ratios=ssd_anchor_generator_config.aspect_ratios,\n",
    "        interpolated_scale_aspect_ratio=(\n",
    "            ssd_anchor_generator_config.interpolated_scale_aspect_ratio),\n",
    "        base_anchor_size=[\n",
    "            ssd_anchor_generator_config.base_anchor_height,\n",
    "            ssd_anchor_generator_config.base_anchor_width\n",
    "        ],\n",
    "        anchor_strides=anchor_strides,\n",
    "        anchor_offsets=anchor_offsets,\n",
    "        reduce_boxes_in_lowest_layer=(\n",
    "            ssd_anchor_generator_config.reduce_boxes_in_lowest_layer))\n",
    "  elif anchor_generator_config.WhichOneof(\n",
    "      'anchor_generator_oneof') == 'multiscale_anchor_generator':\n",
    "    cfg = anchor_generator_config.multiscale_anchor_generator\n",
    "    return multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator(\n",
    "        cfg.min_level,\n",
    "        cfg.max_level,\n",
    "        cfg.anchor_scale,\n",
    "        [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],\n",
    "        cfg.scales_per_octave,\n",
    "        cfg.normalize_coordinates\n",
    "    )\n",
    "  else:\n",
    "    raise ValueError('Empty anchor generator.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/box_predictor.py\n",
    "\n",
    "class RfcnBoxPredictor(BoxPredictor):\n",
    "  \"\"\"RFCN Box Predictor.\n",
    "\n",
    "  Applies a position sensitive ROI pooling on position sensitive feature maps to\n",
    "  predict classes and refined locations. See https://arxiv.org/abs/1605.06409\n",
    "  for details.\n",
    "\n",
    "  This is used for the second stage of the RFCN meta architecture. Notice that\n",
    "  locations are *not* shared across classes, thus for each anchor, a separate\n",
    "  prediction is made for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               num_classes,\n",
    "               conv_hyperparams_fn,\n",
    "               num_spatial_bins,\n",
    "               depth,\n",
    "               crop_size,\n",
    "               box_code_size):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: Indicates whether the BoxPredictor is in training mode.\n",
    "      num_classes: number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "      conv_hyperparams_fn: A function to construct tf-slim arg_scope with\n",
    "        hyperparameters for convolutional layers.\n",
    "      num_spatial_bins: A list of two integers `[spatial_bins_y,\n",
    "        spatial_bins_x]`.\n",
    "      depth: Target depth to reduce the input feature maps to.\n",
    "      crop_size: A list of two integers `[crop_height, crop_width]`.\n",
    "      box_code_size: Size of encoding for each box.\n",
    "    \"\"\"\n",
    "    super(RfcnBoxPredictor, self).__init__(is_training, num_classes)\n",
    "    self._conv_hyperparams_fn = conv_hyperparams_fn\n",
    "    self._num_spatial_bins = num_spatial_bins\n",
    "    self._depth = depth\n",
    "    self._crop_size = crop_size\n",
    "    self._box_code_size = box_code_size\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  def _predict(self, image_features, num_predictions_per_location,\n",
    "               proposal_boxes):\n",
    "    \"\"\"Computes encoded object locations and corresponding confidences.\n",
    "\n",
    "    Args:\n",
    "      image_features: A list of float tensors of shape [batch_size, height_i,\n",
    "      width_i, channels_i] containing features for a batch of images.\n",
    "      num_predictions_per_location: A list of integers representing the number\n",
    "        of box predictions to be made per spatial location for each feature map.\n",
    "        Currently, this must be set to [1], or an error will be raised.\n",
    "      proposal_boxes: A float tensor of shape [batch_size, num_proposals,\n",
    "        box_code_size].\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: A list of float tensors of shape\n",
    "        [batch_size, num_anchors_i, q, code_size] representing the location of\n",
    "        the objects, where q is 1 or the number of classes. Each entry in the\n",
    "        list corresponds to a feature map in the input `image_features` list.\n",
    "      class_predictions_with_background: A list of float tensors of shape\n",
    "        [batch_size, num_anchors_i, num_classes + 1] representing the class\n",
    "        predictions for the proposals. Each entry in the list corresponds to a\n",
    "        feature map in the input `image_features` list.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if num_predictions_per_location is not 1 or if\n",
    "        len(image_features) is not 1.\n",
    "    \"\"\"\n",
    "    if (len(num_predictions_per_location) != 1 or\n",
    "        num_predictions_per_location[0] != 1):\n",
    "      raise ValueError('Currently RfcnBoxPredictor only supports '\n",
    "                       'predicting a single box per class per location.')\n",
    "    if len(image_features) != 1:\n",
    "      raise ValueError('length of `image_features` must be 1. Found {}'.\n",
    "                       format(len(image_features)))\n",
    "    image_feature = image_features[0]\n",
    "    num_predictions_per_location = num_predictions_per_location[0]\n",
    "    batch_size = tf.shape(proposal_boxes)[0]\n",
    "    num_boxes = tf.shape(proposal_boxes)[1]\n",
    "    def get_box_indices(proposals):\n",
    "      proposals_shape = proposals.get_shape().as_list()\n",
    "      if any(dim is None for dim in proposals_shape):\n",
    "        proposals_shape = tf.shape(proposals)\n",
    "      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "      multiplier = tf.expand_dims(\n",
    "          tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "      return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "    net = image_feature\n",
    "    with slim.arg_scope(self._conv_hyperparams_fn()):\n",
    "      net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')\n",
    "      # Location predictions.\n",
    "      location_feature_map_depth = (self._num_spatial_bins[0] *\n",
    "                                    self._num_spatial_bins[1] *\n",
    "                                    self.num_classes *\n",
    "                                    self._box_code_size)\n",
    "      location_feature_map = slim.conv2d(net, location_feature_map_depth,\n",
    "                                         [1, 1], activation_fn=None,\n",
    "                                         scope='refined_locations')\n",
    "      box_encodings = ops.position_sensitive_crop_regions(\n",
    "          location_feature_map,\n",
    "          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n",
    "          box_ind=get_box_indices(proposal_boxes),\n",
    "          crop_size=self._crop_size,\n",
    "          num_spatial_bins=self._num_spatial_bins,\n",
    "          global_pool=True)\n",
    "      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[1, 2])\n",
    "      box_encodings = tf.reshape(box_encodings,\n",
    "                                 [batch_size * num_boxes, 1, self.num_classes,\n",
    "                                  self._box_code_size])\n",
    "\n",
    "      # Class predictions.\n",
    "      total_classes = self.num_classes + 1  # Account for background class.\n",
    "      class_feature_map_depth = (self._num_spatial_bins[0] *\n",
    "                                 self._num_spatial_bins[1] *\n",
    "                                 total_classes)\n",
    "      class_feature_map = slim.conv2d(net, class_feature_map_depth, [1, 1],\n",
    "                                      activation_fn=None,\n",
    "                                      scope='class_predictions')\n",
    "      class_predictions_with_background = ops.position_sensitive_crop_regions(\n",
    "          class_feature_map,\n",
    "          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),\n",
    "          box_ind=get_box_indices(proposal_boxes),\n",
    "          crop_size=self._crop_size,\n",
    "          num_spatial_bins=self._num_spatial_bins,\n",
    "          global_pool=True)\n",
    "      class_predictions_with_background = tf.squeeze(\n",
    "          class_predictions_with_background, squeeze_dims=[1, 2])\n",
    "      class_predictions_with_background = tf.reshape(\n",
    "          class_predictions_with_background,\n",
    "          [batch_size * num_boxes, 1, total_classes])\n",
    "\n",
    "    return {BOX_ENCODINGS: [box_encodings],\n",
    "            CLASS_PREDICTIONS_WITH_BACKGROUND:\n",
    "            [class_predictions_with_background]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/model.py\n",
    "class DetectionModel(object):\n",
    "  \"\"\"Abstract base class for detection models.\"\"\"\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  def __init__(self, num_classes):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      num_classes: number of classes.  Note that num_classes *does not* include\n",
    "      background categories that might be implicitly predicted in various\n",
    "      implementations.\n",
    "    \"\"\"\n",
    "    self._num_classes = num_classes\n",
    "    self._groundtruth_lists = {}\n",
    "\n",
    "  @property\n",
    "  def num_classes(self):\n",
    "    return self._num_classes\n",
    "\n",
    "  def groundtruth_lists(self, field):\n",
    "    \"\"\"Access list of groundtruth tensors.\n",
    "\n",
    "    Args:\n",
    "      field: a string key, options are\n",
    "        fields.BoxListFields.{boxes,classes,masks,keypoints}\n",
    "\n",
    "    Returns:\n",
    "      a list of tensors holding groundtruth information (see also\n",
    "      provide_groundtruth function below), with one entry for each image in the\n",
    "      batch.\n",
    "    Raises:\n",
    "      RuntimeError: if the field has not been provided via provide_groundtruth.\n",
    "    \"\"\"\n",
    "    if field not in self._groundtruth_lists:\n",
    "      raise RuntimeError('Groundtruth tensor %s has not been provided', field)\n",
    "    return self._groundtruth_lists[field]\n",
    "\n",
    "  def groundtruth_has_field(self, field):\n",
    "    \"\"\"Determines whether the groundtruth includes the given field.\n",
    "\n",
    "    Args:\n",
    "      field: a string key, options are\n",
    "        fields.BoxListFields.{boxes,classes,masks,keypoints}\n",
    "\n",
    "    Returns:\n",
    "      True if the groundtruth includes the given field, False otherwise.\n",
    "    \"\"\"\n",
    "    return field in self._groundtruth_lists\n",
    "\n",
    "  @abstractmethod\n",
    "  def preprocess(self, inputs):\n",
    "    \"\"\"Input preprocessing.\n",
    "\n",
    "    To be overridden by implementations.\n",
    "\n",
    "    This function is responsible for any scaling/shifting of input values that\n",
    "    is necessary prior to running the detector on an input image.\n",
    "    It is also responsible for any resizing, padding that might be necessary\n",
    "    as images are assumed to arrive in arbitrary sizes.  While this function\n",
    "    could conceivably be part of the predict method (below), it is often\n",
    "    convenient to keep these separate --- for example, we may want to preprocess\n",
    "    on one device, place onto a queue, and let another device (e.g., the GPU)\n",
    "    handle prediction.\n",
    "\n",
    "    A few important notes about the preprocess function:\n",
    "    + We assume that this operation does not have any trainable variables nor\n",
    "    does it affect the groundtruth annotations in any way (thus data\n",
    "    augmentation operations such as random cropping should be performed\n",
    "    externally).\n",
    "    + There is no assumption that the batchsize in this function is the same as\n",
    "    the batch size in the predict function.  In fact, we recommend calling the\n",
    "    preprocess function prior to calling any batching operations (which should\n",
    "    happen outside of the model) and thus assuming that batch sizes are equal\n",
    "    to 1 in the preprocess function.\n",
    "    + There is also no explicit assumption that the output resolutions\n",
    "    must be fixed across inputs --- this is to support \"fully convolutional\"\n",
    "    settings in which input images can have different shapes/resolutions.\n",
    "\n",
    "    Args:\n",
    "      inputs: a [batch, height_in, width_in, channels] float32 tensor\n",
    "        representing a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: a [batch, height_out, width_out, channels] float32\n",
    "        tensor representing a batch of images.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def predict(self, preprocessed_inputs, true_image_shapes):\n",
    "    \"\"\"Predict prediction tensors from inputs tensor.\n",
    "\n",
    "    Outputs of this function can be passed to loss or postprocess functions.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: a [batch, height, width, channels] float32 tensor\n",
    "        representing a batch of images.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      prediction_dict: a dictionary holding prediction tensors to be\n",
    "        passed to the Loss or Postprocess functions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def postprocess(self, prediction_dict, true_image_shapes, **params):\n",
    "    \"\"\"Convert predicted output tensors to final detections.\n",
    "\n",
    "    Outputs adhere to the following conventions:\n",
    "    * Classes are integers in [0, num_classes); background classes are removed\n",
    "      and the first non-background class is mapped to 0. If the model produces\n",
    "      class-agnostic detections, then no output is produced for classes.\n",
    "    * Boxes are to be interpreted as being in [y_min, x_min, y_max, x_max]\n",
    "      format and normalized relative to the image window.\n",
    "    * `num_detections` is provided for settings where detections are padded to a\n",
    "      fixed number of boxes.\n",
    "    * We do not specifically assume any kind of probabilistic interpretation\n",
    "      of the scores --- the only important thing is their relative ordering.\n",
    "      Thus implementations of the postprocess function are free to output\n",
    "      logits, probabilities, calibrated probabilities, or anything else.\n",
    "\n",
    "    Args:\n",
    "      prediction_dict: a dictionary holding prediction tensors.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "        DetectionModel.\n",
    "\n",
    "    Returns:\n",
    "      detections: a dictionary containing the following fields\n",
    "        detection_boxes: [batch, max_detections, 4]\n",
    "        detection_scores: [batch, max_detections]\n",
    "        detection_classes: [batch, max_detections]\n",
    "          (If a model is producing class-agnostic detections, this field may be\n",
    "          missing)\n",
    "        instance_masks: [batch, max_detections, image_height, image_width]\n",
    "          (optional)\n",
    "        keypoints: [batch, max_detections, num_keypoints, 2] (optional)\n",
    "        num_detections: [batch]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def loss(self, prediction_dict, true_image_shapes):\n",
    "    \"\"\"Compute scalar loss tensors with respect to provided groundtruth.\n",
    "\n",
    "    Calling this function requires that groundtruth tensors have been\n",
    "    provided via the provide_groundtruth function.\n",
    "\n",
    "    Args:\n",
    "      prediction_dict: a dictionary holding predicted tensors\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary mapping strings (loss names) to scalar tensors representing\n",
    "        loss values.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  def provide_groundtruth(self,\n",
    "                          groundtruth_boxes_list,\n",
    "                          groundtruth_classes_list,\n",
    "                          groundtruth_masks_list=None,\n",
    "                          groundtruth_keypoints_list=None,\n",
    "                          groundtruth_weights_list=None,\n",
    "                          groundtruth_is_crowd_list=None):\n",
    "    \"\"\"Provide groundtruth tensors.\n",
    "\n",
    "    Args:\n",
    "      groundtruth_boxes_list: a list of 2-D tf.float32 tensors of shape\n",
    "        [num_boxes, 4] containing coordinates of the groundtruth boxes.\n",
    "          Groundtruth boxes are provided in [y_min, x_min, y_max, x_max]\n",
    "          format and assumed to be normalized and clipped\n",
    "          relative to the image window with y_min <= y_max and x_min <= x_max.\n",
    "      groundtruth_classes_list: a list of 2-D tf.float32 one-hot (or k-hot)\n",
    "        tensors of shape [num_boxes, num_classes] containing the class targets\n",
    "        with the 0th index assumed to map to the first non-background class.\n",
    "      groundtruth_masks_list: a list of 3-D tf.float32 tensors of\n",
    "        shape [num_boxes, height_in, width_in] containing instance\n",
    "        masks with values in {0, 1}.  If None, no masks are provided.\n",
    "        Mask resolution `height_in`x`width_in` must agree with the resolution\n",
    "        of the input image tensor provided to the `preprocess` function.\n",
    "      groundtruth_keypoints_list: a list of 3-D tf.float32 tensors of\n",
    "        shape [num_boxes, num_keypoints, 2] containing keypoints.\n",
    "        Keypoints are assumed to be provided in normalized coordinates and\n",
    "        missing keypoints should be encoded as NaN.\n",
    "      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape\n",
    "        [num_boxes] containing weights for groundtruth boxes.\n",
    "      groundtruth_is_crowd_list: A list of 1-D tf.bool tensors of shape\n",
    "        [num_boxes] containing is_crowd annotations\n",
    "    \"\"\"\n",
    "    self._groundtruth_lists[fields.BoxListFields.boxes] = groundtruth_boxes_list\n",
    "    self._groundtruth_lists[\n",
    "        fields.BoxListFields.classes] = groundtruth_classes_list\n",
    "    if groundtruth_weights_list:\n",
    "      self._groundtruth_lists[fields.BoxListFields.\n",
    "                              weights] = groundtruth_weights_list\n",
    "    if groundtruth_masks_list:\n",
    "      self._groundtruth_lists[\n",
    "          fields.BoxListFields.masks] = groundtruth_masks_list\n",
    "    if groundtruth_keypoints_list:\n",
    "      self._groundtruth_lists[\n",
    "          fields.BoxListFields.keypoints] = groundtruth_keypoints_list\n",
    "    if groundtruth_is_crowd_list:\n",
    "      self._groundtruth_lists[\n",
    "          fields.BoxListFields.is_crowd] = groundtruth_is_crowd_list\n",
    "\n",
    "  @abstractmethod\n",
    "  def restore_map(self, fine_tune_checkpoint_type='detection'):\n",
    "    \"\"\"Returns a map of variables to load from a foreign checkpoint.\n",
    "\n",
    "    Returns a map of variable names to load from a checkpoint to variables in\n",
    "    the model graph. This enables the model to initialize based on weights from\n",
    "    another task. For example, the feature extractor variables from a\n",
    "    classification model can be used to bootstrap training of an object\n",
    "    detector. When loading from an object detection model, the checkpoint model\n",
    "    should have the same parameters as this detection model with exception of\n",
    "    the num_classes parameter.\n",
    "\n",
    "    Args:\n",
    "      fine_tune_checkpoint_type: whether to restore from a full detection\n",
    "        checkpoint (with compatible variable names) or to restore from a\n",
    "        classification checkpoint for initialization prior to training.\n",
    "        Valid values: `detection`, `classification`. Default 'detection'.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping variable names (to load from a checkpoint) to variables in\n",
    "      the model graph.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# meta_architectures/faster_rcnn_meta_arch.py\n",
    "class FasterRCNNMetaArch(DetectionModel):\n",
    "  \"\"\"Faster R-CNN Meta-architecture definition.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               is_training,\n",
    "               num_classes,\n",
    "               image_resizer_fn,\n",
    "               feature_extractor,\n",
    "               number_of_stages,\n",
    "               first_stage_anchor_generator,\n",
    "               first_stage_atrous_rate,\n",
    "               first_stage_box_predictor_arg_scope_fn,\n",
    "               first_stage_box_predictor_kernel_size,\n",
    "               first_stage_box_predictor_depth,\n",
    "               first_stage_minibatch_size,\n",
    "               first_stage_positive_balance_fraction,\n",
    "               first_stage_nms_score_threshold,\n",
    "               first_stage_nms_iou_threshold,\n",
    "               first_stage_max_proposals,\n",
    "               first_stage_localization_loss_weight,\n",
    "               first_stage_objectness_loss_weight,\n",
    "               initial_crop_size,\n",
    "               maxpool_kernel_size,\n",
    "               maxpool_stride,\n",
    "               second_stage_mask_rcnn_box_predictor,\n",
    "               second_stage_batch_size,\n",
    "               second_stage_balance_fraction,\n",
    "               second_stage_non_max_suppression_fn,\n",
    "               second_stage_score_conversion_fn,\n",
    "               second_stage_localization_loss_weight,\n",
    "               second_stage_classification_loss_weight,\n",
    "               second_stage_classification_loss,\n",
    "               second_stage_mask_prediction_loss_weight=1.0,\n",
    "               hard_example_miner=None,\n",
    "               parallel_iterations=16,\n",
    "               add_summaries=True,\n",
    "               use_matmul_crop_and_resize=False):\n",
    "    \"\"\"FasterRCNNMetaArch Constructor.\n",
    "\n",
    "    Args:\n",
    "      is_training: A boolean indicating whether the training version of the\n",
    "        computation graph should be constructed.\n",
    "      num_classes: Number of classes.  Note that num_classes *does not*\n",
    "        include the background category, so if groundtruth labels take values\n",
    "        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\n",
    "        assigned classification targets can range from {0,... K}).\n",
    "      image_resizer_fn: A callable for image resizing.  This callable\n",
    "        takes a rank-3 image tensor of shape [height, width, channels]\n",
    "        (corresponding to a single image), an optional rank-3 instance mask\n",
    "        tensor of shape [num_masks, height, width] and returns a resized rank-3\n",
    "        image tensor, a resized mask tensor if one was provided in the input. In\n",
    "        addition this callable must also return a 1-D tensor of the form\n",
    "        [height, width, channels] containing the size of the true image, as the\n",
    "        image resizer can perform zero padding. See protos/image_resizer.proto.\n",
    "      feature_extractor: A FasterRCNNFeatureExtractor object.\n",
    "      number_of_stages:  An integer values taking values in {1, 2, 3}. If\n",
    "        1, the function will construct only the Region Proposal Network (RPN)\n",
    "        part of the model. If 2, the function will perform box refinement and\n",
    "        other auxiliary predictions all in the second stage. If 3, it will\n",
    "        extract features from refined boxes and perform the auxiliary\n",
    "        predictions on the non-maximum suppressed refined boxes.\n",
    "        If is_training is true and the value of number_of_stages is 3, it is\n",
    "        reduced to 2 since all the model heads are trained in parallel in second\n",
    "        stage during training.\n",
    "      first_stage_anchor_generator: An anchor_generator.AnchorGenerator object\n",
    "        (note that currently we only support\n",
    "        grid_anchor_generator.GridAnchorGenerator objects)\n",
    "      first_stage_atrous_rate: A single integer indicating the atrous rate for\n",
    "        the single convolution op which is applied to the `rpn_features_to_crop`\n",
    "        tensor to obtain a tensor to be used for box prediction. Some feature\n",
    "        extractors optionally allow for producing feature maps computed at\n",
    "        denser resolutions.  The atrous rate is used to compensate for the\n",
    "        denser feature maps by using an effectively larger receptive field.\n",
    "        (This should typically be set to 1).\n",
    "      first_stage_box_predictor_arg_scope_fn: A function to construct tf-slim\n",
    "        arg_scope for conv2d, separable_conv2d and fully_connected ops for the\n",
    "        RPN box predictor.\n",
    "      first_stage_box_predictor_kernel_size: Kernel size to use for the\n",
    "        convolution op just prior to RPN box predictions.\n",
    "      first_stage_box_predictor_depth: Output depth for the convolution op\n",
    "        just prior to RPN box predictions.\n",
    "      first_stage_minibatch_size: The \"batch size\" to use for computing the\n",
    "        objectness and location loss of the region proposal network. This\n",
    "        \"batch size\" refers to the number of anchors selected as contributing\n",
    "        to the loss function for any given image within the image batch and is\n",
    "        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\n",
    "      first_stage_positive_balance_fraction: Fraction of positive examples\n",
    "        per image for the RPN. The recommended value for Faster RCNN is 0.5.\n",
    "      first_stage_nms_score_threshold: Score threshold for non max suppression\n",
    "        for the Region Proposal Network (RPN).  This value is expected to be in\n",
    "        [0, 1] as it is applied directly after a softmax transformation.  The\n",
    "        recommended value for Faster R-CNN is 0.\n",
    "      first_stage_nms_iou_threshold: The Intersection Over Union (IOU) threshold\n",
    "        for performing Non-Max Suppression (NMS) on the boxes predicted by the\n",
    "        Region Proposal Network (RPN).\n",
    "      first_stage_max_proposals: Maximum number of boxes to retain after\n",
    "        performing Non-Max Suppression (NMS) on the boxes predicted by the\n",
    "        Region Proposal Network (RPN).\n",
    "      first_stage_localization_loss_weight: A float\n",
    "      first_stage_objectness_loss_weight: A float\n",
    "      initial_crop_size: A single integer indicating the output size\n",
    "        (width and height are set to be the same) of the initial bilinear\n",
    "        interpolation based cropping during ROI pooling.\n",
    "      maxpool_kernel_size: A single integer indicating the kernel size of the\n",
    "        max pool op on the cropped feature map during ROI pooling.\n",
    "      maxpool_stride: A single integer indicating the stride of the max pool\n",
    "        op on the cropped feature map during ROI pooling.\n",
    "      second_stage_mask_rcnn_box_predictor: Mask R-CNN box predictor to use for\n",
    "        the second stage.\n",
    "      second_stage_batch_size: The batch size used for computing the\n",
    "        classification and refined location loss of the box classifier.  This\n",
    "        \"batch size\" refers to the number of proposals selected as contributing\n",
    "        to the loss function for any given image within the image batch and is\n",
    "        only called \"batch_size\" due to terminology from the Faster R-CNN paper.\n",
    "      second_stage_balance_fraction: Fraction of positive examples to use\n",
    "        per image for the box classifier. The recommended value for Faster RCNN\n",
    "        is 0.25.\n",
    "      second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression\n",
    "        callable that takes `boxes`, `scores`, optional `clip_window` and\n",
    "        optional (kwarg) `mask` inputs (with all other inputs already set)\n",
    "        and returns a dictionary containing tensors with keys:\n",
    "        `detection_boxes`, `detection_scores`, `detection_classes`,\n",
    "        `num_detections`, and (optionally) `detection_masks`. See\n",
    "        `post_processing.batch_multiclass_non_max_suppression` for the type and\n",
    "        shape of these tensors.\n",
    "      second_stage_score_conversion_fn: Callable elementwise nonlinearity\n",
    "        (that takes tensors as inputs and returns tensors).  This is usually\n",
    "        used to convert logits to probabilities.\n",
    "      second_stage_localization_loss_weight: A float indicating the scale factor\n",
    "        for second stage localization loss.\n",
    "      second_stage_classification_loss_weight: A float indicating the scale\n",
    "        factor for second stage classification loss.\n",
    "      second_stage_classification_loss: Classification loss used by the second\n",
    "        stage classifier. Either losses.WeightedSigmoidClassificationLoss or\n",
    "        losses.WeightedSoftmaxClassificationLoss.\n",
    "      second_stage_mask_prediction_loss_weight: A float indicating the scale\n",
    "        factor for second stage mask prediction loss. This is applicable only if\n",
    "        second stage box predictor is configured to predict masks.\n",
    "      hard_example_miner:  A losses.HardExampleMiner object (can be None).\n",
    "      parallel_iterations: (Optional) The number of iterations allowed to run\n",
    "        in parallel for calls to tf.map_fn.\n",
    "      add_summaries: boolean (default: True) controlling whether summary ops\n",
    "        should be added to tensorflow graph.\n",
    "      use_matmul_crop_and_resize: Force the use of matrix multiplication based\n",
    "        crop and resize instead of standard tf.image.crop_and_resize while\n",
    "        computing second stage input feature maps.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at\n",
    "        training time.\n",
    "      ValueError: If first_stage_anchor_generator is not of type\n",
    "        grid_anchor_generator.GridAnchorGenerator.\n",
    "    \"\"\"\n",
    "    # TODO(rathodv): add_summaries is currently unused. Respect that directive\n",
    "    # in the future.\n",
    "    super(FasterRCNNMetaArch, self).__init__(num_classes=num_classes)\n",
    "\n",
    "    if is_training and second_stage_batch_size > first_stage_max_proposals:\n",
    "      raise ValueError('second_stage_batch_size should be no greater than '\n",
    "                       'first_stage_max_proposals.')\n",
    "    if not isinstance(first_stage_anchor_generator,\n",
    "                      GridAnchorGenerator):\n",
    "      raise ValueError('first_stage_anchor_generator must be of type '\n",
    "                       'grid_anchor_generator.GridAnchorGenerator.')\n",
    "\n",
    "    self._is_training = is_training\n",
    "    self._image_resizer_fn = image_resizer_fn\n",
    "    self._feature_extractor = feature_extractor\n",
    "    self._number_of_stages = number_of_stages\n",
    "\n",
    "    # The first class is reserved as background.\n",
    "    unmatched_cls_target = tf.constant(\n",
    "        [1] + self._num_classes * [0], dtype=tf.float32)\n",
    "    self._proposal_target_assigner = target_assigner.create_target_assigner(\n",
    "        'FasterRCNN', 'proposal')\n",
    "    self._detector_target_assigner = target_assigner.create_target_assigner(\n",
    "        'FasterRCNN', 'detection', unmatched_cls_target=unmatched_cls_target)\n",
    "    # Both proposal and detector target assigners use the same box coder\n",
    "    self._box_coder = self._proposal_target_assigner.box_coder\n",
    "\n",
    "    # (First stage) Region proposal network parameters\n",
    "    self._first_stage_anchor_generator = first_stage_anchor_generator\n",
    "    self._first_stage_atrous_rate = first_stage_atrous_rate\n",
    "    self._first_stage_box_predictor_arg_scope_fn = (\n",
    "        first_stage_box_predictor_arg_scope_fn)\n",
    "    self._first_stage_box_predictor_kernel_size = (\n",
    "        first_stage_box_predictor_kernel_size)\n",
    "    self._first_stage_box_predictor_depth = first_stage_box_predictor_depth\n",
    "    self._first_stage_minibatch_size = first_stage_minibatch_size\n",
    "    self._first_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n",
    "        positive_fraction=first_stage_positive_balance_fraction)\n",
    "    self._first_stage_box_predictor = box_predictor.ConvolutionalBoxPredictor(\n",
    "        self._is_training, num_classes=1,\n",
    "        conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,\n",
    "        min_depth=0, max_depth=0, num_layers_before_predictor=0,\n",
    "        use_dropout=False, dropout_keep_prob=1.0, kernel_size=1,\n",
    "        box_code_size=self._box_coder.code_size)\n",
    "\n",
    "    self._first_stage_nms_score_threshold = first_stage_nms_score_threshold\n",
    "    self._first_stage_nms_iou_threshold = first_stage_nms_iou_threshold\n",
    "    self._first_stage_max_proposals = first_stage_max_proposals\n",
    "\n",
    "    self._first_stage_localization_loss = (\n",
    "        losses.WeightedSmoothL1LocalizationLoss())\n",
    "    self._first_stage_objectness_loss = (\n",
    "        losses.WeightedSoftmaxClassificationLoss())\n",
    "    self._first_stage_loc_loss_weight = first_stage_localization_loss_weight\n",
    "    self._first_stage_obj_loss_weight = first_stage_objectness_loss_weight\n",
    "\n",
    "    # Per-region cropping parameters\n",
    "    self._initial_crop_size = initial_crop_size\n",
    "    self._maxpool_kernel_size = maxpool_kernel_size\n",
    "    self._maxpool_stride = maxpool_stride\n",
    "\n",
    "    self._mask_rcnn_box_predictor = second_stage_mask_rcnn_box_predictor\n",
    "\n",
    "    self._second_stage_batch_size = second_stage_batch_size\n",
    "    self._second_stage_sampler = sampler.BalancedPositiveNegativeSampler(\n",
    "        positive_fraction=second_stage_balance_fraction)\n",
    "\n",
    "    self._second_stage_nms_fn = second_stage_non_max_suppression_fn\n",
    "    self._second_stage_score_conversion_fn = second_stage_score_conversion_fn\n",
    "\n",
    "    self._second_stage_localization_loss = (\n",
    "        losses.WeightedSmoothL1LocalizationLoss())\n",
    "    self._second_stage_classification_loss = second_stage_classification_loss\n",
    "    self._second_stage_mask_loss = (\n",
    "        losses.WeightedSigmoidClassificationLoss())\n",
    "    self._second_stage_loc_loss_weight = second_stage_localization_loss_weight\n",
    "    self._second_stage_cls_loss_weight = second_stage_classification_loss_weight\n",
    "    self._second_stage_mask_loss_weight = (\n",
    "        second_stage_mask_prediction_loss_weight)\n",
    "    self._use_matmul_crop_and_resize = use_matmul_crop_and_resize\n",
    "    self._hard_example_miner = hard_example_miner\n",
    "    self._parallel_iterations = parallel_iterations\n",
    "\n",
    "    if self._number_of_stages <= 0 or self._number_of_stages > 3:\n",
    "      raise ValueError('Number of stages should be a value in {1, 2, 3}.')\n",
    "\n",
    "  @property\n",
    "  def first_stage_feature_extractor_scope(self):\n",
    "    return 'FirstStageFeatureExtractor'\n",
    "\n",
    "  @property\n",
    "  def second_stage_feature_extractor_scope(self):\n",
    "    return 'SecondStageFeatureExtractor'\n",
    "\n",
    "  @property\n",
    "  def first_stage_box_predictor_scope(self):\n",
    "    return 'FirstStageBoxPredictor'\n",
    "\n",
    "  @property\n",
    "  def second_stage_box_predictor_scope(self):\n",
    "    return 'SecondStageBoxPredictor'\n",
    "\n",
    "  @property\n",
    "  def max_num_proposals(self):\n",
    "    \"\"\"Max number of proposals (to pad to) for each image in the input batch.\n",
    "\n",
    "    At training time, this is set to be the `second_stage_batch_size` if hard\n",
    "    example miner is not configured, else it is set to\n",
    "    `first_stage_max_proposals`. At inference time, this is always set to\n",
    "    `first_stage_max_proposals`.\n",
    "\n",
    "    Returns:\n",
    "      A positive integer.\n",
    "    \"\"\"\n",
    "    if self._is_training and not self._hard_example_miner:\n",
    "      return self._second_stage_batch_size\n",
    "    return self._first_stage_max_proposals\n",
    "\n",
    "  @property\n",
    "  def anchors(self):\n",
    "    if not self._anchors:\n",
    "      raise RuntimeError('anchors have not been constructed yet!')\n",
    "    if not isinstance(self._anchors, box_list.BoxList):\n",
    "      raise RuntimeError('anchors should be a BoxList object, but is not.')\n",
    "    return self._anchors\n",
    "\n",
    "  def preprocess(self, inputs):\n",
    "    \"\"\"Feature-extractor specific preprocessing.\n",
    "\n",
    "    See base class.\n",
    "\n",
    "    For Faster R-CNN, we perform image resizing in the base class --- each\n",
    "    class subclassing FasterRCNNMetaArch is responsible for any additional\n",
    "    preprocessing (e.g., scaling pixel values to be in [-1, 1]).\n",
    "\n",
    "    Args:\n",
    "      inputs: a [batch, height_in, width_in, channels] float tensor representing\n",
    "        a batch of images with values between 0 and 255.0.\n",
    "\n",
    "    Returns:\n",
    "      preprocessed_inputs: a [batch, height_out, width_out, channels] float\n",
    "        tensor representing a batch of images.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "    Raises:\n",
    "      ValueError: if inputs tensor does not have type tf.float32\n",
    "    \"\"\"\n",
    "    if inputs.dtype is not tf.float32:\n",
    "      raise ValueError('`preprocess` expects a tf.float32 tensor')\n",
    "    with tf.name_scope('Preprocessor'):\n",
    "      outputs = shape_utils.static_or_dynamic_map_fn(\n",
    "          self._image_resizer_fn,\n",
    "          elems=inputs,\n",
    "          dtype=[tf.float32, tf.int32],\n",
    "          parallel_iterations=self._parallel_iterations)\n",
    "      resized_inputs = outputs[0]\n",
    "      true_image_shapes = outputs[1]\n",
    "      return (self._feature_extractor.preprocess(resized_inputs),\n",
    "              true_image_shapes)\n",
    "\n",
    "  def _compute_clip_window(self, image_shapes):\n",
    "    \"\"\"Computes clip window for non max suppression based on image shapes.\n",
    "\n",
    "    This function assumes that the clip window's left top corner is at (0, 0).\n",
    "\n",
    "    Args:\n",
    "      image_shapes: A 2-D int32 tensor of shape [batch_size, 3] containing\n",
    "      shapes of images in the batch. Each row represents [height, width,\n",
    "      channels] of an image.\n",
    "\n",
    "    Returns:\n",
    "      A 2-D float32 tensor of shape [batch_size, 4] containing the clip window\n",
    "      for each image in the form [ymin, xmin, ymax, xmax].\n",
    "    \"\"\"\n",
    "    clip_heights = image_shapes[:, 0]\n",
    "    clip_widths = image_shapes[:, 1]\n",
    "    clip_window = tf.to_float(tf.stack([tf.zeros_like(clip_heights),\n",
    "                                        tf.zeros_like(clip_heights),\n",
    "                                        clip_heights, clip_widths], axis=1))\n",
    "    return clip_window\n",
    "\n",
    "  def predict(self, preprocessed_inputs, true_image_shapes):\n",
    "    \"\"\"Predicts unpostprocessed tensors from input tensor.\n",
    "\n",
    "    This function takes an input batch of images and runs it through the\n",
    "    forward pass of the network to yield \"raw\" un-postprocessed predictions.\n",
    "    If `number_of_stages` is 1, this function only returns first stage\n",
    "    RPN predictions (un-postprocessed).  Otherwise it returns both\n",
    "    first stage RPN predictions as well as second stage box classifier\n",
    "    predictions.\n",
    "\n",
    "    Other remarks:\n",
    "    + Anchor pruning vs. clipping: following the recommendation of the Faster\n",
    "    R-CNN paper, we prune anchors that venture outside the image window at\n",
    "    training time and clip anchors to the image window at inference time.\n",
    "    + Proposal padding: as described at the top of the file, proposals are\n",
    "    padded to self._max_num_proposals and flattened so that proposals from all\n",
    "    images within the input batch are arranged along the same batch dimension.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: a [batch, height, width, channels] float tensor\n",
    "        representing a batch of images.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      prediction_dict: a dictionary holding \"raw\" prediction tensors:\n",
    "        1) rpn_box_predictor_features: A 4-D float32 tensor with shape\n",
    "          [batch_size, height, width, depth] to be used for predicting proposal\n",
    "          boxes and corresponding objectness scores.\n",
    "        2) rpn_features_to_crop: A 4-D float32 tensor with shape\n",
    "          [batch_size, height, width, depth] representing image features to crop\n",
    "          using the proposal boxes predicted by the RPN.\n",
    "        3) image_shape: a 1-D tensor of shape [4] representing the input\n",
    "          image shape.\n",
    "        4) rpn_box_encodings:  3-D float tensor of shape\n",
    "          [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "          predicted boxes.\n",
    "        5) rpn_objectness_predictions_with_background: 3-D float tensor of shape\n",
    "          [batch_size, num_anchors, 2] containing class\n",
    "          predictions (logits) for each of the anchors.  Note that this\n",
    "          tensor *includes* background class predictions (at class index 0).\n",
    "        6) anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n",
    "          for the first stage RPN (in absolute coordinates).  Note that\n",
    "          `num_anchors` can differ depending on whether the model is created in\n",
    "          training or inference mode.\n",
    "\n",
    "        (and if number_of_stages > 1):\n",
    "        7) refined_box_encodings: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes, self._box_coder.code_size]\n",
    "          representing predicted (final) refined box encodings, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals. If using\n",
    "          a shared box across classes the shape will instead be\n",
    "          [total_num_proposals, 1, self._box_coder.code_size].\n",
    "        8) class_predictions_with_background: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes + 1] containing class\n",
    "          predictions (logits) for each of the anchors, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals.\n",
    "          Note that this tensor *includes* background class predictions\n",
    "          (at class index 0).\n",
    "        9) num_proposals: An int32 tensor of shape [batch_size] representing the\n",
    "          number of proposals generated by the RPN.  `num_proposals` allows us\n",
    "          to keep track of which entries are to be treated as zero paddings and\n",
    "          which are not since we always pad the number of proposals to be\n",
    "          `self.max_num_proposals` for each image.\n",
    "        10) proposal_boxes: A float32 tensor of shape\n",
    "          [batch_size, self.max_num_proposals, 4] representing\n",
    "          decoded proposal bounding boxes in absolute coordinates.\n",
    "        11) mask_predictions: (optional) a 4-D tensor with shape\n",
    "          [total_num_padded_proposals, num_classes, mask_height, mask_width]\n",
    "          containing instance mask predictions.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `predict` is called before `preprocess`.\n",
    "    \"\"\"\n",
    "    (rpn_box_predictor_features, rpn_features_to_crop, anchors_boxlist,\n",
    "     image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)\n",
    "    (rpn_box_encodings, rpn_objectness_predictions_with_background\n",
    "    ) = self._predict_rpn_proposals(rpn_box_predictor_features)\n",
    "\n",
    "    # The Faster R-CNN paper recommends pruning anchors that venture outside\n",
    "    # the image window at training time and clipping at inference time.\n",
    "    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))\n",
    "    if self._is_training:\n",
    "      (rpn_box_encodings, rpn_objectness_predictions_with_background,\n",
    "       anchors_boxlist) = self._remove_invalid_anchors_and_predictions(\n",
    "           rpn_box_encodings, rpn_objectness_predictions_with_background,\n",
    "           anchors_boxlist, clip_window)\n",
    "    else:\n",
    "      anchors_boxlist = box_list_ops.clip_to_window(\n",
    "          anchors_boxlist, clip_window)\n",
    "\n",
    "    self._anchors = anchors_boxlist\n",
    "    prediction_dict = {\n",
    "        'rpn_box_predictor_features': rpn_box_predictor_features,\n",
    "        'rpn_features_to_crop': rpn_features_to_crop,\n",
    "        'image_shape': image_shape,\n",
    "        'rpn_box_encodings': rpn_box_encodings,\n",
    "        'rpn_objectness_predictions_with_background':\n",
    "        rpn_objectness_predictions_with_background,\n",
    "        'anchors': self._anchors.get()\n",
    "    }\n",
    "\n",
    "    if self._number_of_stages >= 2:\n",
    "      prediction_dict.update(self._predict_second_stage(\n",
    "          rpn_box_encodings,\n",
    "          rpn_objectness_predictions_with_background,\n",
    "          rpn_features_to_crop,\n",
    "          self._anchors.get(), image_shape, true_image_shapes))\n",
    "\n",
    "    if self._number_of_stages == 3:\n",
    "      prediction_dict = self._predict_third_stage(\n",
    "          prediction_dict, true_image_shapes)\n",
    "\n",
    "    return prediction_dict\n",
    "\n",
    "  def _image_batch_shape_2d(self, image_batch_shape_1d):\n",
    "    \"\"\"Takes a 1-D image batch shape tensor and converts it to a 2-D tensor.\n",
    "\n",
    "    Example:\n",
    "    If 1-D image batch shape tensor is [2, 300, 300, 3]. The corresponding 2-D\n",
    "    image batch tensor would be [[300, 300, 3], [300, 300, 3]]\n",
    "\n",
    "    Args:\n",
    "      image_batch_shape_1d: 1-D tensor of the form [batch_size, height,\n",
    "        width, channels].\n",
    "\n",
    "    Returns:\n",
    "      image_batch_shape_2d: 2-D tensor of shape [batch_size, 3] were each row is\n",
    "        of the form [height, width, channels].\n",
    "    \"\"\"\n",
    "    return tf.tile(tf.expand_dims(image_batch_shape_1d[1:], 0),\n",
    "                   [image_batch_shape_1d[0], 1])\n",
    "\n",
    "  def _predict_second_stage(self, rpn_box_encodings,\n",
    "                            rpn_objectness_predictions_with_background,\n",
    "                            rpn_features_to_crop,\n",
    "                            anchors,\n",
    "                            image_shape,\n",
    "                            true_image_shapes):\n",
    "    \"\"\"Predicts the output tensors from second stage of Faster R-CNN.\n",
    "\n",
    "    Args:\n",
    "      rpn_box_encodings: 4-D float tensor of shape\n",
    "        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n",
    "        predicted boxes.\n",
    "      rpn_objectness_predictions_with_background: 2-D float tensor of shape\n",
    "        [batch_size, num_valid_anchors, 2] containing class\n",
    "        predictions (logits) for each of the anchors.  Note that this\n",
    "        tensor *includes* background class predictions (at class index 0).\n",
    "      rpn_features_to_crop: A 4-D float32 tensor with shape\n",
    "        [batch_size, height, width, depth] representing image features to crop\n",
    "        using the proposal boxes predicted by the RPN.\n",
    "      anchors: 2-D float tensor of shape\n",
    "        [num_anchors, self._box_coder.code_size].\n",
    "      image_shape: A 1D int32 tensors of size [4] containing the image shape.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      prediction_dict: a dictionary holding \"raw\" prediction tensors:\n",
    "        1) refined_box_encodings: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes, self._box_coder.code_size]\n",
    "          representing predicted (final) refined box encodings, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals. If using a\n",
    "          shared box across classes the shape will instead be\n",
    "          [total_num_proposals, 1, self._box_coder.code_size].\n",
    "        2) class_predictions_with_background: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes + 1] containing class\n",
    "          predictions (logits) for each of the anchors, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals.\n",
    "          Note that this tensor *includes* background class predictions\n",
    "          (at class index 0).\n",
    "        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n",
    "          number of proposals generated by the RPN.  `num_proposals` allows us\n",
    "          to keep track of which entries are to be treated as zero paddings and\n",
    "          which are not since we always pad the number of proposals to be\n",
    "          `self.max_num_proposals` for each image.\n",
    "        4) proposal_boxes: A float32 tensor of shape\n",
    "          [batch_size, self.max_num_proposals, 4] representing\n",
    "          decoded proposal bounding boxes in absolute coordinates.\n",
    "        5) proposal_boxes_normalized: A float32 tensor of shape\n",
    "          [batch_size, self.max_num_proposals, 4] representing decoded proposal\n",
    "          bounding boxes in normalized coordinates. Can be used to override the\n",
    "          boxes proposed by the RPN, thus enabling one to extract features and\n",
    "          get box classification and prediction for externally selected areas\n",
    "          of the image.\n",
    "        6) box_classifier_features: a 4-D float32 tensor representing the\n",
    "          features for each proposal.\n",
    "    \"\"\"\n",
    "    image_shape_2d = self._image_batch_shape_2d(image_shape)\n",
    "    proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(\n",
    "        rpn_box_encodings, rpn_objectness_predictions_with_background,\n",
    "        anchors, image_shape_2d, true_image_shapes)\n",
    "\n",
    "    flattened_proposal_feature_maps = (\n",
    "        self._compute_second_stage_input_feature_maps(\n",
    "            rpn_features_to_crop, proposal_boxes_normalized))\n",
    "\n",
    "    box_classifier_features = (\n",
    "        self._feature_extractor.extract_box_classifier_features(\n",
    "            flattened_proposal_feature_maps,\n",
    "            scope=self.second_stage_feature_extractor_scope))\n",
    "\n",
    "    box_predictions = self._mask_rcnn_box_predictor.predict(\n",
    "        [box_classifier_features],\n",
    "        num_predictions_per_location=[1],\n",
    "        scope=self.second_stage_box_predictor_scope,\n",
    "        predict_boxes_and_classes=True)\n",
    "\n",
    "    refined_box_encodings = tf.squeeze(\n",
    "        box_predictions[box_predictor.BOX_ENCODINGS],\n",
    "        axis=1, name='all_refined_box_encodings')\n",
    "    class_predictions_with_background = tf.squeeze(\n",
    "        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n",
    "        axis=1, name='all_class_predictions_with_background')\n",
    "\n",
    "    absolute_proposal_boxes = ops.normalized_to_image_coordinates(\n",
    "        proposal_boxes_normalized, image_shape, self._parallel_iterations)\n",
    "\n",
    "    prediction_dict = {\n",
    "        'refined_box_encodings': refined_box_encodings,\n",
    "        'class_predictions_with_background':\n",
    "        class_predictions_with_background,\n",
    "        'num_proposals': num_proposals,\n",
    "        'proposal_boxes': absolute_proposal_boxes,\n",
    "        'box_classifier_features': box_classifier_features,\n",
    "        'proposal_boxes_normalized': proposal_boxes_normalized,\n",
    "    }\n",
    "\n",
    "    return prediction_dict\n",
    "\n",
    "  def _predict_third_stage(self, prediction_dict, image_shapes):\n",
    "    \"\"\"Predicts non-box, non-class outputs using refined detections.\n",
    "\n",
    "    For training, masks as predicted directly on the box_classifier_features,\n",
    "    which are region-features from the initial anchor boxes.\n",
    "    For inference, this happens after calling the post-processing stage, such\n",
    "    that masks are only calculated for the top scored boxes.\n",
    "\n",
    "    Args:\n",
    "     prediction_dict: a dictionary holding \"raw\" prediction tensors:\n",
    "        1) refined_box_encodings: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes, self._box_coder.code_size]\n",
    "          representing predicted (final) refined box encodings, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals. If using a\n",
    "          shared box across classes the shape will instead be\n",
    "          [total_num_proposals, 1, self._box_coder.code_size].\n",
    "        2) class_predictions_with_background: a 3-D tensor with shape\n",
    "          [total_num_proposals, num_classes + 1] containing class\n",
    "          predictions (logits) for each of the anchors, where\n",
    "          total_num_proposals=batch_size*self._max_num_proposals.\n",
    "          Note that this tensor *includes* background class predictions\n",
    "          (at class index 0).\n",
    "        3) num_proposals: An int32 tensor of shape [batch_size] representing the\n",
    "          number of proposals generated by the RPN.  `num_proposals` allows us\n",
    "          to keep track of which entries are to be treated as zero paddings and\n",
    "          which are not since we always pad the number of proposals to be\n",
    "          `self.max_num_proposals` for each image.\n",
    "        4) proposal_boxes: A float32 tensor of shape\n",
    "          [batch_size, self.max_num_proposals, 4] representing\n",
    "          decoded proposal bounding boxes in absolute coordinates.\n",
    "        5) box_classifier_features: a 4-D float32 tensor representing the\n",
    "          features for each proposal.\n",
    "      image_shapes: A 2-D int32 tensors of shape [batch_size, 3] containing\n",
    "        shapes of images in the batch.\n",
    "\n",
    "    Returns:\n",
    "      prediction_dict: a dictionary that in addition to the input predictions\n",
    "      does hold the following predictions as well:\n",
    "        1) mask_predictions: a 4-D tensor with shape\n",
    "          [batch_size, max_detection, mask_height, mask_width] containing\n",
    "          instance mask predictions.\n",
    "    \"\"\"\n",
    "    if self._is_training:\n",
    "      curr_box_classifier_features = prediction_dict['box_classifier_features']\n",
    "      detection_classes = prediction_dict['class_predictions_with_background']\n",
    "      mask_predictions = self._mask_rcnn_box_predictor.predict(\n",
    "          [curr_box_classifier_features],\n",
    "          num_predictions_per_location=[1],\n",
    "          scope=self.second_stage_box_predictor_scope,\n",
    "          predict_boxes_and_classes=False,\n",
    "          predict_auxiliary_outputs=True)\n",
    "      prediction_dict['mask_predictions'] = tf.squeeze(mask_predictions[\n",
    "          box_predictor.MASK_PREDICTIONS], axis=1)\n",
    "    else:\n",
    "      detections_dict = self._postprocess_box_classifier(\n",
    "          prediction_dict['refined_box_encodings'],\n",
    "          prediction_dict['class_predictions_with_background'],\n",
    "          prediction_dict['proposal_boxes'],\n",
    "          prediction_dict['num_proposals'],\n",
    "          image_shapes)\n",
    "      prediction_dict.update(detections_dict)\n",
    "      detection_boxes = detections_dict[\n",
    "          fields.DetectionResultFields.detection_boxes]\n",
    "      detection_classes = detections_dict[\n",
    "          fields.DetectionResultFields.detection_classes]\n",
    "      rpn_features_to_crop = prediction_dict['rpn_features_to_crop']\n",
    "      batch_size = tf.shape(detection_boxes)[0]\n",
    "      max_detection = tf.shape(detection_boxes)[1]\n",
    "      flattened_detected_feature_maps = (\n",
    "          self._compute_second_stage_input_feature_maps(\n",
    "              rpn_features_to_crop, detection_boxes))\n",
    "      curr_box_classifier_features = (\n",
    "          self._feature_extractor.extract_box_classifier_features(\n",
    "              flattened_detected_feature_maps,\n",
    "              scope=self.second_stage_feature_extractor_scope))\n",
    "\n",
    "      mask_predictions = self._mask_rcnn_box_predictor.predict(\n",
    "          [curr_box_classifier_features],\n",
    "          num_predictions_per_location=[1],\n",
    "          scope=self.second_stage_box_predictor_scope,\n",
    "          predict_boxes_and_classes=False,\n",
    "          predict_auxiliary_outputs=True)\n",
    "\n",
    "      detection_masks = tf.squeeze(mask_predictions[\n",
    "          box_predictor.MASK_PREDICTIONS], axis=1)\n",
    "\n",
    "      _, num_classes, mask_height, mask_width = (\n",
    "          detection_masks.get_shape().as_list())\n",
    "      _, max_detection = detection_classes.get_shape().as_list()\n",
    "      if num_classes > 1:\n",
    "        detection_masks = self._gather_instance_masks(\n",
    "            detection_masks, detection_classes)\n",
    "\n",
    "      prediction_dict[fields.DetectionResultFields.detection_masks] = (\n",
    "          tf.reshape(detection_masks,\n",
    "                     [batch_size, max_detection, mask_height, mask_width]))\n",
    "\n",
    "    return prediction_dict\n",
    "\n",
    "  def _gather_instance_masks(self, instance_masks, classes):\n",
    "    \"\"\"Gathers the masks that correspond to classes.\n",
    "\n",
    "    Args:\n",
    "      instance_masks: A 4-D float32 tensor with shape\n",
    "        [K, num_classes, mask_height, mask_width].\n",
    "      classes: A 2-D int32 tensor with shape [batch_size, max_detection].\n",
    "\n",
    "    Returns:\n",
    "      masks: a 3-D float32 tensor with shape [K, mask_height, mask_width].\n",
    "    \"\"\"\n",
    "    _, num_classes, height, width = instance_masks.get_shape().as_list()\n",
    "    k = tf.shape(instance_masks)[0]\n",
    "    instance_masks = tf.reshape(instance_masks, [-1, height, width])\n",
    "    classes = tf.to_int32(tf.reshape(classes, [-1]))\n",
    "    gather_idx = tf.range(k) * num_classes + classes\n",
    "    return tf.gather(instance_masks, gather_idx)\n",
    "\n",
    "  def _extract_rpn_feature_maps(self, preprocessed_inputs):\n",
    "    \"\"\"Extracts RPN features.\n",
    "\n",
    "    This function extracts two feature maps: a feature map to be directly\n",
    "    fed to a box predictor (to predict location and objectness scores for\n",
    "    proposals) and a feature map from which to crop regions which will then\n",
    "    be sent to the second stage box classifier.\n",
    "\n",
    "    Args:\n",
    "      preprocessed_inputs: a [batch, height, width, channels] image tensor.\n",
    "\n",
    "    Returns:\n",
    "      rpn_box_predictor_features: A 4-D float32 tensor with shape\n",
    "        [batch, height, width, depth] to be used for predicting proposal boxes\n",
    "        and corresponding objectness scores.\n",
    "      rpn_features_to_crop: A 4-D float32 tensor with shape\n",
    "        [batch, height, width, depth] representing image features to crop using\n",
    "        the proposals boxes.\n",
    "      anchors: A BoxList representing anchors (for the RPN) in\n",
    "        absolute coordinates.\n",
    "      image_shape: A 1-D tensor representing the input image shape.\n",
    "    \"\"\"\n",
    "    image_shape = tf.shape(preprocessed_inputs)\n",
    "    rpn_features_to_crop, _ = self._feature_extractor.extract_proposal_features(\n",
    "        preprocessed_inputs, scope=self.first_stage_feature_extractor_scope)\n",
    "\n",
    "    feature_map_shape = tf.shape(rpn_features_to_crop)\n",
    "    anchors = box_list_ops.concatenate(\n",
    "        self._first_stage_anchor_generator.generate([(feature_map_shape[1],\n",
    "                                                      feature_map_shape[2])]))\n",
    "    with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):\n",
    "      kernel_size = self._first_stage_box_predictor_kernel_size\n",
    "      rpn_box_predictor_features = slim.conv2d(\n",
    "          rpn_features_to_crop,\n",
    "          self._first_stage_box_predictor_depth,\n",
    "          kernel_size=[kernel_size, kernel_size],\n",
    "          rate=self._first_stage_atrous_rate,\n",
    "          activation_fn=tf.nn.relu6)\n",
    "    return (rpn_box_predictor_features, rpn_features_to_crop,\n",
    "            anchors, image_shape)\n",
    "\n",
    "  def _predict_rpn_proposals(self, rpn_box_predictor_features):\n",
    "    \"\"\"Adds box predictors to RPN feature map to predict proposals.\n",
    "\n",
    "    Note resulting tensors will not have been postprocessed.\n",
    "\n",
    "    Args:\n",
    "      rpn_box_predictor_features: A 4-D float32 tensor with shape\n",
    "        [batch, height, width, depth] to be used for predicting proposal boxes\n",
    "        and corresponding objectness scores.\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: 3-D float tensor of shape\n",
    "        [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "        predicted boxes.\n",
    "      objectness_predictions_with_background: 3-D float tensor of shape\n",
    "        [batch_size, num_anchors, 2] containing class\n",
    "        predictions (logits) for each of the anchors.  Note that this\n",
    "        tensor *includes* background class predictions (at class index 0).\n",
    "\n",
    "    Raises:\n",
    "      RuntimeError: if the anchor generator generates anchors corresponding to\n",
    "        multiple feature maps.  We currently assume that a single feature map\n",
    "        is generated for the RPN.\n",
    "    \"\"\"\n",
    "    num_anchors_per_location = (\n",
    "        self._first_stage_anchor_generator.num_anchors_per_location())\n",
    "    if len(num_anchors_per_location) != 1:\n",
    "      raise RuntimeError('anchor_generator is expected to generate anchors '\n",
    "                         'corresponding to a single feature map.')\n",
    "    box_predictions = self._first_stage_box_predictor.predict(\n",
    "        [rpn_box_predictor_features],\n",
    "        num_anchors_per_location,\n",
    "        scope=self.first_stage_box_predictor_scope)\n",
    "\n",
    "    box_encodings = tf.concat(\n",
    "        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)\n",
    "    objectness_predictions_with_background = tf.concat(\n",
    "        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],\n",
    "        axis=1)\n",
    "    return (tf.squeeze(box_encodings, axis=2),\n",
    "            objectness_predictions_with_background)\n",
    "\n",
    "  def _remove_invalid_anchors_and_predictions(\n",
    "      self,\n",
    "      box_encodings,\n",
    "      objectness_predictions_with_background,\n",
    "      anchors_boxlist,\n",
    "      clip_window):\n",
    "    \"\"\"Removes anchors that (partially) fall outside an image.\n",
    "\n",
    "    Also removes associated box encodings and objectness predictions.\n",
    "\n",
    "    Args:\n",
    "      box_encodings: 3-D float tensor of shape\n",
    "        [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "        predicted boxes.\n",
    "      objectness_predictions_with_background: 3-D float tensor of shape\n",
    "        [batch_size, num_anchors, 2] containing class\n",
    "        predictions (logits) for each of the anchors.  Note that this\n",
    "        tensor *includes* background class predictions (at class index 0).\n",
    "      anchors_boxlist: A BoxList representing num_anchors anchors (for the RPN)\n",
    "        in absolute coordinates.\n",
    "      clip_window: a 1-D tensor representing the [ymin, xmin, ymax, xmax]\n",
    "        extent of the window to clip/prune to.\n",
    "\n",
    "    Returns:\n",
    "      box_encodings: 4-D float tensor of shape\n",
    "        [batch_size, num_valid_anchors, self._box_coder.code_size] containing\n",
    "        predicted boxes, where num_valid_anchors <= num_anchors\n",
    "      objectness_predictions_with_background: 2-D float tensor of shape\n",
    "        [batch_size, num_valid_anchors, 2] containing class\n",
    "        predictions (logits) for each of the anchors, where\n",
    "        num_valid_anchors <= num_anchors.  Note that this\n",
    "        tensor *includes* background class predictions (at class index 0).\n",
    "      anchors: A BoxList representing num_valid_anchors anchors (for the RPN) in\n",
    "        absolute coordinates.\n",
    "    \"\"\"\n",
    "    pruned_anchors_boxlist, keep_indices = box_list_ops.prune_outside_window(\n",
    "        anchors_boxlist, clip_window)\n",
    "    def _batch_gather_kept_indices(predictions_tensor):\n",
    "      return shape_utils.static_or_dynamic_map_fn(\n",
    "          partial(tf.gather, indices=keep_indices),\n",
    "          elems=predictions_tensor,\n",
    "          dtype=tf.float32,\n",
    "          parallel_iterations=self._parallel_iterations,\n",
    "          back_prop=True)\n",
    "    return (_batch_gather_kept_indices(box_encodings),\n",
    "            _batch_gather_kept_indices(objectness_predictions_with_background),\n",
    "            pruned_anchors_boxlist)\n",
    "\n",
    "  def _flatten_first_two_dimensions(self, inputs):\n",
    "    \"\"\"Flattens `K-d` tensor along batch dimension to be a `(K-1)-d` tensor.\n",
    "\n",
    "    Converts `inputs` with shape [A, B, ..., depth] into a tensor of shape\n",
    "    [A * B, ..., depth].\n",
    "\n",
    "    Args:\n",
    "      inputs: A float tensor with shape [A, B, ..., depth].  Note that the first\n",
    "        two and last dimensions must be statically defined.\n",
    "    Returns:\n",
    "      A float tensor with shape [A * B, ..., depth] (where the first and last\n",
    "        dimension are statically defined.\n",
    "    \"\"\"\n",
    "    combined_shape = shape_utils.combined_static_and_dynamic_shape(inputs)\n",
    "    flattened_shape = tf.stack([combined_shape[0] * combined_shape[1]] +\n",
    "                               combined_shape[2:])\n",
    "    return tf.reshape(inputs, flattened_shape)\n",
    "\n",
    "  def postprocess(self, prediction_dict, true_image_shapes):\n",
    "    \"\"\"Convert prediction tensors to final detections.\n",
    "\n",
    "    This function converts raw predictions tensors to final detection results.\n",
    "    See base class for output format conventions.  Note also that by default,\n",
    "    scores are to be interpreted as logits, but if a score_converter is used,\n",
    "    then scores are remapped (and may thus have a different interpretation).\n",
    "\n",
    "    If number_of_stages=1, the returned results represent proposals from the\n",
    "    first stage RPN and are padded to have self.max_num_proposals for each\n",
    "    image; otherwise, the results can be interpreted as multiclass detections\n",
    "    from the full two-stage model and are padded to self._max_detections.\n",
    "\n",
    "    Args:\n",
    "      prediction_dict: a dictionary holding prediction tensors (see the\n",
    "        documentation for the predict method.  If number_of_stages=1, we\n",
    "        expect prediction_dict to contain `rpn_box_encodings`,\n",
    "        `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,\n",
    "        and `anchors` fields.  Otherwise we expect prediction_dict to\n",
    "        additionally contain `refined_box_encodings`,\n",
    "        `class_predictions_with_background`, `num_proposals`,\n",
    "        `proposal_boxes` and, optionally, `mask_predictions` fields.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      detections: a dictionary containing the following fields\n",
    "        detection_boxes: [batch, max_detection, 4]\n",
    "        detection_scores: [batch, max_detections]\n",
    "        detection_classes: [batch, max_detections]\n",
    "          (this entry is only created if rpn_mode=False)\n",
    "        num_detections: [batch]\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If `predict` is called before `preprocess`.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('FirstStagePostprocessor'):\n",
    "      if self._number_of_stages == 1:\n",
    "        proposal_boxes, proposal_scores, num_proposals = self._postprocess_rpn(\n",
    "            prediction_dict['rpn_box_encodings'],\n",
    "            prediction_dict['rpn_objectness_predictions_with_background'],\n",
    "            prediction_dict['anchors'],\n",
    "            true_image_shapes,\n",
    "            true_image_shapes)\n",
    "        return {\n",
    "            fields.DetectionResultFields.detection_boxes: proposal_boxes,\n",
    "            fields.DetectionResultFields.detection_scores: proposal_scores,\n",
    "            fields.DetectionResultFields.num_detections:\n",
    "                tf.to_float(num_proposals),\n",
    "        }\n",
    "\n",
    "    # TODO(jrru): Remove mask_predictions from _post_process_box_classifier.\n",
    "    with tf.name_scope('SecondStagePostprocessor'):\n",
    "      if (self._number_of_stages == 2 or\n",
    "          (self._number_of_stages == 3 and self._is_training)):\n",
    "        mask_predictions = prediction_dict.get(box_predictor.MASK_PREDICTIONS)\n",
    "        detections_dict = self._postprocess_box_classifier(\n",
    "            prediction_dict['refined_box_encodings'],\n",
    "            prediction_dict['class_predictions_with_background'],\n",
    "            prediction_dict['proposal_boxes'],\n",
    "            prediction_dict['num_proposals'],\n",
    "            true_image_shapes,\n",
    "            mask_predictions=mask_predictions)\n",
    "        return detections_dict\n",
    "\n",
    "    if self._number_of_stages == 3:\n",
    "      # Post processing is already performed in 3rd stage. We need to transfer\n",
    "      # postprocessed tensors from `prediction_dict` to `detections_dict`.\n",
    "      detections_dict = {}\n",
    "      for key in prediction_dict:\n",
    "        if key == fields.DetectionResultFields.detection_masks:\n",
    "          detections_dict[key] = tf.sigmoid(prediction_dict[key])\n",
    "        elif 'detection' in key:\n",
    "          detections_dict[key] = prediction_dict[key]\n",
    "      return detections_dict\n",
    "\n",
    "  def _postprocess_rpn(self,\n",
    "                       rpn_box_encodings_batch,\n",
    "                       rpn_objectness_predictions_with_background_batch,\n",
    "                       anchors,\n",
    "                       image_shapes,\n",
    "                       true_image_shapes):\n",
    "    \"\"\"Converts first stage prediction tensors from the RPN to proposals.\n",
    "\n",
    "    This function decodes the raw RPN predictions, runs non-max suppression\n",
    "    on the result.\n",
    "\n",
    "    Note that the behavior of this function is slightly modified during\n",
    "    training --- specifically, we stop the gradient from passing through the\n",
    "    proposal boxes and we only return a balanced sampled subset of proposals\n",
    "    with size `second_stage_batch_size`.\n",
    "\n",
    "    Args:\n",
    "      rpn_box_encodings_batch: A 3-D float32 tensor of shape\n",
    "        [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "        predicted proposal box encodings.\n",
    "      rpn_objectness_predictions_with_background_batch: A 3-D float tensor of\n",
    "        shape [batch_size, num_anchors, 2] containing objectness predictions\n",
    "        (logits) for each of the anchors with 0 corresponding to background\n",
    "        and 1 corresponding to object.\n",
    "      anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n",
    "        for the first stage RPN.  Note that `num_anchors` can differ depending\n",
    "        on whether the model is created in training or inference mode.\n",
    "      image_shapes: A 2-D tensor of shape [batch, 3] containing the shapes of\n",
    "        images in the batch.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      proposal_boxes: A float tensor with shape\n",
    "        [batch_size, max_num_proposals, 4] representing the (potentially zero\n",
    "        padded) proposal boxes for all images in the batch.  These boxes are\n",
    "        represented as normalized coordinates.\n",
    "      proposal_scores:  A float tensor with shape\n",
    "        [batch_size, max_num_proposals] representing the (potentially zero\n",
    "        padded) proposal objectness scores for all images in the batch.\n",
    "      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n",
    "        representing the number of proposals predicted for each image in\n",
    "        the batch.\n",
    "    \"\"\"\n",
    "    rpn_box_encodings_batch = tf.expand_dims(rpn_box_encodings_batch, axis=2)\n",
    "    rpn_encodings_shape = shape_utils.combined_static_and_dynamic_shape(\n",
    "        rpn_box_encodings_batch)\n",
    "    tiled_anchor_boxes = tf.tile(\n",
    "        tf.expand_dims(anchors, 0), [rpn_encodings_shape[0], 1, 1])\n",
    "    proposal_boxes = self._batch_decode_boxes(rpn_box_encodings_batch,\n",
    "                                              tiled_anchor_boxes)\n",
    "    proposal_boxes = tf.squeeze(proposal_boxes, axis=2)\n",
    "    rpn_objectness_softmax_without_background = tf.nn.softmax(\n",
    "        rpn_objectness_predictions_with_background_batch)[:, :, 1]\n",
    "    clip_window = self._compute_clip_window(image_shapes)\n",
    "    (proposal_boxes, proposal_scores, _, _, _,\n",
    "     num_proposals) = post_processing.batch_multiclass_non_max_suppression(\n",
    "         tf.expand_dims(proposal_boxes, axis=2),\n",
    "         tf.expand_dims(rpn_objectness_softmax_without_background,\n",
    "                        axis=2),\n",
    "         self._first_stage_nms_score_threshold,\n",
    "         self._first_stage_nms_iou_threshold,\n",
    "         self._first_stage_max_proposals,\n",
    "         self._first_stage_max_proposals,\n",
    "         clip_window=clip_window)\n",
    "    if self._is_training:\n",
    "      proposal_boxes = tf.stop_gradient(proposal_boxes)\n",
    "      if not self._hard_example_miner:\n",
    "        (groundtruth_boxlists, groundtruth_classes_with_background_list, _,\n",
    "         _) = self._format_groundtruth_data(true_image_shapes)\n",
    "        (proposal_boxes, proposal_scores,\n",
    "         num_proposals) = self._unpad_proposals_and_sample_box_classifier_batch(\n",
    "             proposal_boxes, proposal_scores, num_proposals,\n",
    "             groundtruth_boxlists, groundtruth_classes_with_background_list)\n",
    "    # normalize proposal boxes\n",
    "    def normalize_boxes(args):\n",
    "      proposal_boxes_per_image = args[0]\n",
    "      image_shape = args[1]\n",
    "      normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(\n",
    "          box_list.BoxList(proposal_boxes_per_image), image_shape[0],\n",
    "          image_shape[1], check_range=False).get()\n",
    "      return normalized_boxes_per_image\n",
    "    normalized_proposal_boxes = shape_utils.static_or_dynamic_map_fn(\n",
    "        normalize_boxes, elems=[proposal_boxes, image_shapes], dtype=tf.float32)\n",
    "    return normalized_proposal_boxes, proposal_scores, num_proposals\n",
    "\n",
    "  def _unpad_proposals_and_sample_box_classifier_batch(\n",
    "      self,\n",
    "      proposal_boxes,\n",
    "      proposal_scores,\n",
    "      num_proposals,\n",
    "      groundtruth_boxlists,\n",
    "      groundtruth_classes_with_background_list):\n",
    "    \"\"\"Unpads proposals and samples a minibatch for second stage.\n",
    "\n",
    "    Args:\n",
    "      proposal_boxes: A float tensor with shape\n",
    "        [batch_size, num_proposals, 4] representing the (potentially zero\n",
    "        padded) proposal boxes for all images in the batch.  These boxes are\n",
    "        represented in absolute coordinates.\n",
    "      proposal_scores:  A float tensor with shape\n",
    "        [batch_size, num_proposals] representing the (potentially zero\n",
    "        padded) proposal objectness scores for all images in the batch.\n",
    "      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n",
    "        representing the number of proposals predicted for each image in\n",
    "        the batch.\n",
    "      groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates\n",
    "        of the groundtruth boxes.\n",
    "      groundtruth_classes_with_background_list: A list of 2-D one-hot\n",
    "        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n",
    "        class targets with the 0th index assumed to map to the background class.\n",
    "\n",
    "    Returns:\n",
    "      proposal_boxes: A float tensor with shape\n",
    "        [batch_size, second_stage_batch_size, 4] representing the (potentially\n",
    "        zero padded) proposal boxes for all images in the batch.  These boxes\n",
    "        are represented in absolute coordinates.\n",
    "      proposal_scores:  A float tensor with shape\n",
    "        [batch_size, second_stage_batch_size] representing the (potentially zero\n",
    "        padded) proposal objectness scores for all images in the batch.\n",
    "      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n",
    "        representing the number of proposals predicted for each image in\n",
    "        the batch.\n",
    "    \"\"\"\n",
    "    single_image_proposal_box_sample = []\n",
    "    single_image_proposal_score_sample = []\n",
    "    single_image_num_proposals_sample = []\n",
    "    for (single_image_proposal_boxes,\n",
    "         single_image_proposal_scores,\n",
    "         single_image_num_proposals,\n",
    "         single_image_groundtruth_boxlist,\n",
    "         single_image_groundtruth_classes_with_background) in zip(\n",
    "             tf.unstack(proposal_boxes),\n",
    "             tf.unstack(proposal_scores),\n",
    "             tf.unstack(num_proposals),\n",
    "             groundtruth_boxlists,\n",
    "             groundtruth_classes_with_background_list):\n",
    "      static_shape = single_image_proposal_boxes.get_shape()\n",
    "      sliced_static_shape = tf.TensorShape([tf.Dimension(None),\n",
    "                                            static_shape.dims[-1]])\n",
    "      single_image_proposal_boxes = tf.slice(\n",
    "          single_image_proposal_boxes,\n",
    "          [0, 0],\n",
    "          [single_image_num_proposals, -1])\n",
    "      single_image_proposal_boxes.set_shape(sliced_static_shape)\n",
    "\n",
    "      single_image_proposal_scores = tf.slice(single_image_proposal_scores,\n",
    "                                              [0],\n",
    "                                              [single_image_num_proposals])\n",
    "      single_image_boxlist = box_list.BoxList(single_image_proposal_boxes)\n",
    "      single_image_boxlist.add_field(fields.BoxListFields.scores,\n",
    "                                     single_image_proposal_scores)\n",
    "      sampled_boxlist = self._sample_box_classifier_minibatch(\n",
    "          single_image_boxlist,\n",
    "          single_image_groundtruth_boxlist,\n",
    "          single_image_groundtruth_classes_with_background)\n",
    "      sampled_padded_boxlist = box_list_ops.pad_or_clip_box_list(\n",
    "          sampled_boxlist,\n",
    "          num_boxes=self._second_stage_batch_size)\n",
    "      single_image_num_proposals_sample.append(tf.minimum(\n",
    "          sampled_boxlist.num_boxes(),\n",
    "          self._second_stage_batch_size))\n",
    "      bb = sampled_padded_boxlist.get()\n",
    "      single_image_proposal_box_sample.append(bb)\n",
    "      single_image_proposal_score_sample.append(\n",
    "          sampled_padded_boxlist.get_field(fields.BoxListFields.scores))\n",
    "    return (tf.stack(single_image_proposal_box_sample),\n",
    "            tf.stack(single_image_proposal_score_sample),\n",
    "            tf.stack(single_image_num_proposals_sample))\n",
    "\n",
    "  def _format_groundtruth_data(self, true_image_shapes):\n",
    "    \"\"\"Helper function for preparing groundtruth data for target assignment.\n",
    "\n",
    "    In order to be consistent with the model.DetectionModel interface,\n",
    "    groundtruth boxes are specified in normalized coordinates and classes are\n",
    "    specified as label indices with no assumed background category.  To prepare\n",
    "    for target assignment, we:\n",
    "    1) convert boxes to absolute coordinates,\n",
    "    2) add a background class at class index 0\n",
    "    3) groundtruth instance masks, if available, are resized to match\n",
    "       image_shape.\n",
    "\n",
    "    Args:\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "\n",
    "    Returns:\n",
    "      groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates\n",
    "        of the groundtruth boxes.\n",
    "      groundtruth_classes_with_background_list: A list of 2-D one-hot\n",
    "        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n",
    "        class targets with the 0th index assumed to map to the background class.\n",
    "      groundtruth_masks_list: If present, a list of 3-D tf.float32 tensors of\n",
    "        shape [num_boxes, image_height, image_width] containing instance masks.\n",
    "        This is set to None if no masks exist in the provided groundtruth.\n",
    "    \"\"\"\n",
    "    groundtruth_boxlists = [\n",
    "        box_list_ops.to_absolute_coordinates(\n",
    "            box_list.BoxList(boxes), true_image_shapes[i, 0],\n",
    "            true_image_shapes[i, 1])\n",
    "        for i, boxes in enumerate(\n",
    "            self.groundtruth_lists(fields.BoxListFields.boxes))\n",
    "    ]\n",
    "    groundtruth_classes_with_background_list = [\n",
    "        tf.to_float(\n",
    "            tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'))\n",
    "        for one_hot_encoding in self.groundtruth_lists(\n",
    "            fields.BoxListFields.classes)]\n",
    "\n",
    "    groundtruth_masks_list = self._groundtruth_lists.get(\n",
    "        fields.BoxListFields.masks)\n",
    "    if groundtruth_masks_list is not None:\n",
    "      resized_masks_list = []\n",
    "      for mask in groundtruth_masks_list:\n",
    "        _, resized_mask, _ = self._image_resizer_fn(\n",
    "            # Reuse the given `image_resizer_fn` to resize groundtruth masks.\n",
    "            # `mask` tensor for an image is of the shape [num_masks,\n",
    "            # image_height, image_width]. Below we create a dummy image of the\n",
    "            # the shape [image_height, image_width, 1] to use with\n",
    "            # `image_resizer_fn`.\n",
    "            image=tf.zeros(tf.stack([tf.shape(mask)[1], tf.shape(mask)[2], 1])),\n",
    "            masks=mask)\n",
    "        resized_masks_list.append(resized_mask)\n",
    "\n",
    "      groundtruth_masks_list = resized_masks_list\n",
    "    groundtruth_weights_list = None\n",
    "    if self.groundtruth_has_field(fields.BoxListFields.weights):\n",
    "      groundtruth_weights_list = self.groundtruth_lists(\n",
    "          fields.BoxListFields.weights)\n",
    "\n",
    "    return (groundtruth_boxlists, groundtruth_classes_with_background_list,\n",
    "            groundtruth_masks_list, groundtruth_weights_list)\n",
    "\n",
    "  def _sample_box_classifier_minibatch(self,\n",
    "                                       proposal_boxlist,\n",
    "                                       groundtruth_boxlist,\n",
    "                                       groundtruth_classes_with_background):\n",
    "    \"\"\"Samples a mini-batch of proposals to be sent to the box classifier.\n",
    "\n",
    "    Helper function for self._postprocess_rpn.\n",
    "\n",
    "    Args:\n",
    "      proposal_boxlist: A BoxList containing K proposal boxes in absolute\n",
    "        coordinates.\n",
    "      groundtruth_boxlist: A Boxlist containing N groundtruth object boxes in\n",
    "        absolute coordinates.\n",
    "      groundtruth_classes_with_background: A tensor with shape\n",
    "        `[N, self.num_classes + 1]` representing groundtruth classes. The\n",
    "        classes are assumed to be k-hot encoded, and include background as the\n",
    "        zero-th class.\n",
    "\n",
    "    Returns:\n",
    "      a BoxList contained sampled proposals.\n",
    "    \"\"\"\n",
    "    (cls_targets, cls_weights, _, _, _) = self._detector_target_assigner.assign(\n",
    "        proposal_boxlist, groundtruth_boxlist,\n",
    "        groundtruth_classes_with_background)\n",
    "    # Selects all boxes as candidates if none of them is selected according\n",
    "    # to cls_weights. This could happen as boxes within certain IOU ranges\n",
    "    # are ignored. If triggered, the selected boxes will still be ignored\n",
    "    # during loss computation.\n",
    "    cls_weights += tf.to_float(tf.equal(tf.reduce_sum(cls_weights), 0))\n",
    "    positive_indicator = tf.greater(tf.argmax(cls_targets, axis=1), 0)\n",
    "    sampled_indices = self._second_stage_sampler.subsample(\n",
    "        tf.cast(cls_weights, tf.bool),\n",
    "        self._second_stage_batch_size,\n",
    "        positive_indicator)\n",
    "    return box_list_ops.boolean_mask(proposal_boxlist, sampled_indices)\n",
    "\n",
    "  def _compute_second_stage_input_feature_maps(self, features_to_crop,\n",
    "                                               proposal_boxes_normalized):\n",
    "    \"\"\"Crops to a set of proposals from the feature map for a batch of images.\n",
    "\n",
    "    Helper function for self._postprocess_rpn. This function calls\n",
    "    `tf.image.crop_and_resize` to create the feature map to be passed to the\n",
    "    second stage box classifier for each proposal.\n",
    "\n",
    "    Args:\n",
    "      features_to_crop: A float32 tensor with shape\n",
    "        [batch_size, height, width, depth]\n",
    "      proposal_boxes_normalized: A float32 tensor with shape [batch_size,\n",
    "        num_proposals, box_code_size] containing proposal boxes in\n",
    "        normalized coordinates.\n",
    "\n",
    "    Returns:\n",
    "      A float32 tensor with shape [K, new_height, new_width, depth].\n",
    "    \"\"\"\n",
    "    def get_box_inds(proposals):\n",
    "      proposals_shape = proposals.get_shape().as_list()\n",
    "      if any(dim is None for dim in proposals_shape):\n",
    "        proposals_shape = tf.shape(proposals)\n",
    "      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n",
    "      multiplier = tf.expand_dims(\n",
    "          tf.range(start=0, limit=proposals_shape[0]), 1)\n",
    "      return tf.reshape(ones_mat * multiplier, [-1])\n",
    "\n",
    "    if self._use_matmul_crop_and_resize:\n",
    "      def _single_image_crop_and_resize(inputs):\n",
    "        single_image_features_to_crop, proposal_boxes_normalized = inputs\n",
    "        return ops.matmul_crop_and_resize(\n",
    "            tf.expand_dims(single_image_features_to_crop, 0),\n",
    "            proposal_boxes_normalized,\n",
    "            [self._initial_crop_size, self._initial_crop_size])\n",
    "\n",
    "      cropped_regions = self._flatten_first_two_dimensions(\n",
    "          shape_utils.static_or_dynamic_map_fn(\n",
    "              _single_image_crop_and_resize,\n",
    "              elems=[features_to_crop, proposal_boxes_normalized],\n",
    "              dtype=tf.float32,\n",
    "              parallel_iterations=self._parallel_iterations))\n",
    "    else:\n",
    "      cropped_regions = tf.image.crop_and_resize(\n",
    "          features_to_crop,\n",
    "          self._flatten_first_two_dimensions(proposal_boxes_normalized),\n",
    "          get_box_inds(proposal_boxes_normalized),\n",
    "          (self._initial_crop_size, self._initial_crop_size))\n",
    "    return slim.max_pool2d(\n",
    "        cropped_regions,\n",
    "        [self._maxpool_kernel_size, self._maxpool_kernel_size],\n",
    "        stride=self._maxpool_stride)\n",
    "\n",
    "  def _postprocess_box_classifier(self,\n",
    "                                  refined_box_encodings,\n",
    "                                  class_predictions_with_background,\n",
    "                                  proposal_boxes,\n",
    "                                  num_proposals,\n",
    "                                  image_shapes,\n",
    "                                  mask_predictions=None):\n",
    "    \"\"\"Converts predictions from the second stage box classifier to detections.\n",
    "\n",
    "    Args:\n",
    "      refined_box_encodings: a 3-D float tensor with shape\n",
    "        [total_num_padded_proposals, num_classes, self._box_coder.code_size]\n",
    "        representing predicted (final) refined box encodings. If using a shared\n",
    "        box across classes the shape will instead be\n",
    "        [total_num_padded_proposals, 1, 4]\n",
    "      class_predictions_with_background: a 3-D tensor float with shape\n",
    "        [total_num_padded_proposals, num_classes + 1] containing class\n",
    "        predictions (logits) for each of the proposals.  Note that this tensor\n",
    "        *includes* background class predictions (at class index 0).\n",
    "      proposal_boxes: a 3-D float tensor with shape\n",
    "        [batch_size, self.max_num_proposals, 4] representing decoded proposal\n",
    "        bounding boxes in absolute coordinates.\n",
    "      num_proposals: a 1-D int32 tensor of shape [batch] representing the number\n",
    "        of proposals predicted for each image in the batch.\n",
    "      image_shapes: a 2-D int32 tensor containing shapes of input image in the\n",
    "        batch.\n",
    "      mask_predictions: (optional) a 4-D float tensor with shape\n",
    "        [total_num_padded_proposals, num_classes, mask_height, mask_width]\n",
    "        containing instance mask prediction logits.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing:\n",
    "        `detection_boxes`: [batch, max_detection, 4]\n",
    "        `detection_scores`: [batch, max_detections]\n",
    "        `detection_classes`: [batch, max_detections]\n",
    "        `num_detections`: [batch]\n",
    "        `detection_masks`:\n",
    "          (optional) [batch, max_detections, mask_height, mask_width]. Note\n",
    "          that a pixel-wise sigmoid score converter is applied to the detection\n",
    "          masks.\n",
    "    \"\"\"\n",
    "    refined_box_encodings_batch = tf.reshape(\n",
    "        refined_box_encodings,\n",
    "        [-1,\n",
    "         self.max_num_proposals,\n",
    "         refined_box_encodings.shape[1],\n",
    "         self._box_coder.code_size])\n",
    "    class_predictions_with_background_batch = tf.reshape(\n",
    "        class_predictions_with_background,\n",
    "        [-1, self.max_num_proposals, self.num_classes + 1]\n",
    "    )\n",
    "    refined_decoded_boxes_batch = self._batch_decode_boxes(\n",
    "        refined_box_encodings_batch, proposal_boxes)\n",
    "    class_predictions_with_background_batch = (\n",
    "        self._second_stage_score_conversion_fn(\n",
    "            class_predictions_with_background_batch))\n",
    "    class_predictions_batch = tf.reshape(\n",
    "        tf.slice(class_predictions_with_background_batch,\n",
    "                 [0, 0, 1], [-1, -1, -1]),\n",
    "        [-1, self.max_num_proposals, self.num_classes])\n",
    "    clip_window = self._compute_clip_window(image_shapes)\n",
    "    mask_predictions_batch = None\n",
    "    if mask_predictions is not None:\n",
    "      mask_height = mask_predictions.shape[2].value\n",
    "      mask_width = mask_predictions.shape[3].value\n",
    "      mask_predictions = tf.sigmoid(mask_predictions)\n",
    "      mask_predictions_batch = tf.reshape(\n",
    "          mask_predictions, [-1, self.max_num_proposals,\n",
    "                             self.num_classes, mask_height, mask_width])\n",
    "    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks, _,\n",
    "     num_detections) = self._second_stage_nms_fn(\n",
    "         refined_decoded_boxes_batch,\n",
    "         class_predictions_batch,\n",
    "         clip_window=clip_window,\n",
    "         change_coordinate_frame=True,\n",
    "         num_valid_boxes=num_proposals,\n",
    "         masks=mask_predictions_batch)\n",
    "    detections = {\n",
    "        fields.DetectionResultFields.detection_boxes: nmsed_boxes,\n",
    "        fields.DetectionResultFields.detection_scores: nmsed_scores,\n",
    "        fields.DetectionResultFields.detection_classes: nmsed_classes,\n",
    "        fields.DetectionResultFields.num_detections: tf.to_float(num_detections)\n",
    "    }\n",
    "    if nmsed_masks is not None:\n",
    "      detections[fields.DetectionResultFields.detection_masks] = nmsed_masks\n",
    "    return detections\n",
    "\n",
    "  def _batch_decode_boxes(self, box_encodings, anchor_boxes):\n",
    "    \"\"\"Decodes box encodings with respect to the anchor boxes.\n",
    "\n",
    "    Args:\n",
    "      box_encodings: a 4-D tensor with shape\n",
    "        [batch_size, num_anchors, num_classes, self._box_coder.code_size]\n",
    "        representing box encodings.\n",
    "      anchor_boxes: [batch_size, num_anchors, self._box_coder.code_size]\n",
    "        representing decoded bounding boxes. If using a shared box across\n",
    "        classes the shape will instead be\n",
    "        [total_num_proposals, 1, self._box_coder.code_size].\n",
    "\n",
    "    Returns:\n",
    "      decoded_boxes: a\n",
    "        [batch_size, num_anchors, num_classes, self._box_coder.code_size]\n",
    "        float tensor representing bounding box predictions (for each image in\n",
    "        batch, proposal and class). If using a shared box across classes the\n",
    "        shape will instead be\n",
    "        [batch_size, num_anchors, 1, self._box_coder.code_size].\n",
    "    \"\"\"\n",
    "    combined_shape = shape_utils.combined_static_and_dynamic_shape(\n",
    "        box_encodings)\n",
    "    num_classes = combined_shape[2]\n",
    "    tiled_anchor_boxes = tf.tile(\n",
    "        tf.expand_dims(anchor_boxes, 2), [1, 1, num_classes, 1])\n",
    "    tiled_anchors_boxlist = box_list.BoxList(\n",
    "        tf.reshape(tiled_anchor_boxes, [-1, 4]))\n",
    "    decoded_boxes = self._box_coder.decode(\n",
    "        tf.reshape(box_encodings, [-1, self._box_coder.code_size]),\n",
    "        tiled_anchors_boxlist)\n",
    "    return tf.reshape(decoded_boxes.get(),\n",
    "                      tf.stack([combined_shape[0], combined_shape[1],\n",
    "                                num_classes, 4]))\n",
    "\n",
    "  def loss(self, prediction_dict, true_image_shapes, scope=None):\n",
    "    \"\"\"Compute scalar loss tensors given prediction tensors.\n",
    "\n",
    "    If number_of_stages=1, only RPN related losses are computed (i.e.,\n",
    "    `rpn_localization_loss` and `rpn_objectness_loss`).  Otherwise all\n",
    "    losses are computed.\n",
    "\n",
    "    Args:\n",
    "      prediction_dict: a dictionary holding prediction tensors (see the\n",
    "        documentation for the predict method.  If number_of_stages=1, we\n",
    "        expect prediction_dict to contain `rpn_box_encodings`,\n",
    "        `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,\n",
    "        `image_shape`, and `anchors` fields.  Otherwise we expect\n",
    "        prediction_dict to additionally contain `refined_box_encodings`,\n",
    "        `class_predictions_with_background`, `num_proposals`, and\n",
    "        `proposal_boxes` fields.\n",
    "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "        of the form [height, width, channels] indicating the shapes\n",
    "        of true images in the resized images, as resized images can be padded\n",
    "        with zeros.\n",
    "      scope: Optional scope name.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary mapping loss keys (`first_stage_localization_loss`,\n",
    "        `first_stage_objectness_loss`, 'second_stage_localization_loss',\n",
    "        'second_stage_classification_loss') to scalar tensors representing\n",
    "        corresponding loss values.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'Loss', prediction_dict.values()):\n",
    "      (groundtruth_boxlists, groundtruth_classes_with_background_list,\n",
    "       groundtruth_masks_list, groundtruth_weights_list\n",
    "      ) = self._format_groundtruth_data(true_image_shapes)\n",
    "      loss_dict = self._loss_rpn(\n",
    "          prediction_dict['rpn_box_encodings'],\n",
    "          prediction_dict['rpn_objectness_predictions_with_background'],\n",
    "          prediction_dict['anchors'], groundtruth_boxlists,\n",
    "          groundtruth_classes_with_background_list, groundtruth_weights_list)\n",
    "      if self._number_of_stages > 1:\n",
    "        loss_dict.update(\n",
    "            self._loss_box_classifier(\n",
    "                prediction_dict['refined_box_encodings'],\n",
    "                prediction_dict['class_predictions_with_background'],\n",
    "                prediction_dict['proposal_boxes'],\n",
    "                prediction_dict['num_proposals'],\n",
    "                groundtruth_boxlists,\n",
    "                groundtruth_classes_with_background_list,\n",
    "                groundtruth_weights_list,\n",
    "                prediction_dict['image_shape'],\n",
    "                prediction_dict.get('mask_predictions'),\n",
    "                groundtruth_masks_list,\n",
    "            ))\n",
    "    return loss_dict\n",
    "\n",
    "  def _loss_rpn(self, rpn_box_encodings,\n",
    "                rpn_objectness_predictions_with_background, anchors,\n",
    "                groundtruth_boxlists, groundtruth_classes_with_background_list,\n",
    "                groundtruth_weights_list):\n",
    "    \"\"\"Computes scalar RPN loss tensors.\n",
    "\n",
    "    Uses self._proposal_target_assigner to obtain regression and classification\n",
    "    targets for the first stage RPN, samples a \"minibatch\" of anchors to\n",
    "    participate in the loss computation, and returns the RPN losses.\n",
    "\n",
    "    Args:\n",
    "      rpn_box_encodings: A 4-D float tensor of shape\n",
    "        [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "        predicted proposal box encodings.\n",
    "      rpn_objectness_predictions_with_background: A 2-D float tensor of shape\n",
    "        [batch_size, num_anchors, 2] containing objectness predictions\n",
    "        (logits) for each of the anchors with 0 corresponding to background\n",
    "        and 1 corresponding to object.\n",
    "      anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n",
    "        for the first stage RPN.  Note that `num_anchors` can differ depending\n",
    "        on whether the model is created in training or inference mode.\n",
    "      groundtruth_boxlists: A list of BoxLists containing coordinates of the\n",
    "        groundtruth boxes.\n",
    "      groundtruth_classes_with_background_list: A list of 2-D one-hot\n",
    "        (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the\n",
    "        class targets with the 0th index assumed to map to the background class.\n",
    "      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape\n",
    "        [num_boxes] containing weights for groundtruth boxes.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary mapping loss keys (`first_stage_localization_loss`,\n",
    "        `first_stage_objectness_loss`) to scalar tensors representing\n",
    "        corresponding loss values.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('RPNLoss'):\n",
    "      (batch_cls_targets, batch_cls_weights, batch_reg_targets,\n",
    "       batch_reg_weights, _) = target_assigner.batch_assign_targets(\n",
    "           self._proposal_target_assigner, box_list.BoxList(anchors),\n",
    "           groundtruth_boxlists,\n",
    "           len(groundtruth_boxlists) * [None], groundtruth_weights_list)\n",
    "      batch_cls_targets = tf.squeeze(batch_cls_targets, axis=2)\n",
    "\n",
    "      def _minibatch_subsample_fn(inputs):\n",
    "        cls_targets, cls_weights = inputs\n",
    "        return self._first_stage_sampler.subsample(\n",
    "            tf.cast(cls_weights, tf.bool),\n",
    "            self._first_stage_minibatch_size, tf.cast(cls_targets, tf.bool))\n",
    "      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(\n",
    "          _minibatch_subsample_fn,\n",
    "          [batch_cls_targets, batch_cls_weights],\n",
    "          dtype=tf.bool,\n",
    "          parallel_iterations=self._parallel_iterations,\n",
    "          back_prop=True))\n",
    "\n",
    "      # Normalize by number of examples in sampled minibatch\n",
    "      normalizer = tf.reduce_sum(batch_sampled_indices, axis=1)\n",
    "      batch_one_hot_targets = tf.one_hot(\n",
    "          tf.to_int32(batch_cls_targets), depth=2)\n",
    "      sampled_reg_indices = tf.multiply(batch_sampled_indices,\n",
    "                                        batch_reg_weights)\n",
    "\n",
    "      localization_losses = self._first_stage_localization_loss(\n",
    "          rpn_box_encodings, batch_reg_targets, weights=sampled_reg_indices)\n",
    "      objectness_losses = self._first_stage_objectness_loss(\n",
    "          rpn_objectness_predictions_with_background,\n",
    "          batch_one_hot_targets, weights=batch_sampled_indices)\n",
    "      localization_loss = tf.reduce_mean(\n",
    "          tf.reduce_sum(localization_losses, axis=1) / normalizer)\n",
    "      objectness_loss = tf.reduce_mean(\n",
    "          tf.reduce_sum(objectness_losses, axis=1) / normalizer)\n",
    "\n",
    "      localization_loss = tf.multiply(self._first_stage_loc_loss_weight,\n",
    "                                      localization_loss,\n",
    "                                      name='localization_loss')\n",
    "      objectness_loss = tf.multiply(self._first_stage_obj_loss_weight,\n",
    "                                    objectness_loss, name='objectness_loss')\n",
    "      loss_dict = {localization_loss.op.name: localization_loss,\n",
    "                   objectness_loss.op.name: objectness_loss}\n",
    "    return loss_dict\n",
    "\n",
    "  def _loss_box_classifier(self,\n",
    "                           refined_box_encodings,\n",
    "                           class_predictions_with_background,\n",
    "                           proposal_boxes,\n",
    "                           num_proposals,\n",
    "                           groundtruth_boxlists,\n",
    "                           groundtruth_classes_with_background_list,\n",
    "                           groundtruth_weights_list,\n",
    "                           image_shape,\n",
    "                           prediction_masks=None,\n",
    "                           groundtruth_masks_list=None):\n",
    "    \"\"\"Computes scalar box classifier loss tensors.\n",
    "\n",
    "    Uses self._detector_target_assigner to obtain regression and classification\n",
    "    targets for the second stage box classifier, optionally performs\n",
    "    hard mining, and returns losses.  All losses are computed independently\n",
    "    for each image and then averaged across the batch.\n",
    "    Please note that for boxes and masks with multiple labels, the box\n",
    "    regression and mask prediction losses are only computed for one label.\n",
    "\n",
    "    This function assumes that the proposal boxes in the \"padded\" regions are\n",
    "    actually zero (and thus should not be matched to).\n",
    "\n",
    "\n",
    "    Args:\n",
    "      refined_box_encodings: a 3-D tensor with shape\n",
    "        [total_num_proposals, num_classes, box_coder.code_size] representing\n",
    "        predicted (final) refined box encodings. If using a shared box across\n",
    "        classes this will instead have shape\n",
    "        [total_num_proposals, 1, box_coder.code_size].\n",
    "      class_predictions_with_background: a 2-D tensor with shape\n",
    "        [total_num_proposals, num_classes + 1] containing class\n",
    "        predictions (logits) for each of the anchors.  Note that this tensor\n",
    "        *includes* background class predictions (at class index 0).\n",
    "      proposal_boxes: [batch_size, self.max_num_proposals, 4] representing\n",
    "        decoded proposal bounding boxes.\n",
    "      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n",
    "        representing the number of proposals predicted for each image in\n",
    "        the batch.\n",
    "      groundtruth_boxlists: a list of BoxLists containing coordinates of the\n",
    "        groundtruth boxes.\n",
    "      groundtruth_classes_with_background_list: a list of 2-D one-hot\n",
    "        (or k-hot) tensors of shape [num_boxes, num_classes + 1] containing the\n",
    "        class targets with the 0th index assumed to map to the background class.\n",
    "      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape\n",
    "        [num_boxes] containing weights for groundtruth boxes.\n",
    "      image_shape: a 1-D tensor of shape [4] representing the image shape.\n",
    "      prediction_masks: an optional 4-D tensor with shape [total_num_proposals,\n",
    "        num_classes, mask_height, mask_width] containing the instance masks for\n",
    "        each box.\n",
    "      groundtruth_masks_list: an optional list of 3-D tensors of shape\n",
    "        [num_boxes, image_height, image_width] containing the instance masks for\n",
    "        each of the boxes.\n",
    "\n",
    "    Returns:\n",
    "      a dictionary mapping loss keys ('second_stage_localization_loss',\n",
    "        'second_stage_classification_loss') to scalar tensors representing\n",
    "        corresponding loss values.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if `predict_instance_masks` in\n",
    "        second_stage_mask_rcnn_box_predictor is True and\n",
    "        `groundtruth_masks_list` is not provided.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('BoxClassifierLoss'):\n",
    "      paddings_indicator = self._padded_batched_proposals_indicator(\n",
    "          num_proposals, self.max_num_proposals)\n",
    "      proposal_boxlists = [\n",
    "          box_list.BoxList(proposal_boxes_single_image)\n",
    "          for proposal_boxes_single_image in tf.unstack(proposal_boxes)]\n",
    "      batch_size = len(proposal_boxlists)\n",
    "\n",
    "      num_proposals_or_one = tf.to_float(tf.expand_dims(\n",
    "          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1))\n",
    "      normalizer = tf.tile(num_proposals_or_one,\n",
    "                           [1, self.max_num_proposals]) * batch_size\n",
    "\n",
    "      (batch_cls_targets_with_background, batch_cls_weights, batch_reg_targets,\n",
    "       batch_reg_weights, _) = target_assigner.batch_assign_targets(\n",
    "           self._detector_target_assigner, proposal_boxlists,\n",
    "           groundtruth_boxlists, groundtruth_classes_with_background_list,\n",
    "           groundtruth_weights_list)\n",
    "\n",
    "      class_predictions_with_background = tf.reshape(\n",
    "          class_predictions_with_background,\n",
    "          [batch_size, self.max_num_proposals, -1])\n",
    "\n",
    "      flat_cls_targets_with_background = tf.reshape(\n",
    "          batch_cls_targets_with_background,\n",
    "          [batch_size * self.max_num_proposals, -1])\n",
    "      one_hot_flat_cls_targets_with_background = tf.argmax(\n",
    "          flat_cls_targets_with_background, axis=1)\n",
    "      one_hot_flat_cls_targets_with_background = tf.one_hot(\n",
    "          one_hot_flat_cls_targets_with_background,\n",
    "          flat_cls_targets_with_background.get_shape()[1])\n",
    "\n",
    "      # If using a shared box across classes use directly\n",
    "      if refined_box_encodings.shape[1] == 1:\n",
    "        reshaped_refined_box_encodings = tf.reshape(\n",
    "            refined_box_encodings,\n",
    "            [batch_size, self.max_num_proposals, self._box_coder.code_size])\n",
    "      # For anchors with multiple labels, picks refined_location_encodings\n",
    "      # for just one class to avoid over-counting for regression loss and\n",
    "      # (optionally) mask loss.\n",
    "      else:\n",
    "        # We only predict refined location encodings for the non background\n",
    "        # classes, but we now pad it to make it compatible with the class\n",
    "        # predictions\n",
    "        refined_box_encodings_with_background = tf.pad(\n",
    "            refined_box_encodings, [[0, 0], [1, 0], [0, 0]])\n",
    "        refined_box_encodings_masked_by_class_targets = tf.boolean_mask(\n",
    "            refined_box_encodings_with_background,\n",
    "            tf.greater(one_hot_flat_cls_targets_with_background, 0))\n",
    "        reshaped_refined_box_encodings = tf.reshape(\n",
    "            refined_box_encodings_masked_by_class_targets,\n",
    "            [batch_size, self.max_num_proposals, self._box_coder.code_size])\n",
    "\n",
    "      second_stage_loc_losses = self._second_stage_localization_loss(\n",
    "          reshaped_refined_box_encodings,\n",
    "          batch_reg_targets, weights=batch_reg_weights) / normalizer\n",
    "      second_stage_cls_losses = ops.reduce_sum_trailing_dimensions(\n",
    "          self._second_stage_classification_loss(\n",
    "              class_predictions_with_background,\n",
    "              batch_cls_targets_with_background,\n",
    "              weights=batch_cls_weights),\n",
    "          ndims=2) / normalizer\n",
    "\n",
    "      second_stage_loc_loss = tf.reduce_sum(\n",
    "          tf.boolean_mask(second_stage_loc_losses, paddings_indicator))\n",
    "      second_stage_cls_loss = tf.reduce_sum(\n",
    "          tf.boolean_mask(second_stage_cls_losses, paddings_indicator))\n",
    "\n",
    "      if self._hard_example_miner:\n",
    "        (second_stage_loc_loss, second_stage_cls_loss\n",
    "        ) = self._unpad_proposals_and_apply_hard_mining(\n",
    "            proposal_boxlists, second_stage_loc_losses,\n",
    "            second_stage_cls_losses, num_proposals)\n",
    "      localization_loss = tf.multiply(self._second_stage_loc_loss_weight,\n",
    "                                      second_stage_loc_loss,\n",
    "                                      name='localization_loss')\n",
    "\n",
    "      classification_loss = tf.multiply(self._second_stage_cls_loss_weight,\n",
    "                                        second_stage_cls_loss,\n",
    "                                        name='classification_loss')\n",
    "\n",
    "      loss_dict = {localization_loss.op.name: localization_loss,\n",
    "                   classification_loss.op.name: classification_loss}\n",
    "      second_stage_mask_loss = None\n",
    "      if prediction_masks is not None:\n",
    "        if groundtruth_masks_list is None:\n",
    "          raise ValueError('Groundtruth instance masks not provided. '\n",
    "                           'Please configure input reader.')\n",
    "\n",
    "        # Create a new target assigner that matches the proposals to groundtruth\n",
    "        # and returns the mask targets.\n",
    "        # TODO(rathodv): Move `unmatched_cls_target` from constructor to assign\n",
    "        # function. This will enable reuse of a single target assigner for both\n",
    "        # class targets and mask targets.\n",
    "        mask_target_assigner = target_assigner.create_target_assigner(\n",
    "            'FasterRCNN', 'detection',\n",
    "            unmatched_cls_target=tf.zeros(image_shape[1:3], dtype=tf.float32))\n",
    "        (batch_mask_targets, _, _,\n",
    "         batch_mask_target_weights, _) = target_assigner.batch_assign_targets(\n",
    "             mask_target_assigner, proposal_boxlists, groundtruth_boxlists,\n",
    "             groundtruth_masks_list, groundtruth_weights_list)\n",
    "\n",
    "        # Pad the prediction_masks with to add zeros for background class to be\n",
    "        # consistent with class predictions.\n",
    "        if prediction_masks.get_shape().as_list()[1] == 1:\n",
    "          # Class agnostic masks or masks for one-class prediction. Logic for\n",
    "          # both cases is the same since background predictions are ignored\n",
    "          # through the batch_mask_target_weights.\n",
    "          prediction_masks_masked_by_class_targets = prediction_masks\n",
    "        else:\n",
    "          prediction_masks_with_background = tf.pad(\n",
    "              prediction_masks, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "          prediction_masks_masked_by_class_targets = tf.boolean_mask(\n",
    "              prediction_masks_with_background,\n",
    "              tf.greater(one_hot_flat_cls_targets_with_background, 0))\n",
    "\n",
    "        mask_height = prediction_masks.shape[2].value\n",
    "        mask_width = prediction_masks.shape[3].value\n",
    "        reshaped_prediction_masks = tf.reshape(\n",
    "            prediction_masks_masked_by_class_targets,\n",
    "            [batch_size, -1, mask_height * mask_width])\n",
    "\n",
    "        batch_mask_targets_shape = tf.shape(batch_mask_targets)\n",
    "        flat_gt_masks = tf.reshape(batch_mask_targets,\n",
    "                                   [-1, batch_mask_targets_shape[2],\n",
    "                                    batch_mask_targets_shape[3]])\n",
    "\n",
    "        # Use normalized proposals to crop mask targets from image masks.\n",
    "        flat_normalized_proposals = box_list_ops.to_normalized_coordinates(\n",
    "            box_list.BoxList(tf.reshape(proposal_boxes, [-1, 4])),\n",
    "            image_shape[1], image_shape[2]).get()\n",
    "\n",
    "        flat_cropped_gt_mask = tf.image.crop_and_resize(\n",
    "            tf.expand_dims(flat_gt_masks, -1),\n",
    "            flat_normalized_proposals,\n",
    "            tf.range(flat_normalized_proposals.shape[0].value),\n",
    "            [mask_height, mask_width])\n",
    "\n",
    "        batch_cropped_gt_mask = tf.reshape(\n",
    "            flat_cropped_gt_mask,\n",
    "            [batch_size, -1, mask_height * mask_width])\n",
    "\n",
    "        second_stage_mask_losses = ops.reduce_sum_trailing_dimensions(\n",
    "            self._second_stage_mask_loss(\n",
    "                reshaped_prediction_masks,\n",
    "                batch_cropped_gt_mask,\n",
    "                weights=batch_mask_target_weights),\n",
    "            ndims=2) / (\n",
    "                mask_height * mask_width * tf.maximum(\n",
    "                    tf.reduce_sum(\n",
    "                        batch_mask_target_weights, axis=1, keep_dims=True\n",
    "                    ), tf.ones((batch_size, 1))))\n",
    "        second_stage_mask_loss = tf.reduce_sum(\n",
    "            tf.boolean_mask(second_stage_mask_losses, paddings_indicator))\n",
    "\n",
    "      if second_stage_mask_loss is not None:\n",
    "        mask_loss = tf.multiply(self._second_stage_mask_loss_weight,\n",
    "                                second_stage_mask_loss, name='mask_loss')\n",
    "        loss_dict[mask_loss.op.name] = mask_loss\n",
    "    return loss_dict\n",
    "\n",
    "  def _padded_batched_proposals_indicator(self,\n",
    "                                          num_proposals,\n",
    "                                          max_num_proposals):\n",
    "    \"\"\"Creates indicator matrix of non-pad elements of padded batch proposals.\n",
    "\n",
    "    Args:\n",
    "      num_proposals: Tensor of type tf.int32 with shape [batch_size].\n",
    "      max_num_proposals: Maximum number of proposals per image (integer).\n",
    "\n",
    "    Returns:\n",
    "      A Tensor of type tf.bool with shape [batch_size, max_num_proposals].\n",
    "    \"\"\"\n",
    "    batch_size = tf.size(num_proposals)\n",
    "    tiled_num_proposals = tf.tile(\n",
    "        tf.expand_dims(num_proposals, 1), [1, max_num_proposals])\n",
    "    tiled_proposal_index = tf.tile(\n",
    "        tf.expand_dims(tf.range(max_num_proposals), 0), [batch_size, 1])\n",
    "    return tf.greater(tiled_num_proposals, tiled_proposal_index)\n",
    "\n",
    "  def _unpad_proposals_and_apply_hard_mining(self,\n",
    "                                             proposal_boxlists,\n",
    "                                             second_stage_loc_losses,\n",
    "                                             second_stage_cls_losses,\n",
    "                                             num_proposals):\n",
    "    \"\"\"Unpads proposals and applies hard mining.\n",
    "\n",
    "    Args:\n",
    "      proposal_boxlists: A list of `batch_size` BoxLists each representing\n",
    "        `self.max_num_proposals` representing decoded proposal bounding boxes\n",
    "        for each image.\n",
    "      second_stage_loc_losses: A Tensor of type `float32`. A tensor of shape\n",
    "        `[batch_size, self.max_num_proposals]` representing per-anchor\n",
    "        second stage localization loss values.\n",
    "      second_stage_cls_losses: A Tensor of type `float32`. A tensor of shape\n",
    "        `[batch_size, self.max_num_proposals]` representing per-anchor\n",
    "        second stage classification loss values.\n",
    "      num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]\n",
    "        representing the number of proposals predicted for each image in\n",
    "        the batch.\n",
    "\n",
    "    Returns:\n",
    "      second_stage_loc_loss: A scalar float32 tensor representing the second\n",
    "        stage localization loss.\n",
    "      second_stage_cls_loss: A scalar float32 tensor representing the second\n",
    "        stage classification loss.\n",
    "    \"\"\"\n",
    "    for (proposal_boxlist, single_image_loc_loss, single_image_cls_loss,\n",
    "         single_image_num_proposals) in zip(\n",
    "             proposal_boxlists,\n",
    "             tf.unstack(second_stage_loc_losses),\n",
    "             tf.unstack(second_stage_cls_losses),\n",
    "             tf.unstack(num_proposals)):\n",
    "      proposal_boxlist = box_list.BoxList(\n",
    "          tf.slice(proposal_boxlist.get(),\n",
    "                   [0, 0], [single_image_num_proposals, -1]))\n",
    "      single_image_loc_loss = tf.slice(single_image_loc_loss,\n",
    "                                       [0], [single_image_num_proposals])\n",
    "      single_image_cls_loss = tf.slice(single_image_cls_loss,\n",
    "                                       [0], [single_image_num_proposals])\n",
    "      return self._hard_example_miner(\n",
    "          location_losses=tf.expand_dims(single_image_loc_loss, 0),\n",
    "          cls_losses=tf.expand_dims(single_image_cls_loss, 0),\n",
    "          decoded_boxlist_list=[proposal_boxlist])\n",
    "\n",
    "  def restore_map(self,\n",
    "                  fine_tune_checkpoint_type='detection',\n",
    "                  load_all_detection_checkpoint_vars=False):\n",
    "    \"\"\"Returns a map of variables to load from a foreign checkpoint.\n",
    "\n",
    "    See parent class for details.\n",
    "\n",
    "    Args:\n",
    "      fine_tune_checkpoint_type: whether to restore from a full detection\n",
    "        checkpoint (with compatible variable names) or to restore from a\n",
    "        classification checkpoint for initialization prior to training.\n",
    "        Valid values: `detection`, `classification`. Default 'detection'.\n",
    "       load_all_detection_checkpoint_vars: whether to load all variables (when\n",
    "         `fine_tune_checkpoint_type` is `detection`). If False, only variables\n",
    "         within the feature extractor scopes are included. Default False.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping variable names (to load from a checkpoint) to variables in\n",
    "      the model graph.\n",
    "    Raises:\n",
    "      ValueError: if fine_tune_checkpoint_type is neither `classification`\n",
    "        nor `detection`.\n",
    "    \"\"\"\n",
    "    if fine_tune_checkpoint_type not in ['detection', 'classification']:\n",
    "      raise ValueError('Not supported fine_tune_checkpoint_type: {}'.format(\n",
    "          fine_tune_checkpoint_type))\n",
    "    if fine_tune_checkpoint_type == 'classification':\n",
    "      return self._feature_extractor.restore_from_classification_checkpoint_fn(\n",
    "          self.first_stage_feature_extractor_scope,\n",
    "          self.second_stage_feature_extractor_scope)\n",
    "\n",
    "    variables_to_restore = tf.global_variables()\n",
    "    variables_to_restore.append(slim.get_or_create_global_step())\n",
    "    # Only load feature extractor variables to be consistent with loading from\n",
    "    # a classification checkpoint.\n",
    "    include_patterns = None\n",
    "    if not load_all_detection_checkpoint_vars:\n",
    "      include_patterns = [\n",
    "          self.first_stage_feature_extractor_scope,\n",
    "          self.second_stage_feature_extractor_scope\n",
    "      ]\n",
    "    feature_extractor_variables = tf.contrib.framework.filter_variables(\n",
    "        variables_to_restore, include_patterns=include_patterns)\n",
    "    return {var.op.name: var for var in feature_extractor_variables}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from object_detection.meta_architectures import faster_rcnn_meta_arch\n",
    "\n",
    "def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):\n",
    "  \"\"\"Builds a Faster R-CNN or R-FCN detection model based on the model config.\n",
    "\n",
    "  Builds R-FCN model if the second_stage_box_predictor in the config is of type\n",
    "  `rfcn_box_predictor` else builds a Faster R-CNN model.\n",
    "\n",
    "  Args:\n",
    "    frcnn_config: A faster_rcnn.proto object containing the config for the\n",
    "      desired FasterRCNNMetaArch or RFCNMetaArch.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tf summaries in the model.\n",
    "\n",
    "  Returns:\n",
    "    FasterRCNNMetaArch based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If frcnn_config.type is not recognized (i.e. not registered in\n",
    "      model_class_map).\n",
    "  \"\"\"\n",
    "  num_classes = frcnn_config.num_classes\n",
    "  image_resizer_fn = image_resizer_builder(frcnn_config.image_resizer)\n",
    "\n",
    "  feature_extractor = _build_faster_rcnn_feature_extractor(\n",
    "      frcnn_config.feature_extractor, is_training,\n",
    "      frcnn_config.inplace_batchnorm_update)\n",
    "\n",
    "  number_of_stages = frcnn_config.number_of_stages\n",
    "  first_stage_anchor_generator = anchor_generator_builder(\n",
    "      frcnn_config.first_stage_anchor_generator)\n",
    "\n",
    "  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate\n",
    "  first_stage_box_predictor_arg_scope_fn = hyperparams_builder(\n",
    "      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\n",
    "  first_stage_box_predictor_kernel_size = (\n",
    "      frcnn_config.first_stage_box_predictor_kernel_size)\n",
    "  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth\n",
    "  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size\n",
    "  first_stage_positive_balance_fraction = (\n",
    "      frcnn_config.first_stage_positive_balance_fraction)\n",
    "  first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold\n",
    "  first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold\n",
    "  first_stage_max_proposals = frcnn_config.first_stage_max_proposals\n",
    "  first_stage_loc_loss_weight = (\n",
    "      frcnn_config.first_stage_localization_loss_weight)\n",
    "  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight\n",
    "\n",
    "  initial_crop_size = frcnn_config.initial_crop_size\n",
    "  maxpool_kernel_size = frcnn_config.maxpool_kernel_size\n",
    "  maxpool_stride = frcnn_config.maxpool_stride\n",
    "\n",
    "  second_stage_box_predictor = box_predictor_builder(\n",
    "      hyperparams_builder,\n",
    "      frcnn_config.second_stage_box_predictor,\n",
    "      is_training=is_training,\n",
    "      num_classes=num_classes)\n",
    "  second_stage_batch_size = frcnn_config.second_stage_batch_size\n",
    "  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction\n",
    "  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn\n",
    "  ) = post_processing_builder(frcnn_config.second_stage_post_processing)\n",
    "  second_stage_localization_loss_weight = (\n",
    "      frcnn_config.second_stage_localization_loss_weight)\n",
    "  second_stage_classification_loss = (\n",
    "      build_faster_rcnn_classification_loss(\n",
    "          frcnn_config.second_stage_classification_loss))\n",
    "  second_stage_classification_loss_weight = (\n",
    "      frcnn_config.second_stage_classification_loss_weight)\n",
    "  second_stage_mask_prediction_loss_weight = (\n",
    "      frcnn_config.second_stage_mask_prediction_loss_weight)\n",
    "\n",
    "  hard_example_miner = None\n",
    "  if frcnn_config.HasField('hard_example_miner'):\n",
    "    hard_example_miner = losses_builder.build_hard_example_miner(\n",
    "        frcnn_config.hard_example_miner,\n",
    "        second_stage_classification_loss_weight,\n",
    "        second_stage_localization_loss_weight)\n",
    "\n",
    "  common_kwargs = {\n",
    "      'is_training': is_training,\n",
    "      'num_classes': num_classes,\n",
    "      'image_resizer_fn': image_resizer_fn,\n",
    "      'feature_extractor': feature_extractor,\n",
    "      'number_of_stages': number_of_stages,\n",
    "      'first_stage_anchor_generator': first_stage_anchor_generator,\n",
    "      'first_stage_atrous_rate': first_stage_atrous_rate,\n",
    "      'first_stage_box_predictor_arg_scope_fn':\n",
    "      first_stage_box_predictor_arg_scope_fn,\n",
    "      'first_stage_box_predictor_kernel_size':\n",
    "      first_stage_box_predictor_kernel_size,\n",
    "      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,\n",
    "      'first_stage_minibatch_size': first_stage_minibatch_size,\n",
    "      'first_stage_positive_balance_fraction':\n",
    "      first_stage_positive_balance_fraction,\n",
    "      'first_stage_nms_score_threshold': first_stage_nms_score_threshold,\n",
    "      'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,\n",
    "      'first_stage_max_proposals': first_stage_max_proposals,\n",
    "      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,\n",
    "      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,\n",
    "      'second_stage_batch_size': second_stage_batch_size,\n",
    "      'second_stage_balance_fraction': second_stage_balance_fraction,\n",
    "      'second_stage_non_max_suppression_fn':\n",
    "      second_stage_non_max_suppression_fn,\n",
    "      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,\n",
    "      'second_stage_localization_loss_weight':\n",
    "      second_stage_localization_loss_weight,\n",
    "      'second_stage_classification_loss':\n",
    "      second_stage_classification_loss,\n",
    "      'second_stage_classification_loss_weight':\n",
    "      second_stage_classification_loss_weight,\n",
    "      'hard_example_miner': hard_example_miner,\n",
    "      'add_summaries': add_summaries}\n",
    "\n",
    "  if isinstance(second_stage_box_predictor, RfcnBoxPredictor):\n",
    "    return rfcn_meta_arch.RFCNMetaArch(\n",
    "        second_stage_rfcn_box_predictor=second_stage_box_predictor,\n",
    "        **common_kwargs)\n",
    "  else:\n",
    "    return FasterRCNNMetaArch(  # faster_rcnn_meta_arch.FasterRCNNMetaArch(\n",
    "        initial_crop_size=initial_crop_size,\n",
    "        maxpool_kernel_size=maxpool_kernel_size,\n",
    "        maxpool_stride=maxpool_stride,\n",
    "        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,\n",
    "        second_stage_mask_prediction_loss_weight=(\n",
    "            second_stage_mask_prediction_loss_weight),\n",
    "        **common_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(model_config, is_training, add_summaries=True,\n",
    "          add_background_class=True):\n",
    "  \"\"\"Builds a DetectionModel based on the model config.\n",
    "\n",
    "  Args:\n",
    "    model_config: A model.proto object containing the config for the desired\n",
    "      DetectionModel.\n",
    "    is_training: True if this model is being built for training purposes.\n",
    "    add_summaries: Whether to add tensorflow summaries in the model graph.\n",
    "    add_background_class: Whether to add an implicit background class to one-hot\n",
    "      encodings of groundtruth labels. Set to false if using groundtruth labels\n",
    "      with an explicit background class or using multiclass scores instead of\n",
    "      truth in the case of distillation. Ignored in the case of faster_rcnn.\n",
    "  Returns:\n",
    "    DetectionModel based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid meta architecture or model.\n",
    "  \"\"\"\n",
    "  if not isinstance(model_config, model_pb2.DetectionModel):\n",
    "    raise ValueError('model_config not of type model_pb2.DetectionModel.')\n",
    "  meta_architecture = model_config.WhichOneof('model')\n",
    "  if meta_architecture == 'ssd':\n",
    "    return _build_ssd_model(model_config.ssd, is_training, add_summaries,\n",
    "                            add_background_class)\n",
    "  if meta_architecture == 'faster_rcnn':\n",
    "    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n",
    "                                    add_summaries)\n",
    "  raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = functools.partial(\n",
    "    model_builder,\n",
    "    model_config=model_config,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(config):\n",
    "    return dataset_builder.make_initializable_iterator(\n",
    "        dataset_builder.build(config)).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_input_dict_fn = functools.partial(get_next, input_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "cluster_data = env.get('cluster', None)\n",
    "cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
    "task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
    "task_info = type('TaskSpec', (object,), task_data)\n",
    "\n",
    "# Parameters for a single worker.\n",
    "ps_tasks = 0\n",
    "worker_replicas = 1\n",
    "worker_job_name = 'lonely_worker'\n",
    "task = 0\n",
    "is_chief = True\n",
    "master = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_data and 'worker' in cluster_data:\n",
    "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
    "    worker_replicas = len(cluster_data['worker']) + 1\n",
    "if cluster_data and 'ps' in cluster_data:\n",
    "    ps_tasks = len(cluster_data['ps'])\n",
    "\n",
    "if worker_replicas > 1 and ps_tasks < 1:\n",
    "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
    "\n",
    "if worker_replicas >= 1 and ps_tasks > 0:\n",
    "    # Set up distributed training.\n",
    "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
    "                             job_name=task_info.type,\n",
    "                             task_index=task_info.index)\n",
    "    if task_info.type == 'ps':\n",
    "        server.join()\n",
    "        raise (\"end\")  # It was 'return' of main()\n",
    "\n",
    "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
    "    task = task_info.index\n",
    "    is_chief = (task_info.type == 'master')\n",
    "    master = server.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rewriter_fn = None\n",
    "if 'graph_rewriter_config' in configs:\n",
    "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
    "        configs['graph_rewriter_config'], is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_builder\n",
    "# hyperparams_builder\n",
    "def hyperparams_builder(hyperparams_config, is_training):\n",
    "  if not isinstance(hyperparams_config,\n",
    "                    hyperparams_pb2.Hyperparams):\n",
    "    raise ValueError('hyperparams_config not of type '\n",
    "                     'hyperparams_pb.Hyperparams.')\n",
    "\n",
    "  batch_norm = None\n",
    "  batch_norm_params = None\n",
    "  if hyperparams_config.HasField('batch_norm'):\n",
    "    batch_norm = slim.batch_norm\n",
    "    batch_norm_params = _build_batch_norm_params(\n",
    "        hyperparams_config.batch_norm, is_training)\n",
    "\n",
    "  affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]\n",
    "  if hyperparams_config.HasField('op') and (\n",
    "      hyperparams_config.op == hyperparams_pb2.Hyperparams.FC):\n",
    "    affected_ops = [slim.fully_connected]\n",
    "  def scope_fn():\n",
    "    with (slim.arg_scope([slim.batch_norm], **batch_norm_params)\n",
    "          if batch_norm_params is not None else\n",
    "          context_manager.IdentityContextManager()):\n",
    "      with slim.arg_scope(\n",
    "          affected_ops,\n",
    "          weights_regularizer=_build_regularizer(\n",
    "              hyperparams_config.regularizer),\n",
    "          weights_initializer=_build_initializer(\n",
    "              hyperparams_config.initializer),\n",
    "          activation_fn=_build_activation_fn(hyperparams_config.activation),\n",
    "          normalizer_fn=batch_norm) as sc:\n",
    "        return sc\n",
    "\n",
    "  return scope_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor_builder(argscope_fn, box_predictor_config, is_training, num_classes):\n",
    "  \"\"\"Builds box predictor based on the configuration.\n",
    "\n",
    "  Builds box predictor based on the configuration. See box_predictor.proto for\n",
    "  configurable options. Also, see box_predictor.py for more details.\n",
    "\n",
    "  Args:\n",
    "    argscope_fn: A function that takes the following inputs:\n",
    "        * hyperparams_pb2.Hyperparams proto\n",
    "        * a boolean indicating if the model is in training mode.\n",
    "      and returns a tf slim argscope for Conv and FC hyperparameters.\n",
    "    box_predictor_config: box_predictor_pb2.BoxPredictor proto containing\n",
    "      configuration.\n",
    "    is_training: Whether the models is in training mode.\n",
    "    num_classes: Number of classes to predict.\n",
    "\n",
    "  Returns:\n",
    "    box_predictor: box_predictor.BoxPredictor object.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On unknown box predictor.\n",
    "  \"\"\"\n",
    "  if not isinstance(box_predictor_config, box_predictor_pb2.BoxPredictor):\n",
    "    raise ValueError('box_predictor_config not of type '\n",
    "                     'box_predictor_pb2.BoxPredictor.')\n",
    "\n",
    "  box_predictor_oneof = box_predictor_config.WhichOneof('box_predictor_oneof')\n",
    "\n",
    "  if  box_predictor_oneof == 'convolutional_box_predictor':\n",
    "    conv_box_predictor = box_predictor_config.convolutional_box_predictor\n",
    "    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.ConvolutionalBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        min_depth=conv_box_predictor.min_depth,\n",
    "        max_depth=conv_box_predictor.max_depth,\n",
    "        num_layers_before_predictor=(conv_box_predictor.\n",
    "                                     num_layers_before_predictor),\n",
    "        use_dropout=conv_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,\n",
    "        kernel_size=conv_box_predictor.kernel_size,\n",
    "        box_code_size=conv_box_predictor.box_code_size,\n",
    "        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,\n",
    "        class_prediction_bias_init=(conv_box_predictor.\n",
    "                                    class_prediction_bias_init),\n",
    "        use_depthwise=conv_box_predictor.use_depthwise\n",
    "    )\n",
    "    return box_predictor_object\n",
    "\n",
    "  if  box_predictor_oneof == 'weight_shared_convolutional_box_predictor':\n",
    "    conv_box_predictor = (box_predictor_config.\n",
    "                          weight_shared_convolutional_box_predictor)\n",
    "    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.WeightSharedConvolutionalBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        depth=conv_box_predictor.depth,\n",
    "        num_layers_before_predictor=(\n",
    "            conv_box_predictor.num_layers_before_predictor),\n",
    "        kernel_size=conv_box_predictor.kernel_size,\n",
    "        box_code_size=conv_box_predictor.box_code_size,\n",
    "        class_prediction_bias_init=conv_box_predictor.\n",
    "        class_prediction_bias_init,\n",
    "        use_dropout=conv_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=conv_box_predictor.dropout_keep_probability)\n",
    "    return box_predictor_object\n",
    "\n",
    "  if box_predictor_oneof == 'mask_rcnn_box_predictor':\n",
    "    mask_rcnn_box_predictor = box_predictor_config.mask_rcnn_box_predictor\n",
    "    fc_hyperparams_fn = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,\n",
    "                                    is_training)\n",
    "    conv_hyperparams_fn = None\n",
    "    if mask_rcnn_box_predictor.HasField('conv_hyperparams'):\n",
    "      conv_hyperparams_fn = argscope_fn(\n",
    "          mask_rcnn_box_predictor.conv_hyperparams, is_training)\n",
    "    box_predictor_object = MaskRCNNBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        fc_hyperparams_fn=fc_hyperparams_fn,\n",
    "        use_dropout=mask_rcnn_box_predictor.use_dropout,\n",
    "        dropout_keep_prob=mask_rcnn_box_predictor.dropout_keep_probability,\n",
    "        box_code_size=mask_rcnn_box_predictor.box_code_size,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        predict_instance_masks=mask_rcnn_box_predictor.predict_instance_masks,\n",
    "        mask_height=mask_rcnn_box_predictor.mask_height,\n",
    "        mask_width=mask_rcnn_box_predictor.mask_width,\n",
    "        mask_prediction_num_conv_layers=(\n",
    "            mask_rcnn_box_predictor.mask_prediction_num_conv_layers),\n",
    "        mask_prediction_conv_depth=(\n",
    "            mask_rcnn_box_predictor.mask_prediction_conv_depth),\n",
    "        masks_are_class_agnostic=(\n",
    "            mask_rcnn_box_predictor.masks_are_class_agnostic),\n",
    "        predict_keypoints=mask_rcnn_box_predictor.predict_keypoints,\n",
    "        share_box_across_classes=(\n",
    "            mask_rcnn_box_predictor.share_box_across_classes))\n",
    "    return box_predictor_object\n",
    "\n",
    "  if box_predictor_oneof == 'rfcn_box_predictor':\n",
    "    rfcn_box_predictor = box_predictor_config.rfcn_box_predictor\n",
    "    conv_hyperparams_fn = argscope_fn(rfcn_box_predictor.conv_hyperparams,\n",
    "                                      is_training)\n",
    "    box_predictor_object = box_predictor.RfcnBoxPredictor(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "        crop_size=[rfcn_box_predictor.crop_height,\n",
    "                   rfcn_box_predictor.crop_width],\n",
    "        num_spatial_bins=[rfcn_box_predictor.num_spatial_bins_height,\n",
    "                          rfcn_box_predictor.num_spatial_bins_width],\n",
    "        depth=rfcn_box_predictor.depth,\n",
    "        box_code_size=rfcn_box_predictor.box_code_size)\n",
    "    return box_predictor_object\n",
    "  raise ValueError('Unknown box predictor: {}'.format(box_predictor_oneof))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builder/post_processing_builder.py\n",
    "def post_processing_builder(post_processing_config):\n",
    "  \"\"\"Builds callables for post-processing operations.\n",
    "\n",
    "  Builds callables for non-max suppression and score conversion based on the\n",
    "  configuration.\n",
    "\n",
    "  Non-max suppression callable takes `boxes`, `scores`, and optionally\n",
    "  `clip_window`, `parallel_iterations` `masks, and `scope` as inputs. It returns\n",
    "  `nms_boxes`, `nms_scores`, `nms_classes` `nms_masks` and `num_detections`. See\n",
    "  post_processing.batch_multiclass_non_max_suppression for the type and shape\n",
    "  of these tensors.\n",
    "\n",
    "  Score converter callable should be called with `input` tensor. The callable\n",
    "  returns the output from one of 3 tf operations based on the configuration -\n",
    "  tf.identity, tf.sigmoid or tf.nn.softmax. See tensorflow documentation for\n",
    "  argument and return value descriptions.\n",
    "\n",
    "  Args:\n",
    "    post_processing_config: post_processing.proto object containing the\n",
    "      parameters for the post-processing operations.\n",
    "\n",
    "  Returns:\n",
    "    non_max_suppressor_fn: Callable for non-max suppression.\n",
    "    score_converter_fn: Callable for score conversion.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the post_processing_config is of incorrect type.\n",
    "  \"\"\"\n",
    "  if not isinstance(post_processing_config, post_processing_pb2.PostProcessing):\n",
    "    raise ValueError('post_processing_config not of type '\n",
    "                     'post_processing_pb2.Postprocessing.')\n",
    "  non_max_suppressor_fn = _build_non_max_suppressor(\n",
    "      post_processing_config.batch_non_max_suppression)\n",
    "  score_converter_fn = _build_score_converter(\n",
    "      post_processing_config.score_converter,\n",
    "      post_processing_config.logit_scale)\n",
    "  return non_max_suppressor_fn, score_converter_fn\n",
    "\n",
    "\n",
    "def _build_non_max_suppressor(nms_config):\n",
    "  \"\"\"Builds non-max suppresson based on the nms config.\n",
    "\n",
    "  Args:\n",
    "    nms_config: post_processing_pb2.PostProcessing.BatchNonMaxSuppression proto.\n",
    "\n",
    "  Returns:\n",
    "    non_max_suppressor_fn: Callable non-max suppressor.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On incorrect iou_threshold or on incompatible values of\n",
    "      max_total_detections and max_detections_per_class.\n",
    "  \"\"\"\n",
    "  if nms_config.iou_threshold < 0 or nms_config.iou_threshold > 1.0:\n",
    "    raise ValueError('iou_threshold not in [0, 1.0].')\n",
    "  if nms_config.max_detections_per_class > nms_config.max_total_detections:\n",
    "    raise ValueError('max_detections_per_class should be no greater than '\n",
    "                     'max_total_detections.')\n",
    "\n",
    "  non_max_suppressor_fn = functools.partial(\n",
    "      batch_multiclass_non_max_suppression,\n",
    "      score_thresh=nms_config.score_threshold,\n",
    "      iou_thresh=nms_config.iou_threshold,\n",
    "      max_size_per_class=nms_config.max_detections_per_class,\n",
    "      max_total_size=nms_config.max_total_detections)\n",
    "  return non_max_suppressor_fn\n",
    "\n",
    "\n",
    "def batch_multiclass_non_max_suppression(boxes,\n",
    "                                         scores,\n",
    "                                         score_thresh,\n",
    "                                         iou_thresh,\n",
    "                                         max_size_per_class,\n",
    "                                         max_total_size=0,\n",
    "                                         clip_window=None,\n",
    "                                         change_coordinate_frame=False,\n",
    "                                         num_valid_boxes=None,\n",
    "                                         masks=None,\n",
    "                                         additional_fields=None,\n",
    "                                         scope=None,\n",
    "                                         parallel_iterations=32):\n",
    "  q = boxes.shape[2].value\n",
    "  num_classes = scores.shape[2].value\n",
    "  if q != 1 and q != num_classes:\n",
    "    raise ValueError('third dimension of boxes must be either 1 or equal '\n",
    "                     'to the third dimension of scores')\n",
    "  if change_coordinate_frame and clip_window is None:\n",
    "    raise ValueError('if change_coordinate_frame is True, then a clip_window'\n",
    "                     'must be specified.')\n",
    "  original_masks = masks\n",
    "  original_additional_fields = additional_fields\n",
    "  with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):\n",
    "    boxes_shape = boxes.shape\n",
    "    batch_size = boxes_shape[0].value\n",
    "    num_anchors = boxes_shape[1].value\n",
    "\n",
    "    if batch_size is None:\n",
    "      batch_size = tf.shape(boxes)[0]\n",
    "    if num_anchors is None:\n",
    "      num_anchors = tf.shape(boxes)[1]\n",
    "\n",
    "    # If num valid boxes aren't provided, create one and mark all boxes as\n",
    "    # valid.\n",
    "    if num_valid_boxes is None:\n",
    "      num_valid_boxes = tf.ones([batch_size], dtype=tf.int32) * num_anchors\n",
    "\n",
    "    # If masks aren't provided, create dummy masks so we can only have one copy\n",
    "    # of _single_image_nms_fn and discard the dummy masks after map_fn.\n",
    "    if masks is None:\n",
    "      masks_shape = tf.stack([batch_size, num_anchors, 1, 0, 0])\n",
    "      masks = tf.zeros(masks_shape)\n",
    "\n",
    "    if clip_window is None:\n",
    "      clip_window = tf.stack([\n",
    "          tf.reduce_min(boxes[:, :, :, 0]),\n",
    "          tf.reduce_min(boxes[:, :, :, 1]),\n",
    "          tf.reduce_max(boxes[:, :, :, 2]),\n",
    "          tf.reduce_max(boxes[:, :, :, 3])\n",
    "      ])\n",
    "    if clip_window.shape.ndims == 1:\n",
    "      clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])\n",
    "\n",
    "    if additional_fields is None:\n",
    "      additional_fields = {}\n",
    "\n",
    "    def _single_image_nms_fn(args):\n",
    "      \"\"\"Runs NMS on a single image and returns padded output.\n",
    "\n",
    "      Args:\n",
    "        args: A list of tensors consisting of the following:\n",
    "          per_image_boxes - A [num_anchors, q, 4] float32 tensor containing\n",
    "            detections. If `q` is 1 then same boxes are used for all classes\n",
    "            otherwise, if `q` is equal to number of classes, class-specific\n",
    "            boxes are used.\n",
    "          per_image_scores - A [num_anchors, num_classes] float32 tensor\n",
    "            containing the scores for each of the `num_anchors` detections.\n",
    "          per_image_masks - A [num_anchors, q, mask_height, mask_width] float32\n",
    "            tensor containing box masks. `q` can be either number of classes\n",
    "            or 1 depending on whether a separate mask is predicted per class.\n",
    "          per_image_clip_window - A 1D float32 tensor of the form\n",
    "            [ymin, xmin, ymax, xmax] representing the window to clip the boxes\n",
    "            to.\n",
    "          per_image_additional_fields - (optional) A variable number of float32\n",
    "            tensors each with size [num_anchors, ...].\n",
    "          per_image_num_valid_boxes - A tensor of type `int32`. A 1-D tensor of\n",
    "            shape [batch_size] representing the number of valid boxes to be\n",
    "            considered for each image in the batch.  This parameter allows for\n",
    "            ignoring zero paddings.\n",
    "\n",
    "      Returns:\n",
    "        'nmsed_boxes': A [max_detections, 4] float32 tensor containing the\n",
    "          non-max suppressed boxes.\n",
    "        'nmsed_scores': A [max_detections] float32 tensor containing the scores\n",
    "          for the boxes.\n",
    "        'nmsed_classes': A [max_detections] float32 tensor containing the class\n",
    "          for boxes.\n",
    "        'nmsed_masks': (optional) a [max_detections, mask_height, mask_width]\n",
    "          float32 tensor containing masks for each selected box. This is set to\n",
    "          None if input `masks` is None.\n",
    "        'nmsed_additional_fields':  (optional) A variable number of float32\n",
    "          tensors each with size [max_detections, ...] corresponding to the\n",
    "          input `per_image_additional_fields`.\n",
    "        'num_detections': A [batch_size] int32 tensor indicating the number of\n",
    "          valid detections per batch item. Only the top num_detections[i]\n",
    "          entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The\n",
    "          rest of the entries are zero paddings.\n",
    "      \"\"\"\n",
    "      per_image_boxes = args[0]\n",
    "      per_image_scores = args[1]\n",
    "      per_image_masks = args[2]\n",
    "      per_image_clip_window = args[3]\n",
    "      per_image_additional_fields = {\n",
    "          key: value\n",
    "          for key, value in zip(additional_fields, args[4:-1])\n",
    "      }\n",
    "      per_image_num_valid_boxes = args[-1]\n",
    "      per_image_boxes = tf.reshape(\n",
    "          tf.slice(per_image_boxes, 3 * [0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1, -1])), [-1, q, 4])\n",
    "      per_image_scores = tf.reshape(\n",
    "          tf.slice(per_image_scores, [0, 0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1])),\n",
    "          [-1, num_classes])\n",
    "      per_image_masks = tf.reshape(\n",
    "          tf.slice(per_image_masks, 4 * [0],\n",
    "                   tf.stack([per_image_num_valid_boxes, -1, -1, -1])),\n",
    "          [-1, q, per_image_masks.shape[2].value,\n",
    "           per_image_masks.shape[3].value])\n",
    "      if per_image_additional_fields is not None:\n",
    "        for key, tensor in per_image_additional_fields.items():\n",
    "          additional_field_shape = tensor.get_shape()\n",
    "          additional_field_dim = len(additional_field_shape)\n",
    "          per_image_additional_fields[key] = tf.reshape(\n",
    "              tf.slice(per_image_additional_fields[key],\n",
    "                       additional_field_dim * [0],\n",
    "                       tf.stack([per_image_num_valid_boxes] +\n",
    "                                (additional_field_dim - 1) * [-1])),\n",
    "              [-1] + [dim.value for dim in additional_field_shape[1:]])\n",
    "      nmsed_boxlist = multiclass_non_max_suppression(\n",
    "          per_image_boxes,\n",
    "          per_image_scores,\n",
    "          score_thresh,\n",
    "          iou_thresh,\n",
    "          max_size_per_class,\n",
    "          max_total_size,\n",
    "          clip_window=per_image_clip_window,\n",
    "          change_coordinate_frame=change_coordinate_frame,\n",
    "          masks=per_image_masks,\n",
    "          additional_fields=per_image_additional_fields)\n",
    "      padded_boxlist = box_list_ops.pad_or_clip_box_list(nmsed_boxlist,\n",
    "                                                         max_total_size)\n",
    "      num_detections = nmsed_boxlist.num_boxes()\n",
    "      nmsed_boxes = padded_boxlist.get()\n",
    "      nmsed_scores = padded_boxlist.get_field(fields.BoxListFields.scores)\n",
    "      nmsed_classes = padded_boxlist.get_field(fields.BoxListFields.classes)\n",
    "      nmsed_masks = padded_boxlist.get_field(fields.BoxListFields.masks)\n",
    "      nmsed_additional_fields = [\n",
    "          padded_boxlist.get_field(key) for key in per_image_additional_fields\n",
    "      ]\n",
    "      return ([nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks] +\n",
    "              nmsed_additional_fields + [num_detections])\n",
    "\n",
    "    num_additional_fields = 0\n",
    "    if additional_fields is not None:\n",
    "      num_additional_fields = len(additional_fields)\n",
    "    num_nmsed_outputs = 4 + num_additional_fields\n",
    "\n",
    "    batch_outputs = shape_utils.static_or_dynamic_map_fn(\n",
    "        _single_image_nms_fn,\n",
    "        elems=([boxes, scores, masks, clip_window] +\n",
    "               list(additional_fields.values()) + [num_valid_boxes]),\n",
    "        dtype=(num_nmsed_outputs * [tf.float32] + [tf.int32]),\n",
    "        parallel_iterations=parallel_iterations)\n",
    "\n",
    "    batch_nmsed_boxes = batch_outputs[0]\n",
    "    batch_nmsed_scores = batch_outputs[1]\n",
    "    batch_nmsed_classes = batch_outputs[2]\n",
    "    batch_nmsed_masks = batch_outputs[3]\n",
    "    batch_nmsed_additional_fields = {\n",
    "        key: value\n",
    "        for key, value in zip(additional_fields, batch_outputs[4:-1])\n",
    "    }\n",
    "    batch_num_detections = batch_outputs[-1]\n",
    "\n",
    "    if original_masks is None:\n",
    "      batch_nmsed_masks = None\n",
    "\n",
    "    if original_additional_fields is None:\n",
    "      batch_nmsed_additional_fields = None\n",
    "\n",
    "    return (batch_nmsed_boxes, batch_nmsed_scores, batch_nmsed_classes,\n",
    "            batch_nmsed_masks, batch_nmsed_additional_fields,\n",
    "            batch_num_detections)\n",
    "\n",
    "\n",
    "def _score_converter_fn_with_logit_scale(tf_score_converter_fn, logit_scale):\n",
    "  \"\"\"Create a function to scale logits then apply a Tensorflow function.\"\"\"\n",
    "  def score_converter_fn(logits):\n",
    "    scaled_logits = tf.divide(logits, logit_scale, name='scale_logits')\n",
    "    return tf_score_converter_fn(scaled_logits, name='convert_scores')\n",
    "  score_converter_fn.__name__ = '%s_with_logit_scale' % (\n",
    "      tf_score_converter_fn.__name__)\n",
    "  return score_converter_fn\n",
    "\n",
    "\n",
    "\n",
    "def _build_score_converter(score_converter_config, logit_scale):\n",
    "  \"\"\"Builds score converter based on the config.\n",
    "\n",
    "  Builds one of [tf.identity, tf.sigmoid, tf.softmax] score converters based on\n",
    "  the config.\n",
    "\n",
    "  Args:\n",
    "    score_converter_config: post_processing_pb2.PostProcessing.score_converter.\n",
    "    logit_scale: temperature to use for SOFTMAX score_converter.\n",
    "\n",
    "  Returns:\n",
    "    Callable score converter op.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On unknown score converter.\n",
    "  \"\"\"\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.IDENTITY:\n",
    "    return _score_converter_fn_with_logit_scale(tf.identity, logit_scale)\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.SIGMOID:\n",
    "    return _score_converter_fn_with_logit_scale(tf.sigmoid, logit_scale)\n",
    "  if score_converter_config == post_processing_pb2.PostProcessing.SOFTMAX:\n",
    "    return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)\n",
    "  raise ValueError('Unknown score converter.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_builder.py\n",
    "def build_faster_rcnn_classification_loss(loss_config):\n",
    "  \"\"\"Builds a classification loss for Faster RCNN based on the loss config.\n",
    "\n",
    "  Args:\n",
    "    loss_config: A losses_pb2.ClassificationLoss object.\n",
    "\n",
    "  Returns:\n",
    "    Loss based on the config.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: On invalid loss_config.\n",
    "  \"\"\"\n",
    "  if not isinstance(loss_config, losses_pb2.ClassificationLoss):\n",
    "    raise ValueError('loss_config not of type losses_pb2.ClassificationLoss.')\n",
    "\n",
    "  loss_type = loss_config.WhichOneof('classification_loss')\n",
    "\n",
    "  if loss_type == 'weighted_sigmoid':\n",
    "    return losses.WeightedSigmoidClassificationLoss()\n",
    "  if loss_type == 'weighted_softmax':\n",
    "    config = loss_config.weighted_softmax\n",
    "    return WeightedSoftmaxClassificationLoss(\n",
    "        logit_scale=config.logit_scale)\n",
    "  if loss_type == 'weighted_logits_softmax':\n",
    "    config = loss_config.weighted_logits_softmax\n",
    "    return losses.WeightedSoftmaxClassificationAgainstLogitsLoss(\n",
    "        logit_scale=config.logit_scale)\n",
    "\n",
    "  # By default, Faster RCNN second stage classifier uses Softmax loss\n",
    "  # with anchor-wise outputs.\n",
    "  config = loss_config.weighted_softmax\n",
    "  return WeightedSoftmaxClassificationLoss(\n",
    "      logit_scale=config.logit_scale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_hard_example_miner(config,\n",
    "                             classification_weight,\n",
    "                             localization_weight):\n",
    "  \"\"\"Builds hard example miner based on the config.\n",
    "\n",
    "  Args:\n",
    "    config: A losses_pb2.HardExampleMiner object.\n",
    "    classification_weight: Classification loss weight.\n",
    "    localization_weight: Localization loss weight.\n",
    "\n",
    "  Returns:\n",
    "    Hard example miner.\n",
    "\n",
    "  \"\"\"\n",
    "  loss_type = None\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.BOTH:\n",
    "    loss_type = 'both'\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.CLASSIFICATION:\n",
    "    loss_type = 'cls'\n",
    "  if config.loss_type == losses_pb2.HardExampleMiner.LOCALIZATION:\n",
    "    loss_type = 'loc'\n",
    "\n",
    "  max_negatives_per_positive = None\n",
    "  num_hard_examples = None\n",
    "  if config.max_negatives_per_positive > 0:\n",
    "    max_negatives_per_positive = config.max_negatives_per_positive\n",
    "  if config.num_hard_examples > 0:\n",
    "    num_hard_examples = config.num_hard_examples\n",
    "  hard_example_miner = losses.HardExampleMiner(\n",
    "      num_hard_examples=num_hard_examples,\n",
    "      iou_threshold=config.iou_threshold,\n",
    "      loss_type=loss_type,\n",
    "      cls_loss_weight=classification_weight,\n",
    "      loc_loss_weight=localization_weight,\n",
    "      max_negatives_per_positive=max_negatives_per_positive,\n",
    "      min_negatives_per_image=config.min_negatives_per_image)\n",
    "  return hard_example_miner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core/losses.py\n",
    "class Loss(object):\n",
    "  \"\"\"Abstract base class for loss functions.\"\"\"\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  def __call__(self,\n",
    "               prediction_tensor,\n",
    "               target_tensor,\n",
    "               ignore_nan_targets=False,\n",
    "               scope=None,\n",
    "               **params):\n",
    "    \"\"\"Call the loss function.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]\n",
    "        representing predicted quantities.\n",
    "      target_tensor: an N-d tensor of shape [batch, anchors, ...] representing\n",
    "        regression or classification targets.\n",
    "      ignore_nan_targets: whether to ignore nan targets in the loss computation.\n",
    "        E.g. can be used if the target tensor is missing groundtruth data that\n",
    "        shouldn't be factored into the loss.\n",
    "      scope: Op scope name. Defaults to 'Loss' if None.\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              the Loss.\n",
    "\n",
    "    Returns:\n",
    "      loss: a tensor representing the value of the loss function.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'Loss',\n",
    "                       [prediction_tensor, target_tensor, params]) as scope:\n",
    "      if ignore_nan_targets:\n",
    "        target_tensor = tf.where(tf.is_nan(target_tensor),\n",
    "                                 prediction_tensor,\n",
    "                                 target_tensor)\n",
    "      return self._compute_loss(prediction_tensor, target_tensor, **params)\n",
    "\n",
    "  @abstractmethod\n",
    "  def _compute_loss(self, prediction_tensor, target_tensor, **params):\n",
    "    \"\"\"Method to be overridden by implementations.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: a tensor representing predicted quantities\n",
    "      target_tensor: a tensor representing regression or classification targets\n",
    "      **params: Additional keyword arguments for specific implementations of\n",
    "              the Loss.\n",
    "\n",
    "    Returns:\n",
    "      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per\n",
    "        anchor\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class WeightedSoftmaxClassificationLoss(Loss):\n",
    "  \"\"\"Softmax loss function.\"\"\"\n",
    "\n",
    "  def __init__(self, logit_scale=1.0):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      logit_scale: When this value is high, the prediction is \"diffused\" and\n",
    "                   when this value is low, the prediction is made peakier.\n",
    "                   (default 1.0)\n",
    "\n",
    "    \"\"\"\n",
    "    self._logit_scale = logit_scale\n",
    "\n",
    "  def _compute_loss(self, prediction_tensor, target_tensor, weights):\n",
    "    \"\"\"Compute loss function.\n",
    "\n",
    "    Args:\n",
    "      prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "      target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "      weights: a float tensor of shape [batch_size, num_anchors]\n",
    "\n",
    "    Returns:\n",
    "      loss: a float tensor of shape [batch_size, num_anchors]\n",
    "        representing the value of the loss function.\n",
    "    \"\"\"\n",
    "    num_classes = prediction_tensor.get_shape().as_list()[-1]\n",
    "    prediction_tensor = tf.divide(\n",
    "        prediction_tensor, self._logit_scale, name='scale_logit')\n",
    "    per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.reshape(target_tensor, [-1, num_classes]),\n",
    "        logits=tf.reshape(prediction_tensor, [-1, num_classes])))\n",
    "    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensor_dict_fn = create_input_dict_fn\n",
    "create_model_fn = model_fn\n",
    "train_config = train_config\n",
    "master = master\n",
    "task = task\n",
    "num_clones = num_clones\n",
    "worker_replicas = worker_replicas\n",
    "clone_on_cpu = clone_on_cpu\n",
    "ps_tasks = ps_tasks\n",
    "worker_job_name = worker_job_name\n",
    "is_chief = is_chief\n",
    "train_dir = train_dir\n",
    "graph_hook_fn=graph_rewriter_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_assigner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-b95fd2eb34a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetection_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-0d7d4ed30bdd>\u001b[0m in \u001b[0;36mmodel_builder\u001b[1;34m(model_config, is_training, add_summaries, add_background_class)\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmeta_architecture\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'faster_rcnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n\u001b[1;32m---> 28\u001b[1;33m                                     add_summaries)\n\u001b[0m\u001b[0;32m     29\u001b[0m   \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown meta architecture: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-f483a8f64005>\u001b[0m in \u001b[0;36m_build_faster_rcnn_model\u001b[1;34m(frcnn_config, is_training, add_summaries)\u001b[0m\n\u001b[0;32m    124\u001b[0m         second_stage_mask_prediction_loss_weight=(\n\u001b[0;32m    125\u001b[0m             second_stage_mask_prediction_loss_weight),\n\u001b[1;32m--> 126\u001b[1;33m         **common_kwargs)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-d6429e20e730>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, is_training, num_classes, image_resizer_fn, feature_extractor, number_of_stages, first_stage_anchor_generator, first_stage_atrous_rate, first_stage_box_predictor_arg_scope_fn, first_stage_box_predictor_kernel_size, first_stage_box_predictor_depth, first_stage_minibatch_size, first_stage_positive_balance_fraction, first_stage_nms_score_threshold, first_stage_nms_iou_threshold, first_stage_max_proposals, first_stage_localization_loss_weight, first_stage_objectness_loss_weight, initial_crop_size, maxpool_kernel_size, maxpool_stride, second_stage_mask_rcnn_box_predictor, second_stage_batch_size, second_stage_balance_fraction, second_stage_non_max_suppression_fn, second_stage_score_conversion_fn, second_stage_localization_loss_weight, second_stage_classification_loss_weight, second_stage_classification_loss, second_stage_mask_prediction_loss_weight, hard_example_miner, parallel_iterations, add_summaries, use_matmul_crop_and_resize)\u001b[0m\n\u001b[0;32m    417\u001b[0m     unmatched_cls_target = tf.constant(\n\u001b[0;32m    418\u001b[0m         [1] + self._num_classes * [0], dtype=tf.float32)\n\u001b[1;32m--> 419\u001b[1;33m     self._proposal_target_assigner = target_assigner.create_target_assigner(\n\u001b[0m\u001b[0;32m    420\u001b[0m         'FasterRCNN', 'proposal')\n\u001b[0;32m    421\u001b[0m     self._detector_target_assigner = target_assigner.create_target_assigner(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_assigner' is not defined"
     ]
    }
   ],
   "source": [
    "detection_model = create_model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_options = [preprocessor_builder.build(step)\n",
    "                             for step in train_config.data_augmentation_options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "deploy_config = model_deploy.DeploymentConfig(\n",
    "    num_clones=num_clones,\n",
    "    clone_on_cpu=clone_on_cpu,\n",
    "    replica_id=task,\n",
    "    num_replicas=worker_replicas,\n",
    "    num_ps_tasks=ps_tasks,\n",
    "    worker_job_name=worker_job_name)\n",
    "\n",
    "# Place the global step on the device storing the variables.\n",
    "with tf.device(deploy_config.variables_device()):\n",
    "    global_step = slim.create_global_step()\n",
    "\n",
    "if num_clones != 1 and train_config.sync_replicas:\n",
    "    raise ValueError('In Synchronous SGD mode num_clones must ',\n",
    "                     'be 1. Found num_clones: {}'.format(num_clones))\n",
    "batch_size = train_config.batch_size // num_clones\n",
    "if train_config.sync_replicas:\n",
    "    batch_size //= train_config.replicas_to_aggregate\n",
    "\n",
    "with tf.device(deploy_config.inputs_device()):\n",
    "    input_queue = create_input_queue(\n",
    "        batch_size, create_tensor_dict_fn,\n",
    "        train_config.batch_queue_capacity,\n",
    "        train_config.num_batch_queue_threads,\n",
    "        train_config.prefetch_queue_capacity, data_augmentation_options)\n",
    "\n",
    "# Gather initial summaries.\n",
    "# TODO(rathodv): See if summaries can be added/extracted from global tf\n",
    "# collections so that they don't have to be passed around.\n",
    "summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "global_summaries = set([])\n",
    "\n",
    "model_fn = functools.partial(_create_losses,\n",
    "                             create_model_fn=create_model_fn,\n",
    "                             train_config=train_config)\n",
    "clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\n",
    "first_clone_scope = clones[0].scope\n",
    "\n",
    "if graph_hook_fn:\n",
    "    with tf.device(deploy_config.variables_device()):\n",
    "        graph_hook_fn()\n",
    "\n",
    "# Gather update_ops from the first clone. These contain, for example,\n",
    "# the updates for the batch_norm variables created by model_fn.\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "with tf.device(deploy_config.optimizer_device()):\n",
    "    training_optimizer, optimizer_summary_vars = optimizer_builder.build(\n",
    "        train_config.optimizer)\n",
    "    for var in optimizer_summary_vars:\n",
    "        tf.summary.scalar(var.op.name, var, family='LearningRate')\n",
    "\n",
    "sync_optimizer = None\n",
    "if train_config.sync_replicas:\n",
    "    training_optimizer = tf.train.SyncReplicasOptimizer(\n",
    "        training_optimizer,\n",
    "        replicas_to_aggregate=train_config.replicas_to_aggregate,\n",
    "        total_num_replicas=worker_replicas)\n",
    "    sync_optimizer = training_optimizer\n",
    "\n",
    "with tf.device(deploy_config.optimizer_device()):\n",
    "    regularization_losses = (None if train_config.add_regularization_loss\n",
    "                             else [])\n",
    "    total_loss, grads_and_vars = model_deploy.optimize_clones(\n",
    "        clones, training_optimizer,\n",
    "        regularization_losses=regularization_losses)\n",
    "    total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')\n",
    "\n",
    "    # Optionally multiply bias gradients by train_config.bias_grad_multiplier.\n",
    "    if train_config.bias_grad_multiplier:\n",
    "        biases_regex_list = ['.*/biases']\n",
    "        grads_and_vars = variables_helper.multiply_gradients_matching_regex(\n",
    "            grads_and_vars,\n",
    "            biases_regex_list,\n",
    "            multiplier=train_config.bias_grad_multiplier)\n",
    "\n",
    "    # Optionally freeze some layers by setting their gradients to be zero.\n",
    "    if train_config.freeze_variables:\n",
    "        grads_and_vars = variables_helper.freeze_gradients_matching_regex(\n",
    "            grads_and_vars, train_config.freeze_variables)\n",
    "\n",
    "    # Optionally clip gradients\n",
    "    if train_config.gradient_clipping_by_norm > 0:\n",
    "        with tf.name_scope('clip_grads'):\n",
    "            grads_and_vars = slim.learning.clip_gradient_norms(\n",
    "                grads_and_vars, train_config.gradient_clipping_by_norm)\n",
    "\n",
    "    # Create gradient updates.\n",
    "    grad_updates = training_optimizer.apply_gradients(grads_and_vars,\n",
    "                                                      global_step=global_step)\n",
    "    update_ops.append(grad_updates)\n",
    "    update_op = tf.group(*update_ops, name='update_barrier')\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        train_tensor = tf.identity(total_loss, name='train_op')\n",
    "\n",
    "# Add summaries.\n",
    "for model_var in slim.get_model_variables():\n",
    "    global_summaries.add(tf.summary.histogram('ModelVars/' +\n",
    "                                              model_var.op.name, model_var))\n",
    "for loss_tensor in tf.losses.get_losses():\n",
    "    global_summaries.add(tf.summary.scalar('Losses/' + loss_tensor.op.name,\n",
    "                                           loss_tensor))\n",
    "global_summaries.add(\n",
    "    tf.summary.scalar('Losses/TotalLoss', tf.losses.get_total_loss()))\n",
    "\n",
    "# Add the summaries from the first clone. These contain the summaries\n",
    "# created by model_fn and either optimize_clones() or _gather_clone_loss().\n",
    "summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                   first_clone_scope))\n",
    "summaries |= global_summaries\n",
    "\n",
    "# Merge all summaries together.\n",
    "summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "# Soft placement allows placing on CPU ops without GPU implementation.\n",
    "session_config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                                log_device_placement=False)\n",
    "\n",
    "# Save checkpoints regularly.\n",
    "keep_checkpoint_every_n_hours = train_config.keep_checkpoint_every_n_hours\n",
    "saver = tf.train.Saver(\n",
    "    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n",
    "\n",
    "# Create ops required to initialize the model from a given checkpoint.\n",
    "init_fn = None\n",
    "if train_config.fine_tune_checkpoint:\n",
    "    if not train_config.fine_tune_checkpoint_type:\n",
    "        # train_config.from_detection_checkpoint field is deprecated. For\n",
    "        # backward compatibility, fine_tune_checkpoint_type is set based on\n",
    "        # from_detection_checkpoint.\n",
    "        if train_config.from_detection_checkpoint:\n",
    "            train_config.fine_tune_checkpoint_type = 'detection'\n",
    "        else:\n",
    "            train_config.fine_tune_checkpoint_type = 'classification'\n",
    "    var_map = detection_model.restore_map(\n",
    "        fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,\n",
    "        load_all_detection_checkpoint_vars=(\n",
    "            train_config.load_all_detection_checkpoint_vars))\n",
    "    available_var_map = (variables_helper.\n",
    "        get_variables_available_in_checkpoint(\n",
    "        var_map, train_config.fine_tune_checkpoint,\n",
    "        include_global_step=False))\n",
    "    init_saver = tf.train.Saver(available_var_map)\n",
    "\n",
    "\n",
    "    def initializer_fn(sess):\n",
    "        init_saver.restore(sess, train_config.fine_tune_checkpoint)\n",
    "\n",
    "\n",
    "    init_fn = initializer_fn\n",
    "\n",
    "slim.learning.train(\n",
    "    train_tensor,\n",
    "    logdir=train_dir,\n",
    "    master=master,\n",
    "    is_chief=is_chief,\n",
    "    session_config=session_config,\n",
    "    startup_delay_steps=train_config.startup_delay_steps,\n",
    "    init_fn=init_fn,\n",
    "    summary_op=summary_op,\n",
    "    number_of_steps=(\n",
    "        train_config.num_steps if train_config.num_steps else None),\n",
    "    save_summaries_secs=120,\n",
    "    sync_optimizer=sync_optimizer,\n",
    "    saver=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
